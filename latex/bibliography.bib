@article{mindek2017,
title = "Data-sensitive visual navigation",
journal = "Computers & Graphics",
volume = "67",
pages = "77 - 85",
year = "2017",
issn = "0097-8493",
doi = "https://doi.org/10.1016/j.cag.2017.05.012",
url = "http://www.sciencedirect.com/science/article/pii/S0097849317300614",
author = "Peter Mindek and Gabriel Mistelbauer and Eduard Gröller and Stefan Bruckner",
keywords = "Navigation, Exploration, Medical visualization"
}


@article{petersch_real_2005,
	title = {Real time computation and temporal coherence of opacity transfer functions for direct volume rendering of ultrasound data},
	volume = {29},
	issn = {0895-6111},
	doi = {10.1016/j.compmedimag.2004.09.013},
	abstract = {Opacity transfer function (OTF) generation for direct volume rendering of medical image data is an intensely discussed subject. Several automatic methods exist for CT and MRI data, which are not apt for ultrasound data, mainly due to its low signal-to-noise ratio. Furthermore, ultrasound (US) imaging is able to produce time-varying 3D datasets in real time thus opening the door to 4D visualization. However, OTF design for 4D datasets has not been exhaustively discussed until now. We present an efficient solution to generate an optimized OTF for a given 3DUS dataset in real time. Our method results in excellent visualization which we demonstrate using 3D fetus datasets. Finally, we discuss the applicability of our method to 4DUS visualization.},
	number = {1},
	journal = {Computerized medical imaging and graphics: the official journal of the Computerized Medical Imaging Society},
	author = {Petersch, Bernhard and Hadwiger, Markus and Hauser, Helwig and H{\"o}nigmann, Dieter},
	month = jan,
	year = {2005},
	pmid = {15710541},
	keywords = {3D/4D ultrasound, Austria, direct volume rendering, Humans, Magnetic resonance imaging, Tomography, X-Ray Computed, transfer function},
	pages = {53--63}
}

@inproceedings{balabanian_temporal_2008,
	title = {Temporal Styles for Time-Varying Volume Data},
	location = {Atlanta, {USA}},
	url = {http://www.cc.gatech.edu/research/reports/GT-IC-08-05},
	abstract = {This paper introduces interaction mechanisms for conveying temporal characteristics of time-varying volume data based on temporal styles. We demonstrate the flexibility of the new concept through different temporal style transfer function types and we define a set of temporal compositors as operators on them. The data is rendered by a multi-volume {GPU} raycaster that does not require any grid alignment over the individual time-steps of our data nor a rectilinear grid structure. The paper presents the applicability of the new concept on different data sets from partial to full voxel alignment with rectilinear and curvilinear grid layout.},
	booktitle = {Proceedings of {3DPVT'08} - the Fourth International Symposium on {3D} Data Processing, Visualization and Transmission},
	author = {Balabanian, Jean-Paul and Viola, Ivan and M{\"o}ller, Torsten and Gr{\"o}ller, Eduard},
	editor = {Gumhold, Stephan and Kosecka, Jana and Staadt, Oliver},
	month = jun,
	year = {2008},
	pages = {81-89},
	file = {Balabanian2008_3DPVT.pdf:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\vuyr0ioc.default\zotero\storage\UMEITD39\Balabanian2008_3DPVT.pdf:application/pdf}
}

@inproceedings{mantiuk2011hdr,
	title={HDR-VDP-2: a calibrated visual metric for visibility and quality predictions in all luminance conditions},
	author={Mantiuk, Rafat and Kim, Kil Joong and Rempel, Allan G and Heidrich, Wolfgang},
	booktitle={ACM Transactions on Graphics (TOG)},
	volume={30},
	number={4},
	pages={40},
	year={2011},
	organization={ACM}
}

@article{wang_importance-driven_2008,
	title = {Importance-Driven Time-Varying Data Visualization},
	volume = {14},
	issn = {1077-2626},
	doi = {10.1109/TVCG.2008.140},
	abstract = {The ability to identify and present the most essential aspects of time-varying data is critically important in many areas of science and engineering. This paper introduces an importance-driven approach to time-varying volume data visualization for enhancing that ability. By conducting a block-wise analysis of the data in the joint feature-temporal space, we derive an importance curve for each data block based on the formulation of conditional entropy from information theory. Each curve characterizes the local temporal behavior of the respective block, and clustering the importance curves of all the volume blocks effectively classifies the underlying data. Based on different temporal trends exhibited by importance curves and their clustering results, we suggest several interesting and effective visualization techniques to reveal the important aspects of time-varying data.},
	number = {6},
	journal = {{IEEE} Transactions on Visualization and Computer Graphics},
	author = {Wang, Chaoli and Yu, Hongfeng and Ma, Kwan-Liu},
	month = dec,
	year = {2008},
	keywords = {block-wise analysis, conditional entropy, data visualisation, entropy, feature-temporal space, importance-driven time-varying volume data visualization, Information Theory, pattern classification, pattern clustering},
	pages = {1547 --1554},
	file = {IEEE Xplore Abstract Record:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\vuyr0ioc.default\zotero\storage\7DH5SWKH\cookiedetectresponse.html:text/html;IEEE Xplore Full Text PDF:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\vuyr0ioc.default\zotero\storage\JVHT7PDI\Wang et al. - 2008 - Importance-Driven Time-Varying Data Visualization.pdf:application/pdf}
}

@article{tikhonova_exploratory_2010,
	title = {An {Exploratory} {Technique} for {Coherent} {Visualization} of {Time}-varying {Volume} {Data}},
	volume = {29},
	copyright = {© 2010 The Author(s) Journal compilation © 2010 The Eurographics Association and Blackwell Publishing Ltd.},
	issn = {1467-8659},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1467-8659.2009.01690.x/abstract},
	doi = {10.1111/j.1467-8659.2009.01690.x},
	abstract = {The selection of an appropriate global transfer function is essential for visualizing time-varying simulation data. This is especially challenging when the global data range is not known in advance, as is often the case in remote and in-situ visualization settings. Since the data range may vary dramatically as the simulation progresses, volume rendering using local transfer functions may not be coherent for all time steps. We present an exploratory technique that enables coherent classification of time-varying volume data. Unlike previous approaches, which require pre-processing of all time steps, our approach lets the user explore the transfer function space without accessing the original 3D data. This is useful for interactive visualization, and absolutely essential for in-situ visualization, where the entire simulation data range is not known in advance. Our approach generates a compact representation of each time step at rendering time in the form of ray attenuation functions, which are used for subsequent operations on the opacity and color mappings. The presented approach offers interactive exploration of time-varying simulation data that alleviates the cost associated with reloading and caching large data sets.},
	language = {en},
	number = {3},
	urldate = {2014-11-04},
	journal = {Computer Graphics Forum},
	author = {Tikhonova, A. and Correa, C. D. and Ma, K.-L.},
	month = jun,
	year = {2010},
	keywords = {and texture, I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism—Color, shading, shadowing},
	pages = {783--792}
}

@mastersthesis{valverde-real-2010,
	title = {Real Time Rendering of Animated Volumetric Data},
	school = {University of Dublin, Trinity College},
	author = {Valverde, Luis},
	year = {2010}
}

@inproceedings{westermann_gpu-supported_2010,
	title = {A {GPU}-{Supported} {Lossless} {Compression} {Scheme} for {Rendering} {Time}-{Varying} {Volume} {Data}},
	isbn = {978-3-905674-23-1},
	doi = {10.2312/VG/VG10/109-116},
	booktitle = {{IEEE}/ {EG} {Symposium} on {Volume} {Graphics}},
	publisher = {The Eurographics Association},
	author = {Mensmann, J{\"o}rg and Ropinski, Timo and Hinrichs, Klaus},
	editor = {Westermann, Ruediger and Kindlmann, Gordon},
	year = {2010}
}

@article{wang_application-driven_2009,
	title = {Application-Driven Compression for Visualizing Large-Scale Time-Varying Volume Data},
	volume = {{PP}},
	issn = {0272-1716},
	doi = {10.1109/MCG.2009.104},
	abstract = {In many areas of science and engineering, the desires to study a problem at the highest possible resolution have led to an explosive growth of data. It is imperative to reduce the data to a manageable scale for analysis and visualization. For high-precision floating-point data, compressing the data solely based on values can only achieve a limited saving. Further reduction is possible with the fact that usually only a smaller subset of the data is of interest in analysis. In this paper, we present an application-driven approach to compressing large-scale time-varying volume data. Our method identifies a reference feature to partition the data into space-time blocks, which are compressed with various precisions depending on their association to the feature. Runtime decompression is performed with bit-wise texture packing and deferred filtering. We show that our method achieves high compression rates and interactive rendering while preserving fine details surrounding regions of interest. Such an application-driven approach points us to a promising direction for coping with the large data problems facing computational scientists.},
	number = {99},
	journal = {{IEEE} Computer Graphics and Applications},
	author = {Wang, C and Yu, H and Ma, K},
	year = {2009},
	pages = {1},
	file = {cga10-adc.pdf:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\vuyr0ioc.default\zotero\storage\AGQWPF3P\cga10-adc.pdf:application/pdf;IEEE Xplore Abstract Record:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\vuyr0ioc.default\zotero\storage\I3QE39SZ\articleDetails.html:text/html;IEEE Xplore Full Text PDF:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\vuyr0ioc.default\zotero\storage\MPMKKTAX\Wang et al. - 2009 - Application-Driven Compression for Visualizing Lar.pdf:application/pdf}
}

@phdthesis{garcia_parallel_2006,
	address = {Columbus, {OH}, {USA}},
	title = {Parallel time varying volume rendering on tile displays},
	abstract = {Volume rendering is a process with high computation demands. Time-Varying Volume Data {(TVVD)} increases the challenge not only for computing but also for loading and storing of the data in real time. Scientific fields are generating data at a much faster rate than processing power; thus parallel solutions are necessary for rendering such large datasets. Furthermore, tile displays provide more pixels to present the features of the phenomena under study with finer detail. However, current object-space partitioning schemes for parallel rendering can make load imbalance across processors very severe. This work provides an alternative based on wavelet theory. The size reduction of the data allows data migration and makes image-space partitioning schemes possible for commodity processors deployed in {PC-clusters.}},
	school = {Ohio State University},
	author = {Garcia, Antonio},
	year = {2006},
	note = {{AAI3197874}},
	file = {Garcia Antonio.pdf:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\vuyr0ioc.default\zotero\storage\72A53DRR\Garcia Antonio.pdf:application/pdf}
}

@incollection{serlie_computed_2003,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Computed {Cleansing} for {Virtual} {Colonoscopy} {Using} a {Three}-{Material} {Transition} {Model}},
	copyright = {©2003 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-540-20464-0 978-3-540-39903-2},
	url = {http://link.springer.com/chapter/10.1007/978-3-540-39903-2_22},
	abstract = {Virtual colonoscopy is a non-invasive technique for the detection of polyps. Currently, a clean colon is required; as without cleansing the colonic wall cannot be segmented. Enhanced bowel preparation schemes opacify intraluminal remains to enable colon segmentation. Computed cleansing (as opposed to physical cleansing of the bowels) allows removal of tagged intraluminal remains. This paper describes a model that allows proper classification of transitions between three materials: gas, tissue and tagged intraluminal remains. The computed cleansing effectively detects and removes the remains from the data. Inspection of the ‘clean’ wall is possible using common surface visualization techniques.},
	number = {2879},
	urldate = {2012-11-20},
	booktitle = {Medical {Image} {Computing} and {Computer}-{Assisted} {Intervention} - {MICCAI} 2003},
	publisher = {Springer Berlin Heidelberg},
	author = {Serlie, Iwo and Truyen, Roel and Florie, Jasper and Post, Frits and Vliet, Lucas van and Vos, Frans},
	editor = {Ellis, Randy E. and Peters, Terry M.},
	month = jan,
	year = {2003},
	keywords = {Artificial Intelligence (incl. Robotics), cleansing, colonography, colonoscopy, Computer graphics, Health Informatics, Image Processing and Computer Vision, Imaging / Radiology, Pattern Recognition, virtual endoscopy, volume segmentation},
	pages = {175--183}
}

@article{rezk-salama_opacity_2006,
	title = {Opacity Peeling for Direct Volume Rendering},
	volume = {25},
	issn = {1467-8659},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1467-8659.2006.00979.x/abstract},
	doi = {10.1111/j.1467-8659.2006.00979.x},
	abstract = {The most important technique to visualize {3D} scalar data, as they arise e.g. in medicine from tomographic measurement, is direct volume rendering. A transfer function maps the scalar values to optical properties which are used to solve the integral of light transport in participating media. Many medical data sets, especially {MRI} data, however, are difficult to visualize due to different tissue types being represented by the same scalar value. The main problem is that interesting structures will be occluded by less important structures because they share the same range of data values. Occlusion, however, is a view-dependent problem and cannot be solved easily by transfer function design. This paper proposes a new method to display different entities inside the volume data in a single rendering pass. The proposed opacity peeling technique reveals structures in the data set that cannot be visualized directly by one-or multi-dimensional transfer functions without explicit segmentation. We also demonstrate real-time implementations using texture mapping and multiple render {targets.Categories} and Subject Descriptors (according to {ACM} {CCS):} I.3.7 {[Computer} Graphics]: Three-Dimensional Graphics and Realism},
	language = {en},
	number = {3},
	urldate = {2012-11-20},
	journal = {Computer Graphics Forum},
	author = {Rezk-Salama, Christof and Kolb, Andreas},
	year = {2006},
	pages = {597-606},
	file = {Full Text PDF:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\vuyr0ioc.default\zotero\storage\TQQXJMAD\Rezk-Salama and Kolb - 2006 - Opacity Peeling for Direct Volume Rendering.pdf:application/pdf;Snapshot:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\vuyr0ioc.default\zotero\storage\XFUU898M\abstract.html:text/html}
}

@phdthesis{neophytou_generalized_2006,
	address = {Stony Brook, {NY}, {USA}},
	title = {A generalized framework for interactive volumetric point-based rendering},
	abstract = {Volume Visualization is the process of displaying volumetric data represented as sample points on a regular or irregular {3D} grid. The data is currently produced by medical scanners such as {MRI}, {CT}, etc, and by numerical methods such as scientific simulations. Techniques that have been proposed for this purpose over the years include direct volume-rendering which seeks to capture a visual impression of the complete {3D} dataset by accounting for the emission and light absorption effects of all the data elements. This technique is effective when rendering volumes of space-filling gasses or volumes composed of many micro-surfaces, such as tissue in medical datasets. We have focused our efforts on Point-Based Volume rendering and specifically on the image-aligned post-shaded splatting algorithm which was proposed as a remedy to the drawbacks of existing algorithms with special focus on image quality. In the course of this dissertation, we will follow the evolution of this algorithm through several stages of maturity. Our contributions to this algorithm include the ability to render efficient grid topologies with significant storage and rendering time gains for both {3D} as well as time-varying datasets. We have also proposed a post-convolved volume rendering technique to accelerate magnified viewing. The framework has been ported to a hardware-accelerated implementation that has the ability to interactively slice point based volumes and was optimized to take full advantage of the splatting algorithm's inherent advantages for empty-space skipping and early splat elimination. Finally, we have generalized our framework for the interactive rendering of irregular datasets consisting of ellipsoidal kernels of arbitrary size and orientation. Our suggested algorithm outperforms existing hardware approaches by about an order of magnitude in terms of throughput. Different modeling approaches have been explored for encoding the data using either {RBF} (Radial Basis Functions) or {EBF} (Elliptical Basis Functions), and new methods are proposed for ongoing/future work. The final goal is a truly general point-based hardware-accelerated framework for the interactive visualization of both regular and irregular volumetric data in high-fidelity.},
	school = {State University of New York at Stony Brook},
	author = {Neophytou, Neophytos},
	year = {2006},
	note = {{AAI3258861}},
	file = {10.1.1.87.8944.pdf:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\vuyr0ioc.default\zotero\storage\HSDPER48\10.1.1.87.8944.pdf:application/pdf}
}










@inproceedings{kindlmann_semi-automatic_1998,
	title = {Semi-automatic generation of transfer functions for direct volume rendering},
	doi = {10.1109/SVV.1998.729588},
	abstract = {Although direct volume rendering is a powerful tool for visualizing complex structures within volume data, the size and complexity of the parameter space controlling the rendering process makes generating an informative rendering challenging. In particular, the specification of the transfer function-the mapping from data values to renderable optical properties-is frequently a time consuming and unintuitive task. Ideally, the data being visualized should itself suggest an appropriate transfer function that brings out the features of interest without obscuring them with elements of little importance. We demonstrate that this is possible for a large class of scalar volume data, namely that where the regions of interest are the boundaries between different materials. A transfer function which makes boundaries readily visible can be generated from the relationship between three quantities: the data value and its first and second directional derivatives along the gradient direction. A data structure we term the histogram volume captures the relationship between these quantities throughout the volume in a position independent, computationally efficient fashion. We describe the theoretical importance of the quantities measured by the histogram volume, the implementation issues in its calculation, and a method for semiautomatic transfer function generation through its analysis. We conclude with results of the method on both idealized synthetic data as well as real world datasets.},
	booktitle = {{IEEE} Symposium on Volume Visualization, 1998},
	author = {Kindlmann, G. and Durkin, {J.W.}},
	year = {1998},
	keywords = {complex structure visualization, Computer Graphics, data structure, data structures, data values, data visualisation, data visualization, Direct volume rendering, directional derivatives, gradient direction, histogram volume, Histograms, idealized synthetic data, informative rendering, parameter space, Power generation, Process control, real world datasets, renderable optical properties, rendering (computer graphics), rendering process, scalar volume data, semi automatic generation, semiautomatic transfer function generation, Size control, transfer function, transfer functions, unintuitive task, Volume measurement},
	pages = {79--86},
	file = {IEEE Xplore Abstract Record:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\vuyr0ioc.default\zotero\storage\KCAR6R7J\abs_all.html:text/html;IEEE Xplore Full Text PDF:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\vuyr0ioc.default\zotero\storage\Z4775FAJ\Kindlmann and Durkin - 1998 - Semi-automatic generation of transfer functions fo.pdf:application/pdf}
}

@article{bruckner_isosurface_2010,
	title = {Isosurface {Similarity} {Maps}},
	volume = {29},
	copyright = {© 2010 The Author(s) Journal compilation © 2010 The Eurographics Association and Blackwell Publishing Ltd.},
	issn = {1467-8659},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1467-8659.2009.01689.x/abstract},
	doi = {10.1111/j.1467-8659.2009.01689.x},
	abstract = {In this paper, we introduce the concept of isosurface similarity maps for the visualization of volume data. Iso-surface similarity maps present structural information of a volume data set by depicting similarities between individual isosurfaces quantified by a robust information-theoretic measure. Unlike conventional histograms, they are not based on the frequency of isovalues and/or derivatives and therefore provide complementary information. We demonstrate that this new representation can be used to guide transfer function design and visualization parameter specification. Furthermore, we use isosurface similarity to develop an automatic parameter-free method for identifying representative isovalues. Using real-world data sets, we show that isosurface similarity maps can be a useful addition to conventional classification techniques.},
	language = {en},
	number = {3},
	urldate = {2015-11-10},
	journal = {Computer Graphics Forum},
	author = {Bruckner, Stefan and M{\"o}ller, Torsten},
	month = jun,
	year = {2010},
	keywords = {I.3.3 [Computer Graphics]: Picture/Image Generation—Display algorithms},
	pages = {773--782}
}

@inproceedings{bajaj_contour_1997,
	title = {The contour spectrum},
	doi = {10.1109/VISUAL.1997.663875},
	abstract = {The authors introduce the contour spectrum, a user interface component that improves qualitative user interaction and provides real-time exact quantification in the visualization of isocontours. The contour spectrum is a signature consisting of a variety of scalar data and contour attributes, computed over the range of scalar values {Ï‰âˆˆR.} They explore the use of surface, area, volume, and gradient integral of the contour that are shown to be univariate B-spline functions of the scalar value Ï‰ for multi-dimensional unstructured triangular grids. These quantitative properties are calculated in real-time and presented to the user as a collection of signature graphs (plots of functions of Ï‰) to assist in selecting relevant isovalues Ï‰ 0 for informative visualization. For time-varying data, these quantitative properties can also be computed over time, and displayed using a {2D} interface, giving the user an overview of the time-varying function, and allowing interaction in both isovalue and time step. The effectiveness of the current system and potential extensions are discussed.},
	booktitle = {Visualization '97., Proceedings},
	author = {Bajaj, {C.L.} and Bremer, P.-T. and Schikore, {D.R.}},
	year = {1997},
	keywords = {{2D} interface, area, Computer displays, Computer interfaces, contour attributes, contour spectrum, data acquisition, data visualisation, data visualization, gradient integral, Image analysis, informative visualization, isocontour visualization, isovalue selection, Knowledge based systems, multi-dimensional unstructured triangular grids, Postal services, qualitative user interaction, real-time exact quantification, scalar data, signature graphs, Spline, surface, time step, time-varying data, transfer functions, univariate B-spline functions, user interface component, user interfaces, Volume},
	pages = {167--173},
	file = {IEEE Xplore Abstract Record:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\vuyr0ioc.default\zotero\storage\CN2S2H2X\abs_all.html:text/html;IEEE Xplore Full Text PDF:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\vuyr0ioc.default\zotero\storage\NUWPK37M\Bajaj et al. - 1997 - The contour spectrum.pdf:application/pdf}
}

@article{levoy_display_1988,
	title = {Display of surfaces from volume data},
	volume = {8},
	issn = {0272-1716},
	doi = {10.1109/38.511},
	abstract = {The application of volume-rendering techniques to the display of surfaces from sampled scalar functions of three spatial dimensions is discussed. It is not necessary to fit geometric primitives to the sampled data; images are formed by directly shading each sample and projecting it onto the picture plane. Surface-shading calculations are performed at every voxel with local gradient vectors serving as surface normals. In a separate step, surface classification operators are applied to compute a partial opacity of every voxel. Operators that detect isovalue contour surfaces and region boundary surfaces are examined. The technique is simple and fast, yet displays surfaces exhibiting smooth silhouettes and few other aliasing artifacts. The use of selective blurring and supersampling to further improve image quality is described. Examples from molecular graphics and medical imaging are given.{\textless}{\textgreater}},
	number = {3},
	journal = {{IEEE} Computer Graphics and Applications},
	author = {Levoy, M.},
	year = {1988},
	keywords = {Application software, Biomedical imaging, Computed tomography, Computer Graphics, data visualization, Detectors, Displays, isovalue contour surfaces, medical imaging, molecular graphics, picture plane, region boundary surfaces, rendering (computer graphics), smooth silhouettes, surface classification, surface fitting, Surface treatment, Volume Data, volume-rendering},
	pages = {29--37},
	file = {IEEE Xplore Abstract Record:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\vuyr0ioc.default\zotero\storage\5JJN2BCW\abs_all.html:text/html;IEEE Xplore Full Text PDF:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\vuyr0ioc.default\zotero\storage\QIWVZ394\Levoy - 1988 - Display of surfaces from volume data.pdf:application/pdf}
}

@inproceedings{kniss_interactive_2001,
	address = {Washington, {DC}, {USA}},
	series = {{VIS} '01},
	title = {Interactive volume rendering using multi-dimensional transfer functions and direct manipulation widgets},
	isbn = {0-7803-7200-X},
	url = {http://dl.acm.org/citation.cfm?id=601671.601711},
	abstract = {Most direct volume renderings produced today employ one-dimensional transfer functions, which assign color and opacity to the volume based solely on the single scalar quantity which comprises the dataset. Though they have not received widespread attention, multi-dimensional transfer functions are a very effective way to extract specific material boundaries and convey subtle surface properties. However, identifying good transfer functions is difficult enough in one dimension, let alone two or three dimensions. This paper demonstrates an important class of three-dimensional transfer functions for scalar data (based on data value, gradient magnitude, and a second directional derivative), and describes a set of direct manipulation widgets which make specifying such transfer functions intuitive and convenient. We also describe how to use modern graphics hardware to interactively render with multi-dimensional transfer functions. The transfer functions, widgets, and hardware combine to form a powerful system for interactive volume exploration.},
	urldate = {2013-05-07},
	booktitle = {Proceedings of the conference on Visualization '01},
	publisher = {{IEEE} Computer Society},
	author = {Kniss, Joe and Kindlmann, Gordon and Hansen, Charles},
	year = {2001},
	keywords = {direct manipulation widgets, Direct volume rendering, graphics hardware, Multi-Dimensional Transfer Functions, Volume Visualization},
	pages = {255-262},
	file = {ACM Full Text PDF:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\vuyr0ioc.default\zotero\storage\NJ6XH437\Kniss et al. - 2001 - Interactive volume rendering using multi-dimension.pdf:application/pdf}
}

@article{pfister_transfer_2001,
	title = {The transfer function bake-off},
	volume = {21},
	issn = {0272-1716},
	doi = {10.1109/38.920623},
	abstract = {Direct volume rendering is a key technology for visualizing large {3D} data sets from scientific or medical applications. Transfer functions are particularly important to the quality of direct volume-rendered images. A transfer function assigns optical properties, such as color and opacity, to original values of the data set being visualized. Unfortunately, finding good transfer functions proves difficult. It is one of the major problems in volume visualization. The article examines four of the currently most promising approaches to transfer function design. The four approaches are: trial and error, with minimum computer aid; data-centric, with no underlying assumed model; data-centric, using an underlying data model; and image-centric, using organized sampling},
	number = {3},
	journal = {{IEEE} Computer Graphics and Applications},
	author = {Pfister, H. and Lorensen, B. and Bajaj, C. and Kindlmann, G. and Schroeder, W. and Avila, {L.S.} and Raghu, {K.M.} and Machiraju, R. and Lee, Jinho},
	year = {2001},
	keywords = {Color, Computer errors, data visualisation, data visualization, data-centric, Direct volume rendering, direct volume-rendered images, Heart, Image generation, image-centric, Isosurfaces, Knee, large {3D} data set visualization, Magnetic Resonance Imaging, opacity, optical properties, organized sampling, rendering (computer graphics), Teeth, transfer function, transfer functions, trial and error, underlying data model, Volume Visualization},
	pages = {16--22},
	file = {IEEE Xplore Abstract Record:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\vuyr0ioc.default\zotero\storage\4JF8848P\abs_all.html:text/html;IEEE Xplore Full Text PDF:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\vuyr0ioc.default\zotero\storage\RZ6HNR3G\Pfister et al. - 2001 - The transfer function bake-off.pdf:application/pdf}
}

@article{kniss_multidimensional_2002,
	title = {Multidimensional Transfer Functions for Interactive Volume Rendering},
	volume = {8},
	issn = {1077-2626},
	url = {http://dx.doi.org/10.1109/TVCG.2002.1021579},
	doi = {10.1109/TVCG.2002.1021579},
	abstract = {Most direct volume renderings produced today employ one-dimensional transfer functions which assign color and opacity to the volume based solely on the single scalar quantity which comprises the data set. Though they have not received widespread attention, multidimensional transfer functions are a very effective way to extract materials and their boundaries for both scalar and multivariate data. However, identifying good transfer functions is difficult enough in one dimension, let alone two or three dimensions. This paper demonstrates an important class of three-dimensional transfer functions for scalar data, and describes the application of multidimensional transfer functions to multivariate data. We present a set of direct manipulation widgets that make specifying such transfer functions intuitive and convenient. We also describe how to use modern graphics hardware to both interactively render with multidimensional transfer functions and to provide interactive shadows for volumes. The transfer functions, widgets, and hardware combine to form a powerful system for interactive volume exploration.},
	number = {3},
	urldate = {2013-05-01},
	journal = {{IEEE} Transactions on Visualization and Computer Graphics},
	author = {Kniss, Joe and Kindlmann, Gordon and Hansen, Charles},
	month = jul,
	year = {2002},
	keywords = {direct manipulation widgets, Direct volume rendering, graphics hardware., multidimensional transfer functions, Volume Visualization},
	pages = {270-285},
	file = {kniss_tvcg02-small.pdf:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\vuyr0ioc.default\zotero\storage\9UNIT82H\kniss_tvcg02-small.pdf:application/pdf}
}

@inproceedings{jankun-kelly_study_2001,
	address = {Aire-la-Ville, Switzerland, Switzerland},
	series = {{VG'01}},
	title = {A study of transfer function generation for time-varying volume data},
	isbn = {3-211-83737-X},
	url = {http://dx.doi.org/10.2312/VG/VG01/051-066},
	doi = {10.2312/VG/VG01/051-066},
	abstract = {The proper usage and creation of transfer functions for time-varying data sets is an often ignored problem in volume visualization. Although methods and guidelines exist for time-invariant data, little formal study for the timevarying case has been performed. This paper examines this problem, and reports the study that we have conducted to determine how the dynamic behavior of time-varying data may be captured by a single or small set of transfer functions. The criteria which dictate when more than one transfer function is needed were also investigated. Four data sets with different temporal characteristics were used for our study. Results obtained using two different classes of methods are discussed, along with lessons learned. These methods, including a new multiresolution opacity map approach, can be used for semi-automatic generation of transfer functions to explore large-scale time-varying data sets.},
	urldate = {2012-11-06},
	booktitle = {Proceedings of the 2001 Eurographics conference on Volume Graphics},
	publisher = {Eurographics Association},
	author = {Jankun-Kelly, T. J. and Ma, Kwan-Liu},
	year = {2001},
	pages = {51-66},
	file = {10.1.1.64.8722.pdf:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\vuyr0ioc.default\zotero\storage\37SIAVEV\10.1.1.64.8722.pdf:application/pdf}
}

@article{woodring_semi-automatic_2009,
	title = {Semi-Automatic Time-Series Transfer Functions via Temporal Clustering and Sequencing},
	volume = {28},
	copyright = {Â© 2009 The Author(s) Journal compilation Â© 2009 The Eurographics Association and Blackwell Publishing Ltd.},
	issn = {1467-8659},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1467-8659.2009.01472.x/abstract},
	doi = {10.1111/j.1467-8659.2009.01472.x},
	abstract = {When creating transfer functions for time-varying data, it is not clear what range of values to use for classification, as data value ranges and distributions change over time. In order to generate time-varying transfer functions, we search the data for classes that have similar behavior over time, assuming that data points that behave similarly belong to the same feature. We utilize a method we call temporal clustering and sequencing to find dynamic features in value space and create a corresponding transfer function. First, clustering finds groups of data points that have the same value space activity over time. Then, sequencing derives a progression of clusters over time, creating chains that follow value distribution changes. Finally, the cluster sequences are used to create transfer functions, as sequences describe the value range distributions over time in a data set.},
	language = {en},
	number = {3},
	urldate = {2012-11-07},
	journal = {Computer Graphics Forum},
	author = {Woodring, Jonathan and Shen, Han-Wei},
	year = {2009},
	keywords = {[I.3.7]:, Applications, Computer, Graphics},
	pages = {791-798},
	file = {Full Text PDF:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\vuyr0ioc.default\zotero\storage\J2BICZDM\Woodring and Shen - 2009 - Semi-Automatic Time-Series Transfer Functions via .pdf:application/pdf;Snapshot:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\vuyr0ioc.default\zotero\storage\ZM4UWMH9\abstract.html:text/html}
}

@article{nguyen_clustering-based_2012,
	title = {A clustering-based system to automate transfer function design for medical image visualization},
	volume = {28},
	issn = {0178-2789, 1432-2315},
	url = {http://link.springer.com/article/10.1007/s00371-011-0634-3},
	doi = {10.1007/s00371-011-0634-3},
	abstract = {Finding good transfer functions for rendering medical volumes is difficult, non-intuitive, and time-consuming. We introduce a clustering-based framework for the automatic generation of transfer functions for volumetric data. The system first applies mean shift clustering to oversegment the volume boundaries according to their low-high ({LH)} values and their spatial coordinates, and then uses hierarchical clustering to group similar voxels. A transfer function is then automatically generated for each cluster such that the number of occlusions is reduced. The framework also allows for semi-automatic operation, where the user can vary the hierarchical clustering results or the transfer functions generated. The system improves the efficiency and effectiveness of visualizing medical images and is suitable for medical imaging applications.},
	language = {en},
	number = {2},
	urldate = {2012-11-15},
	journal = {The Visual Computer},
	author = {Nguyen, Binh P. and Tay, Wei-Liang and Chui, Chee-Kong and Ong, Sim-Heng},
	month = feb,
	year = {2012},
	keywords = {Artificial Intelligence (incl. Robotics), Clustering, Computer Graphics, Computer Science, general, Image Processing and Computer Vision, {LH} histogram, Transfer function design, volume rendering},
	pages = {181--191},
	file = {Full Text PDF:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\vuyr0ioc.default\zotero\storage\FKFK3BTD\Nguyen et al. - 2012 - A clustering-based system to automate transfer fun.pdf:application/pdf;Snapshot:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\vuyr0ioc.default\zotero\storage\N34J54PI\s00371-011-0634-3.html:text/html}
}

@inproceedings{brambilla_illustrative_2012,
	title = {Illustrative Flow Visualization: State of the Art, Trends and Challenges},
	location = {Cagliari, Italy},
	url = {http://diglib.eg.org/EG/DL/conf/EG2012/stars/075-094.pdf},
	doi = {10.2312/conf/EG2012/stars/075-094},
	abstract = {Flow visualization is a well established branch of scientific visualization and it currently represents an invaluable resource to many fields, like automotive design, meteorology and medical imaging. Thanks to the capabilities of modern hardware, flow datasets are increasing in size and complexity, and traditional flow visualization techniques need to be updated and improved in order to deal with the upcoming challenges. A fairly recent trend to enhance the expressiveness of scientific visualization is to produce depictions of physical phenomena taking inspiration from traditional handcrafted illustrations: this approach is known as illustrative visualization, and it is getting a foothold in flow visualization as well. In this state of the art report we give an overview of the existing illustrative techniques for flow visualization, we highlight which problems have been solved and which issues still need further investigation, and, finally, we provide remarks and insights on the current trends in illustrative flow visualization.},
	booktitle = {{EuroGraphics} 2012 State of the Art Reports ({STARs)}},
	author = {Brambilla, Andrea and Carnecky, Robert and Peikert, Ronald and Viola, Ivan and Hauser, Helwig},
	year = {2012},
	pages = {75-94},
	file = {Brambilla12Illustrative.pdf:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\vuyr0ioc.default\zotero\storage\K8Q6HABQ\Brambilla12Illustrative.pdf:application/pdf;Brambilla12Illustrative.pptx:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\vuyr0ioc.default\zotero\storage\6724H32P\Brambilla12Illustrative.pptx:application/vnd.openxmlformats-officedocument.presentationml.presentation}
}

@article{viola_importance-driven_2005,
	title = {Importance-Driven Feature Enhancement in Volume Visualization},
	volume = {11},
	issn = {1077-2626},
	url = {http://dx.doi.org/10.1109/TVCG.2005.62},
	doi = {10.1109/TVCG.2005.62},
	abstract = {This paper presents importance-driven feature enhancement as a technique for the automatic generation of cut-away and ghosted views out of volumetric data. The presented focus+context approach removes or suppresses less important parts of a scene to reveal more important underlying information. However, less important parts are fully visible in those regions, where important visual information is not lost, i.e., more relevant features are not occluded. Features within the volumetric data are first classified according to a new dimension, denoted as object importance. This property determines which structures should be readily discernible and which structures are less important. Next, for each feature, various representations (levels of sparseness) from a dense to a sparse depiction are defined. Levels of sparseness define a spectrum of optical properties or rendering styles. The resulting image is generated by ray-casting and combining the intersected features proportional to their importance (importance compositing). The paper includes an extended discussion on several possible schemes for levels of sparseness specification. Furthermore, different approaches to importance compositing are treated.},
	number = {4},
	urldate = {2013-05-12},
	journal = {{IEEE} Transactions on Visualization and Computer Graphics},
	author = {Viola, Ivan and Kanitsar, Armin and Gr{\"o}ller, M. Eduard},
	month = jul,
	year = {2005},
	keywords = {Algorithms, Biomedical imaging, biomedical optical imaging, Computer Graphics, Computer Simulation, data visualisation, Data visualization, feature extraction, {Focus+Context} Techniques, focus-context approach, Focusing, hidden feature removal, illustrative techniques., image enhancement, Image Interpretation, Computer-Assisted, Imaging, Three-Dimensional, importance-driven feature enhancement, Index Terms- View-dependent visualization, Layout, Lesions, level-of-detail techniques, Liver neoplasms, Medical diagnostic imaging, Models, Biological, Numerical Analysis, Computer-Assisted, Online Systems, Pattern Recognition, Automated, ray tracing, ray-casting, rendering (computer graphics), shape, sparseness specification, User-Computer Interface, visual databases, volume rendering, Volume Visualization},
	pages = {408-418},
	file = {01432686.pdf:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\t929e2iy.default\\zotero\\storage\\TXVS4V7B\\01432686.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\t929e2iy.default\\zotero\\storage\\REKRXQ25\\articleDetails.html:text/html}
}

@inproceedings{stompel_visualization_2002,
	address = {Washington, {DC}, {USA}},
	series = {{PG} '02},
	title = {Visualization of Multidimensional, Multivariate Volume Data Using Hardware-Accelerated Non-Photorealistic Rendering Techniques},
	isbn = {0-7695-1784-6},
	url = {http://dl.acm.org/citation.cfm?id=826030.826635},
	abstract = {This paper presents a set of feature enhancement techniques coupled with hardware-accelerated non-photorealistic rendering for generating more perceptually effective visualizations of multidimensional, multivariate volume data, such as those obtained from typical computational fluid dynamics simulations. For time-invariant data, one or more variables are used to either highlight important features in another variable, or add contextural information to the visualization. For time-varying data, rendering of each time step also takes into account the values at neighboring time steps to reinforce the perception of the changingfeatures in the data over time. With hardware-accelerated rendering, interactive visualization becomes possible leading to increased explorability and comprehension of the data.},
	urldate = {2013-05-12},
	booktitle = {Proceedings of the 10th Pacific Conference on Computer Graphics and Applications},
	publisher = {{IEEE} Computer Society},
	author = {Stompel, Aleksander and Lum, Eric B. and Ma, Kwan-Liu},
	year = {2002},
	keywords = {animation, hardware-accelerated rendering, multidimensional data, multivariate data, non-photorealistic rendering, scientific visualization, streamlines, stroke based rendering, vector field, Visual Perception, volume rendering},
	pages = {394},
	file = {FeatureEnhancedVisualizationofMultidimensionalMultivariateVolumeDataUsingNonphotorealisticRenderingTechniques.pdf:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\vuyr0ioc.default\zotero\storage\NNHBN7P3\FeatureEnhancedVisualizationofMultidimensionalMultivariateVolumeDataUsingNonphotorealisticRenderingTechniques.pdf:application/pdf}
}

@article{maciejewski_structuring_2009,
	title = {Structuring Feature Space: A Non-Parametric Method for Volumetric Transfer Function Generation},
	volume = {15},
	issn = {1077-2626},
	shorttitle = {Structuring Feature Space},
	doi = {10.1109/TVCG.2009.185},
	abstract = {The use of multi-dimensional transfer functions for direct volume rendering has been shown to be an effective means of extracting materials and their boundaries for both scalar and multivariate data. The most common multi-dimensional transfer function consists of a two-dimensional ({2D)} histogram with axes representing a subset of the feature space (e.g., value vs. value gradient magnitude), with each entry in the {2D} histogram being the number of voxels at a given feature space pair. Users then assign color and opacity to the voxel distributions within the given feature space through the use of interactive widgets (e.g., box, circular, triangular selection). Unfortunately, such tools lead users through a trial-and-error approach as they assess which data values within the feature space map to a given area of interest within the volumetric space. In this work, we propose the addition of non-parametric clustering within the transfer function feature space in order to extract patterns and guide transfer function generation. We apply a non-parametric kernel density estimation to group voxels of similar features within the {2D} histogram. These groups are then binned and colored based on their estimated density, and the user may interactively grow and shrink the binned regions to explore feature boundaries and extract regions of interest. We also extend this scheme to temporal volumetric data in which time steps of {2D} histograms are composited into a histogram volume. A three-dimensional ({3D)} density estimation is then applied, and users can explore regions within the feature space across time without adjusting the transfer function at each time step. Our work enables users to effectively explore the structures found within a feature space of the volume and provide a context in which the user can understand how these structures relate to their volumetric data. We provide tools for enhanced exploration and manipulation of the transfer function, and we show that the initial t ransfer function generation serves as a reasonable base for volumetric rendering, reducing the trial-and-error overhead typically found in transfer function design.},
	number = {6},
	journal = {{IEEE} Transactions on Visualization and Computer Graphics},
	author = {Maciejewski, R. and Woo, Insoo and Chen, Wei and Ebert, {D.S.}},
	year = {2009},
	keywords = {Algorithms, Cluster Analysis, colour graphics, Computer Graphics, data mining, Diagnostic Imaging, Direct volume rendering, feature extraction, Histograms, Humans, Image Processing, Computer-Assisted, Kernel, kernel density estimation, Multi-Dimensional Transfer Functions, nonparametric clustering, rendering (computer graphics), Shape, Statistics, Nonparametric, temporal volume rendering, three-dimensional density estimation, Transfer function design, transfer function feature space, transfer functions, two-dimensional histogram, volume rendering, volumetric transfer function generation},
	pages = {1473--1480},
	file = {IEEE Xplore Abstract Record:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\45J3J883\articleDetails.html:text/html;IEEE Xplore Full Text PDF:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\NZ5KPR4U\Maciejewski et al. - 2009 - Structuring Feature Space A Non-Parametric Method.pdf:application/pdf}
}

@inproceedings{park_multi-dimensional_2004,
	title = {Multi-dimensional transfer functions for interactive {3D} flow visualization},
	doi = {10.1109/PCCGA.2004.1348348},
	abstract = {Transfer functions are a standard technique used in volume rendering to assign color and opacity to a volume of a scalar field. Multidimensional transfer functions ({MDTFs)} have proven to be an effective way to extract specific features with subtle properties. As {3D} texture-based methods gain widespread popularity for the visualization of steady and unsteady flow field data, there is a need to define and apply similar {MDTFs} to interactive {3D} flow visualization. We exploit flow field properties such as velocity, gradient, curl, helicity, and divergence using vector calculus methods to define an {MDTF} that can be used to extract and track features in a flow field. We show how the defined {MDTF} can be applied to interactive {3D} flow visualization by combining them with state-of-the-art texture-based flow visualization of steady and unsteady fields. We demonstrate that {MDTFs} can be used to help alleviate the problem of occlusion, which is one of the main inherent drawbacks of {3D} texture-based flow visualization techniques. In our implementation, we make use of current graphics hardware to obtain interactive frame rates.},
	booktitle = {12th Pacific Conference on Computer Graphics and Applications, 2004. {PG} 2004. Proceedings},
	author = {Park, {S.W.} and Budge, B. and Linsen, L. and Hamann, B. and Joy, {K.I.}},
	month = oct,
	year = {2004},
	keywords = {data visualisation, digital simulation, feature extraction, feature tracking, flow visualisation, hidden feature removal, image texture, interactive {3D} flow visualization, interactive systems, multidimensional transfer functions, rendering (computer graphics), texture-based flow visualization, transfer functions, vector calculus, volume rendering},
	pages = {177 -- 185},
	file = {01348348.pdf:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\vuyr0ioc.default\zotero\storage\EI5APGPU\01348348.pdf:application/pdf;IEEE Xplore Abstract Record:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\vuyr0ioc.default\zotero\storage\TII5GVKZ\abs_all.html:text/html}
}

@article{wang_automating_2012,
	title = {Automating {Transfer} {Function} {Design} with {Valley} {Cell}-{Based} {Clustering} of 2D {Density} {Plots}},
	volume = {31},
	copyright = {© 2012 The Author(s) Computer Graphics Forum © 2012 The Eurographics Association and Blackwell Publishing Ltd.},
	issn = {1467-8659},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1467-8659.2012.03122.x/abstract},
	doi = {10.1111/j.1467-8659.2012.03122.x},
	abstract = {Two-dimensional transfer functions are an effective and well-accepted tool in volume classification. The design of them mostly depends on the user's experience and thus remains a challenge. Therefore, we present an approach in this paper to automate the transfer function design based on 2D density plots. By exploiting their smoothness, we adopted the Morse theory to automatically decompose the feature space into a set of valley cells. We design a simplification process based on cell separability to eliminate cells which are mainly caused by noise in the original volume data. Boundary persistence is first introduced to measure the separability between adjacent cells and to suitably merge them. Afterward, a reasonable classification result is achieved where each cell represents a potential feature in the volume data. This classification procedure is automatic and facilitates an arbitrary number and shape of features in the feature space. The opacity of each feature is determined by its persistence and size. To further incorporate the user's prior knowledge, a hierarchical feature representation is created by successive merging of the cells. With this representation, the user is allowed to merge or split features of interest and set opacity and color freely. Experiments on various volumetric data sets demonstrate the effectiveness and usefulness of our approach in transfer function generation.},
	language = {en},
	number = {3pt4},
	urldate = {2015-05-14},
	journal = {Computer Graphics Forum},
	author = {Wang, Yunhai and Zhang, Jian and Lehmann, Dirk J. and Theisel, Holger and Chi, Xuebin},
	month = jun,
	year = {2012},
	keywords = {I.3.3 [Computer Graphics]: Picture/Image Generation—Line and curve generation},
	pages = {1295--1304}
}

@inproceedings{svakhine_illustration_2005,
	title = {Illustration and photography inspired visualization of flows and volumes},
	doi = {10.1109/VISUAL.2005.1532858},
	abstract = {Understanding and analyzing complex volumetrically varying data is a difficult problem. Many computational visualization techniques have had only limited success in succinctly portraying the structure of three-dimensional turbulent flow. Motivated by both the extensive history and success of illustration and photographic flow visualization techniques, we have developed a new interactive volume rendering and visualization system for flows and volumes that simulates and enhances traditional illustration, experimental advection, and photographic flow visualization techniques. Our system uses a combination of varying focal and contextual illustrative styles, new advanced two-dimensional transfer functions, enhanced Schlieren and shadowgraphy shaders, and novel oriented structure enhancement techniques to allow interactive visualization, exploration, and comparative analysis of scalar, vector, and time-varying volume datasets. Both traditional illustration techniques and photographic flow visualization techniques effectively reduce visual clutter by using compact oriented structure information to convey three-dimensional structures. Therefore, a key to the effectiveness of our system is using one-dimensional (Schlieren and shadowgraphy) and two-dimensional (silhouette) oriented structural information to reduce visual clutter, while still providing enough three-dimensional structural information for the user's visual system to understand complex three-dimensional flow data. By combining these oriented feature visualization techniques with flexible transfer function controls, we can visualize scalar and vector data, allow comparative visualization of flow properties in a succinct, informative manner, and provide continuity for visualizing time-varying datasets.},
	booktitle = {{IEEE} Visualization, 2005. {VIS} 05},
	author = {Svakhine, {N.A.} and Jang, Yun and Ebert, D. and Gaither, K.},
	month = oct,
	year = {2005},
	keywords = {Color, Computer Graphics, contextual illustrative styles, data visualisation, data visualization, Electric shock, flow visualisation, interactive systems, Interactive Volume Rendering, photographic flow visualization techniques, photography, rendering (computer graphics), schlieren systems, shadowgraphy shaders, Space vehicles, Stress, structure enhancement techniques, Temperature, three-dimensional turbulent flow, time-varying volume datasets, transfer functions, turbulence, two-dimensional transfer functions, visual clutter, Visual System},
	pages = {687--694},
	file = {IEEE Xplore Abstract Record:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\vuyr0ioc.default\zotero\storage\VEFJ4TW7\abs_all.html:text/html;IEEE Xplore Full Text PDF:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\vuyr0ioc.default\zotero\storage\8WSWHD43\Svakhine et al. - Illustration and photography inspired visualizatio.pdf:application/pdf}
}

@article{joshi_case_2009,
	title = {Case Study on Visualizing Hurricanes Using Illustration-Inspired Techniques},
	volume = {15},
	issn = {1077-2626},
	doi = {10.1109/TVCG.2008.105},
	abstract = {The devastating power of hurricanes was evident during the 2005 hurricane season, the most active season on record. This has prompted increased efforts by researchers to understand the physical processes that underlie the genesis, intensification, and tracks of hurricanes. This research aims at facilitating an improved understanding into the structure of hurricanes with the aid of visualization techniques. Our approach was developed by a mixed team of visualization and domain experts. To better understand these systems, and to explore their representation in {NWP} models, we use a variety of illustration-inspired techniques to visualize their structure and time evolution. Illustration-inspired techniques aid in the identification of the amount of vertical wind shear in a hurricane, which can help meteorologists predict dissipation. Illustration-style visualization, in combination with standard visualization techniques, helped explore the vortex rollup phenomena and the mesovortices contained within. We evaluated the effectiveness of our visualization with the help of six hurricane experts. The expert evaluation showed that the illustration-inspired techniques were preferred over existing tools. Visualization of the evolution of structural features is a prelude to a deeper visual analysis of the underlying dynamics.},
	number = {5},
	journal = {{IEEE} Transactions on Visualization and Computer Graphics},
	author = {Joshi, A. and Caban, J. and Rheingans, P. and Sparling, L.},
	month = oct,
	year = {2009},
	keywords = {data visualisation, geophysics computing, hurricanes, illustration-inspired visualization, illustration-style visualization, mesovortice, meteorology, {NWP} model, storms, visual analysis, vortex rollup phenomena},
	pages = {709 --718},
	file = {IEEE Xplore Abstract Record:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\vuyr0ioc.default\zotero\storage\TFEPHQTN\articleDetails.html:text/html;IEEE Xplore Full Text PDF:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\vuyr0ioc.default\zotero\storage\ZZ327BHR\Joshi et al. - 2009 - Case Study on Visualizing Hurricanes Using Illustr.pdf:application/pdf}
}

@article{joshi_evaluation_2008,
	title = {Evaluation of illustration-inspired techniques for time-varying data visualization},
	volume = {27},
	copyright = {Â© 2008 The Author(s) Journal compilation Â© 2008 The Eurographics Association and Blackwell Publishing Ltd.},
	issn = {1467-8659},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1467-8659.2008.01235.x/abstract},
	doi = {10.1111/j.1467-8659.2008.01235.x},
	abstract = {Illustration-inspired techniques have provided alternative ways to visualize time-varying data. Techniques such as speedlines, flow ribbons, strobe silhouettes and opacity-based techniques provide temporal context to the current timestep being visualized. We evaluated the effectiveness of these illustrative techniques by conducting a user study. We compared the ability of subjects to visually track features using snapshots, snapshots augmented by illustration techniques, animations, and animations augmented by illustration techniques. User accuracy, time required to perform a task, and user confidence were used as measures to evaluate the techniques. The results indicate that the use of illustration-inspired techniques provides a significant improvement in user accuracy and the time required to complete the task. Subjects performed significantly better on each metric when using augmented animations as compared to augmented snapshots.},
	language = {en},
	number = {3},
	urldate = {2013-05-08},
	journal = {Computer Graphics Forum},
	author = {Joshi, Alark and Rheingans, Penny},
	year = {2008},
	keywords = {[Human, factors]:, H.1.2},
	pages = {999-1006},
	file = {Full Text PDF:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\vuyr0ioc.default\zotero\storage\ZA5NM26G\Joshi and Rheingans - 2008 - Evaluation of illustration-inspired techniques for.pdf:application/pdf;Snapshot:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\vuyr0ioc.default\zotero\storage\ZKSGD5UU\abstract.html:text/html}
}

@inproceedings{joshi_illustration-inspired_2005,
	title = {Illustration-inspired techniques for visualizing time-varying data},
	doi = {10.1109/VISUAL.2005.1532857},
	abstract = {Traditionally, time-varying data has been visualized using snapshots of the individual time steps or an animation of the snapshots shown in a sequential manner. For larger datasets with many time-varying features, animation can be limited in its use, as an observer can only track a limited number of features over the last few frames. Visually inspecting each snapshot is not practical either for a large number of time-steps. We propose new techniques inspired from the illustration literature to convey change over time more effectively in a time-varying dataset. Speedlines are used extensively by cartoonists to convey motion, speed, or change over different panels. Flow ribbons are another technique used by cartoonists to depict motion in a single frame. Strobe silhouettes are used to depict previous positions of an object to convey the previous positions of the object to the user. These illustration-inspired techniques can be used in conjunction with animation to convey change over time.},
	booktitle = {{IEEE} Visualization, 2005. {VIS} 05},
	author = {Joshi, A. and Rheingans, P.},
	month = oct,
	year = {2005},
	keywords = {Algorithm design and analysis, animation, Birds, computer animation, data analysis, data mining, data visualisation, data visualization, feature extraction, flow ribbons, Humans, illustration-inspired techniques, motion estimation, rendering (computer graphics), strobe silhouettes, time-varying data visualization, Ultrasonic imaging, weather forecasting},
	pages = {679 -- 686},
	file = {IEEE Xplore Abstract Record:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\vuyr0ioc.default\zotero\storage\3SHN97IQ\abs_all.html:text/html;IEEE Xplore Full Text PDF:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\vuyr0ioc.default\zotero\storage\TFVW9N94\Joshi and Rheingans - 2005 - Illustration-inspired techniques for visualizing t.pdf:application/pdf}
}

@inproceedings{annen_vector_2008,
	address = {Toronto, Ont., Canada, Canada},
	series = {{GI} '08},
	title = {Vector {Field} {Contours}},
	isbn = {978-1-56881-423-0},
	url = {http://dl.acm.org/citation.cfm?id=1375714.1375731},
	abstract = {We describe an approach to define contours of 3D vector fields and employ them as an interactive flow visualization tool. Although contours are well-defined and commonly used for surfaces and 3D scalar fields, they have no straightforward extension in vector fields. Our approach is to extract and visualize specific stream lines which show the most similar behavior to contours on surfaces. This way, the vector field contours are a particular set of isolated stream line segments that depend on the view direction and few additional parameters. We present an analysis of the usefulness of vector field contours by demonstrating their application to linear vector fields. In order to achieve interactive visualization, we develop an efficient GPU-based implementation for real-time extraction and rendering of vector field contours. We show the potential of our approach by applying it to a number of example data sets.},
	urldate = {2015-11-12},
	booktitle = {Proceedings of {Graphics} {Interface} 2008},
	publisher = {Canadian Information Processing Society},
	author = {Annen, T. and Theisel, H. and R{\"o}ssl, C. and Ziegler, G. and Seidel, H.-P.},
	year = {2008},
	pages = {97--105}
}

@article{chen_illustrative_2011,
	title = {An {Illustrative} {Visualization} {Framework} for 3D {Vector} {Fields}},
	volume = {30},
	issn = {1467-8659},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1467-8659.2011.02064.x/abstract},
	doi = {10.1111/j.1467-8659.2011.02064.x},
	abstract = {Most 3D vector field visualization techniques suffer from the problem of visual clutter, and it remains a challenging task to effectively convey both directional and structural information of 3D vector fields. In this paper, we present a novel visualization framework that combines the advantages of clustering methods and illustrative rendering techniques to generate a concise and informative depiction of complex flow structures. Given a 3D vector field, we first generate a number of streamlines covering the important regions based on an entropy measurement. Then we decompose the streamlines into different groups based on a categorization of vector information, wherein the streamline pattern in each group is ensured to be coherent or nearly coherent. For each group, we select a set of representative streamlines and render them in an illustrative fashion to enhance depth cues and succinctly show local flow characteristics. The results demonstrate that our approach can generate a visualization that is relatively free of visual clutter while facilitating perception of salient information of complex vector fields.},
	language = {en},
	number = {7},
	urldate = {2016-09-05},
	journal = {Computer Graphics Forum},
	author = {Chen, Cheng-Kai and Yan, Shi and Yu, Hongfeng and Max, Nelson and Ma, Kwan-Liu},
	month = sep,
	year = {2011},
	keywords = {I.3.3 [Computer Graphics]: Picture/Image Generation —Line and curve generation},
	pages = {1941--1951},
	file = {Snapshot:C\:\\Users\\lazaro\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\9kdnyu8g.default\\zotero\\storage\\D5CMU2VC\\abstract.html:text/html}
}

@inproceedings{ma_coherent_2013,
	title = {Coherent view-dependent streamline selection for importance-driven flow visualization},
	volume = {8654},
	url = {http://dx.doi.org/10.1117/12.2001887},
	doi = {10.1117/12.2001887},
	abstract = {Streamline visualization can be formulated as the problem of streamline placement or streamline selection. In this
	paper, we present an importance-driven approach to view-dependent streamline selection that guarantees coherent
	streamline update when the view changes gradually. Given a large number of randomly or uniformly seeded
	and traced streamlines and sample viewpoints, our approach evaluates, for each streamline, the view-dependent
	importance by considering the amount of information shared by the 3D streamline and its 2D projection as well as
	how stereoscopic the streamline’s shape is reflected under each viewpoint. We achieve coherent view-dependent
	streamline selection following a two-pass solution that considers i) the relationships between local viewpoints
	and the global streamline set selected in a view-independent manner and ii) the continuity between adjacent
	viewpoints. We demonstrate the effectiveness of our approach with several synthesized and simulated flow fields
	and compare our view-dependent streamline selection algorithm with a naïve algorithm that selects streamlines
	solely based on the information at the current viewpoint.},
	urldate = {2016-09-09},
	author = {Ma, Jun and Wang, Chaoli and Shene, Ching-Kuang},
	year = {2013},
	pages = {865407--865407--15}
}

@inproceedings{kuhn_clustering-based_2011,
	title = {A Clustering-based Visualization Technique to Emphasize Meaningful Regions of Vector Fields},
	booktitle = {Proc. of Vision, Modeling, and Visualization ({VMV} 2011)},
	publisher = {Eurographics Assosciation},
	author = {Kuhn, A. and Lehmann, D. J. and Gaststeiger, R. and Neugebauer, Matthias and Preim, B. and Theisel, H.},
	year = {2011},
	pages = {191-198},
	file = {Kuhn_2011_VMV.pdf:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\vuyr0ioc.default\zotero\storage\TPQ37S7P\Kuhn_2011_VMV.pdf:application/pdf}
}

@inproceedings{mattausch_strategies_2003,
	address = {New York, {NY}, {USA}},
	series = {{SCCG} '03},
	title = {Strategies for interactive exploration of {3D} flow using evenly-spaced illuminated streamlines},
	isbn = {1-58113-861-X},
	url = {http://doi.acm.org/10.1145/984952.984987},
	doi = {10.1145/984952.984987},
	abstract = {This paper presents several strategies to interactively explore {3D} flow. Based on a fast illuminated streamlines algorithm, standard graphics hardware is sufficient to gain interactive rendering rates. Our approach does not require the user to have any prior knowledge of flow features. After the streamlines are computed in a short preprocessing time, the user can interactively change appearance and density of the streamlines to further explore the flow. Most important flow features like velocity or pressure not only can be mapped to all available streamline appearance properties like streamline width, material, opacity, but also to streamline density. To improve spatial perception of the {3D} flow we apply techniques based on animation, depth cueing, and halos along a streamline if it is crossed by another streamline in the foreground. Finally, we make intense use of focus+context methods like magic volumes, region of interest driven streamline placing, and spotlights to solve the occlusion problem.},
	urldate = {2013-05-13},
	booktitle = {Proceedings of the 19th spring conference on Computer graphics},
	publisher = {{ACM}},
	author = {Mattausch, Oliver and Theu{\ss}l, Thomas and Hauser, Helwig and Gr{\"o}ller, Eduard},
	year = {2003},
	keywords = {{3D} flow visualization, focus-context visualization, illuminated streamlines, Interactive exploration},
	pages = {213-222},
	file = {ACM Full Text PDF:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\t929e2iy.default\\zotero\\storage\\TWQNURM4\\Mattausch et al. - 2003 - Strategies for interactive exploration of 3D flow .pdf:application/pdf}
}

@inproceedings{zockler_interactive_1996,
	title = {Interactive visualization of {3D-vector} fields using illuminated stream lines},
	doi = {10.1109/VISUAL.1996.567777},
	abstract = {A new technique for interactive vector field visualization using large numbers of properly illuminated stream lines is presented. Taking into account ambient, diffuse, and specular reflection terms as well as transparency, we employ a realistic shading model which significantly increases quality and realism of the resulting images. While many graphics workstations offer hardware support for illuminating surface primitives, usually no means for an accurate shading of line primitives are provided. However, we show that proper illumination of lines can be implemented by exploiting the texture mapping capabilities of modern graphics hardware. In this way high rendering performance with interactive frame rates can be achieved. We apply the technique to render large numbers of integral curves in a vector field. The impression of the resulting images can be further improved by making the curves partially transparent. We also describe methods for controlling the distribution of stream lines in space. These methods enable us to use illuminated stream lines within an interactive visualization environment.},
	booktitle = {Visualization '96. Proceedings.},
	author = {Zockler, M. and Stalling, D. and Hege, H.-C.},
	month = oct,
	year = {1996},
	keywords = {{3D} vector fields, accurate shading, Computer Graphics, data visualisation, graphics workstations, Hardware, high rendering performance, illuminated stream lines, Image generation, integral curves, interactive frame rates, interactive vector field visualization, interactive visualization environment, Layout, lighting, line primitives, modern graphics hardware, Optical reflection, partially transparent curves, realistic shading model, rendering (computer graphics), specular reflection terms, Streaming media, texture mapping capabilities, transparency, vector field, visualization, Workstations},
	pages = {107-113},
	file = {IEEE Xplore Abstract Record:C:\Users\JoeShengzhou\AppData\Roaming\Zotero\Zotero\Profiles\0m7eflhb.default\zotero\storage\5GWZUUZK\abs_all.html:text/html;IEEE Xplore Full Text PDF:C:\Users\JoeShengzhou\AppData\Roaming\Zotero\Zotero\Profiles\0m7eflhb.default\zotero\storage\2QQT93CD\Zockler et al. - 1996 - Interactive visualization of 3D-vector fields usin.pdf:application/pdf}
}

@inproceedings{arens_survey_2010,
	address = {Aire-la-Ville, Switzerland, Switzerland},
	series = {{VG'10}},
	title = {A survey of transfer functions suitable for volume rendering},
	isbn = {978-3-905674-23-1},
	url = {http://dx.doi.org/10.2312/VG/VG10/077-083},
	doi = {10.2312/VG/VG10/077-083},
	abstract = {There are many transfer functions ({TF)} that emphasize or hide special features of volume data. Their potential to cleverly generate a color and opacity value for direct volume rendering is primarily determined by the used metrics besides the input data value. Despite this variety of {TFs}, for the most part simple one dimensional data value only based {TFs} are used in practice. This survey will therefore examine the differences in representative {TF} types (defined by their used metrics) to provide a basis for selecting the right {TF} type for the boundary conditions that describe an individual field of application and task. Besides fundamental properties like metrics or memory consumption, we will also give an assessment about user interaction and quality of feature emphasis.},
	urldate = {2013-05-09},
	booktitle = {Proceedings of the 8th {IEEE/EG} international conference on Volume Graphics},
	publisher = {Eurographics Association},
	author = {Arens, S. and Domik, G.},
	year = {2010},
	pages = {77-83},
	file = {A_Survey_of_Transfer_Functions_suitable_for_Volume_Rendering.pdf:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\vuyr0ioc.default\zotero\storage\QTTQZXG6\A_Survey_of_Transfer_Functions_suitable_for_Volume_Rendering.pdf:application/pdf}
}

@article{bernardon_transfer-function_2008,
	title = {Transfer-Function Specification for Rendering Disparate Volumes},
	volume = {10},
	issn = {1521-9615},
	doi = {10.1109/MCSE.2008.158},
	abstract = {Being able to create insightful visualizations from both simulated and measured data is important for the visualization community. For scalar volumes, direct volume rendering has proved a useful tool for data exploration. Using a transfer function, we can map scalar values to colors and opacities to identify and enhance important features. Although researchers have developed some automatic techniques for transfer-function specification, the exploration process still requires users to tune the parameters manually until they can produce the desired visualization. Researchers have conducted substantial work to assist users in this specification task by providing interactive widgets. These tools generally assist users by letting them create and manipulate widgets over one or more dimensions of histogram information representing the data. The authors' framework combines elements of existing transfer-function specification techniques and introduces new features to handle the direct volume rendering of a diversity of volumetric data sets.},
	number = {6},
	journal = {Computing in Science Engineering},
	author = {Bernardon, {F.F.} and Ha, {L.K.} and Callahan, {S.P.} and Comba, {J.L.D.} and Silva, {C.T.}},
	year = {2008},
	keywords = {Data Exploration, data mining, data representation, data structures, data visualisation, data visualization, Direct volume rendering, Dynamic range, feature extraction, histogram information, Histograms, interactive widget, Medical simulation, Multidimensional systems, rendering (computer graphics), scientific visualization, table lookup, transfer functions, transfer-function specification, volume rendering, volumetric data set},
	pages = {82--89},
	file = {IEEE Xplore Abstract Record:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\vuyr0ioc.default\zotero\storage\8V2HTWWA\articleDetails.html:text/html;IEEE Xplore Full Text PDF:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\vuyr0ioc.default\zotero\storage\WKT9AMQA\Bernardon et al. - 2008 - Transfer-Function Specification for Rendering Disp.pdf:application/pdf}
}

@article{svakhine_illustration-inspired_2009,
	title = {Illustration-Inspired Depth Enhanced Volumetric Medical Visualization},
	volume = {15},
	issn = {1077-2626},
	doi = {http://doi.ieeecomputersociety.org/10.1109/TVCG.2008.56},
	number = {1},
	journal = {{IEEE} Transactions on Visualization and Computer Graphics},
	author = {Svakhine, Nikolai A. and Ebert, David S. and Andrews, William M.},
	year = {2009},
	keywords = {Algorithms, anatomical structure, and texture, Biomedical imaging, biomedical {MRI}, Color, Computer Graphics, computerised tomography, {CT} scanner, data visualisation, data visualization, Eyes, Focusing, graphics processors, illustration-inspired depth enhanced volumetric medical visualization, image enhancement, Image Interpretation, Computer-Assisted, Imaging, Three-Dimensional, interaction techniques, Magnetic Resonance Imaging, Manuals, medical image processing, medical textbook, Medicine, {MRI} scanner, pipeline rendering, Pipelines, rendering (computer graphics), shading, shadowing, spatial relationship, Subtraction Technique, surgery, surgery manual, Tomography, X-Ray Computed, transfer functions, Visualization systems and software, Visualization techniques and methodologies, volume rendering, Volume Visualization, Volumetric, volumetric dataset},
	pages = {77--86},
	file = {IEEE Xplore Abstract Record:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\vuyr0ioc.default\zotero\storage\X8MBR94A\abs_all.html:text/html;IEEE Xplore Full Text PDF:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\vuyr0ioc.default\zotero\storage\SH95V2DT\Svakhine et al. - 2009 - Illustration-Inspired Depth Enhanced Volumetric Me.pdf:application/pdf}
}

@article{post_state_2003,
	title = {The {State} of the {Art} in {Flow} {Visualisation}: {Feature} {Extraction} and {Tracking}},
	volume = {22},
	issn = {1467-8659},
	shorttitle = {The {State} of the {Art} in {Flow} {Visualisation}},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1467-8659.2003.00723.x/abstract},
	doi = {10.1111/j.1467-8659.2003.00723.x},
	abstract = {Flow visualisation is an attractive topic in data visualisation, offering great challenges for research. Very large data sets must be processed, consisting of multivariate data at large numbers of grid points, often arranged in many time steps. Recently, the steadily increasing performance of computers again has become a driving force for new advances in flow visualisation, especially in techniques based on texturing, feature extraction, vector field clustering, and topology extraction.   In this article we present the state of the art in feature-based flow visualisation techniques. We will present numerous feature extraction techniques, categorised according to the type of feature. Next, feature tracking and event detection algorithms are discussed, for studying the evolution of features in time-dependent data sets. Finally, various visualisation techniques are demonstrated.   ACM CSS: I.3.8 Computer Graphics—applications},
	language = {en},
	number = {4},
	urldate = {2016-09-05},
	journal = {Computer Graphics Forum},
	author = {Post, Frits H. and Vrolijk, Benjamin and Hauser, Helwig and Laramee, Robert S. and Doleisch, Helmut},
	month = dec,
	year = {2003},
	keywords = {feature-based flow visualisation, flow visualisation, Visualisation},
	pages = {775--792},
	file = {Snapshot:C\:\\Users\\lazaro\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\9kdnyu8g.default\\zotero\\storage\\XVFFWZEC\\abstract.html:text/html}
}

@article{caban_texture-based_2007,
	title = {Texture-based feature tracking for effective time-varying data visualization},
	volume = {13},
	issn = {1077-2626},
	doi = {10.1109/TVCG.2007.70599},
	abstract = {Analyzing, visualizing, and illustrating changes within time-varying volumetric data is challenging due to the dynamic changes occurring between timesteps. The changes and variations in computational fluid dynamic volumes and atmospheric {3D} datasets do not follow any particular transformation. Features within the data move at different speeds and directions making the tracking and visualization of these features a difficult task. We introduce a texture-based feature tracking technique to overcome some of the current limitations found in the illustration and visualization of dynamic changes within time-varying volumetric data. Our texture-based technique tracks various features individually and then uses the tracked objects to better visualize structural changes. We show the effectiveness of our texture-based tracking technique with both synthetic and real world time-varying data. Furthermore, we highlight the specific visualization, annotation, registration, and feature isolation benefits of our technique. For instance, we show how our texture-based tracking can lead to insightful visualizations of time-varying data. Such visualizations, more than traditional visualization techniques, can assist domain scientists to explore and understand dynamic changes.},
	number = {6},
	journal = {{IEEE} transactions on visualization and computer graphics},
	author = {Caban, Jesus and Joshi, Alark and Rheingans, Penny},
	month = dec,
	year = {2007},
	note = {{PMID:} 17968099},
	pages = {1472--1479},
	file = {tvcg07.pdf:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\EIF6CFCC\tvcg07.pdf:application/pdf}
}

@inproceedings{woodring_high_2003,
	title = {High dimensional direct rendering of time-varying volumetric data},
	doi = {10.1109/VISUAL.2003.1250402},
	abstract = {We present an alternative method for viewing time-varying volumetric data. We consider such data as a four-dimensional data field, rather than considering space and time as separate entities. If we treat the data in this manner, we can apply high dimensional slicing and projection techniques to generate an image hyperplane. The user is provided with an intuitive user interface to specify arbitrary hyperplanes in {4D}, which can be displayed with standard volume rendering techniques. From the volume specification, we are able to extract arbitrary hyperslices, combine slices together into a hyperprojection volume, or apply a {4D} raycasting method to generate the same results. In combination with appropriate integration operators and transfer functions, we are able to extract and present different space-time features to the user.},
	booktitle = {{IEEE} Visualization, 2003. {VIS} 2003},
	author = {Woodring, J. and Wang, Chaoli and Shen, Han-Wei},
	month = oct,
	year = {2003},
	keywords = {{4D} raycasting, data visualisation, four-dimensional data field, high dimensional direct rendering, high dimensional projection, high dimensional slicing, hyperplanes, hyperprojection, hyperprojection volume, hyperslice, image classification, image hyperplane, integration operator, ray tracing, raycasting, rendering (computer graphics), solid modelling, space-time feature, time-varying data, time-varying systems, time-varying volumetric data, transfer function, user interface, volume rendering, volume specification},
	pages = {417 --424},
	file = {IEEE Xplore Abstract Record:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\UMKQ22XF\abs_all.html:text/html;IEEE Xplore Full Text PDF:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\VVJMH9P3\Woodring et al. - 2003 - High dimensional direct rendering of time-varying .pdf:application/pdf}
}

@inproceedings{widanagamaachchi_interactive_2012,
	title = {Interactive exploration of large-scale time-varying data using dynamic tracking graphs},
	doi = {10.1109/LDAV.2012.6378962},
	abstract = {Exploring and analyzing the temporal evolution of features in large-scale time-varying datasets is a common problem in many areas of science and engineering. One natural representation of such data is tracking graphs, i.e., constrained graph layouts that use one spatial dimension to indicate time and show the “tracks” of each feature as it evolves, merges or disappears. However, for practical data sets creating the corresponding optimal graph layouts that minimize the number of intersections can take hours to compute with existing techniques. Furthermore, the resulting graphs are often unmanageably large and complex even with an ideal layout. Finally, due to the cost of the layout, changing the feature definition, e.g. by changing an iso-value, or analyzing properly adjusted sub-graphs is infeasible. To address these challenges, this paper presents a new framework that couples hierarchical feature definitions with progressive graph layout algorithms to provide an interactive exploration of dynamically constructed tracking graphs. Our system enables users to change feature definitions on-the-fly and filter features using arbitrary attributes while providing an interactive view of the resulting tracking graphs. Furthermore, the graph display is integrated into a linked view system that provides a traditional 3D view of the current set of features and allows a cross-linked selection to enable a fully flexible spatio-temporal exploration of data. We demonstrate the utility of our approach with several large-scale scientific simulations from combustion science.},
	booktitle = {2012 {IEEE} {Symposium} on {Large} {Data} {Analysis} and {Visualization} ({LDAV})},
	author = {Widanagamaachchi, W. and Christensen, C. and Bremer, P.-T. and Pascucci, V.},
	month = oct,
	year = {2012},
	keywords = {Computer graphics, constrained graph layouts, Correlation, data visualization, dynamic tracking graphs, feature definition, Feature Detection and Tracking, feature extraction, graph theory, Interactive exploration, interactive systems, large-scale time-varying data, Layout, Measurement, Parallel coordinates, spatial dimension, spatio temporal exploration, temporal evolution, Time-varying data, Topology-based Techniques, Vegetation, Visualization, Visualization in Physical Sciences and Engineering},
	pages = {9--17}
}

@article{ma_machine_2007,
	title = {Machine Learning to Boost the Next Generation of Visualization Technology},
	volume = {27},
	issn = {0272-1716},
	doi = {10.1109/MCG.2007.129},
	abstract = {Visualization has become an indispensable tool in many areas of science and engineering. In particular, the advances made in the field of visualization over the past 20 years have turned visualization from a presentation tool to a discovery tool. Machine learning has received great success in both data mining and computer graphics; surprisingly, the study of systematic ways to employ machine learning in making visualization is meager. Like human learning, we can make a computer program learn from previous input data to optimize its performance on processing new data. In the context of visualization, the use of machine learning can potentially free us from manually sifting through all the data. This paper describes intelligent visualization designs for three different applications: (1) volume classification and visualization, (2) {4D} flow feature extraction and tracking, (3) network scan characterization.},
	number = {5},
	journal = {{IEEE} Computer Graphics and Applications},
	author = {Ma, Kwan-Liu},
	year = {2007},
	keywords = {{4D} flow feature extraction, {4D} flow feature tracking, Biological neural networks, Computer Graphics, data mining, data visualisation, data visualization, feature extraction, information visualization, Intelligent systems, intelligent visualization designs, interface design, learning (artificial intelligence), Machine Learning, network scan characterization, painting, Paints, rendering (computer graphics), scientific visualization, transfer functions, user interfaces, Volume classification, volume rendering, Volume Visualization},
	pages = {6--9},
	file = {IEEE Xplore Abstract Record:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\FJVBS36B\abs_all.html:text/html;IEEE Xplore Full Text PDF:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\4Z76HTGQ\Ma - 2007 - Machine Learning to Boost the Next Generation of V.pdf:application/pdf}
}

@inproceedings{lee_visualizing_2009,
	title = {Visualizing time-varying features with {TAC}-based distance fields},
	doi = {10.1109/PACIFICVIS.2009.4906831},
	abstract = {To analyze time-varying data sets, tracking features over time is often necessary to better understand the dynamic nature of the underlying physical process. Tracking 3D time-varying features, however, is non-trivial when the boundaries of the features cannot be easily defined. In this paper, we propose a new framework to visualize time-varying features and their motion without explicit feature segmentation and tracking. In our framework, a time-varying feature is described by a time series or time activity curve (TAC). To compute the distance, or similarity, between a voxel's time series and the feature, we use the dynamic time warping (DTW) distance metric. The purpose of DTW is to compare the shape similarity between two time series with an optimal warping of time so that the phase shift of the feature in time can be accounted for. After applying DTW to compare each voxel's time series with the feature, a time-invariant distance field can be computed. The amount of time warping required for each voxel to match the feature provides an estimate of the time when the feature is most likely to occur. Based on the TAC-based distance field, several visualization methods can be derived to highlight the position and motion of the feature. We present several case studies to demonstrate and compare the effectiveness of our framework.},
	booktitle = {Visualization {Symposium}, 2009. {PacificVis} '09. {IEEE} {Pacific}},
	author = {Lee, Teng-Yok and Shen, Han-Wei},
	month = apr,
	year = {2009},
	keywords = {Animation, data analysis, data mining, data visualisation, data visualization, distance metric, dynamic time warping, earthquakes, feature extraction, Feature extraction, Feature tracking, feature tracking, I.3.3 [Computing Methodologies]: COMPUTER GRAPHICS—Picture/Image Generation, I.3.7 [Computing Methodologies]: COMPUTER GRAPHICS—Three-Dimensional Graphics and Realism, Shape, shape similarity, Space exploration, Spatiotemporal phenomena, TAC-based distance field, time activity curve, time-invariant distance field, time series, time-varying data set, time-varying feature visualization, tracking},
	pages = {1--8}
}

@article{gu_transgraph_2011,
	title = {{TransGraph:} Hierarchical Exploration of Transition Relationships in Time-Varying Volumetric Data},
	volume = {17},
	issn = {1077-2626},
	shorttitle = {{TransGraph}},
	doi = {10.1109/TVCG.2011.246},
	abstract = {A fundamental challenge for time-varying volume data analysis and visualization is the lack of capability to observe and track data change or evolution in an occlusion-free, controllable, and adaptive fashion. In this paper, we propose to organize a timevarying data set into a hierarchy of states. By deriving transition probabilities among states, we construct a global map that captures the essential transition relationships in the time-varying data. We introduce the {TransGraph}, a graph-based representation to visualize hierarchical state transition relationships. The {TransGraph} not only provides a visual mapping that abstracts data evolution over time in different levels of detail, but also serves as a navigation tool that guides data exploration and tracking. The user interacts with the {TransGraph} and makes connection to the volumetric data through brushing and linking. A set of intuitive queries is provided to enable knowledge extraction from time-varying data. We test our approach with time-varying data sets of different characteristics and the results show that the {TransGraph} can effectively augment our ability in understanding time-varying data.},
	number = {12},
	journal = {{IEEE} Transactions on Visualization and Computer Graphics},
	author = {Gu, Yi and Wang, Chaoli},
	month = dec,
	year = {2011},
	keywords = {data analysis, data mining, data structures, data visualisation, data visualization, graph based representation, graph theory, hierarchical state transition relationships, knowledge extraction, navigation tool, occlusion free, probability, time-varying volumetric data analysis, {TransGraph}, transition probability, visual mapping},
	pages = {2015 --2024},
	file = {IEEE Xplore Abstract Record:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\vuyr0ioc.default\zotero\storage\CM6KWK4T\abs_all.html:text/html;IEEE Xplore Full Text PDF:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\vuyr0ioc.default\zotero\storage\PVXTJTK2\Gu and Wang - 2011 - TransGraph Hierarchical Exploration of Transition.pdf:application/pdf}
}

@article{woodring_multiscale_2009,
	title = {Multiscale Time Activity Data Exploration via Temporal Clustering Visualization Spreadsheet},
	volume = {15},
	issn = {1077-2626},
	doi = {10.1109/TVCG.2008.69},
	abstract = {Time-varying data is usually explored by animation or arrays of static images. Neither is particularly effective for classifying data by different temporal activities. Important temporal trends can be missed due to the lack of ability to find them with current visualization methods. In this paper, we propose a method to explore data at different temporal resolutions to discover and highlight data based upon time-varying trends. Using the wavelet transform along the time axis, we transform data points into multi-scale time series curve sets. The time curves are clustered so that data of similar activity are grouped together, at different temporal resolutions. The data are displayed to the user in a global time view spreadsheet where she is able to select temporal clusters of data points, and filter and brush data across temporal scales. With our method, a user can interact with data based on time activities and create expressive visualizations.},
	number = {1},
	journal = {{IEEE} Transactions on Visualization and Computer Graphics},
	author = {Woodring, J. and Shen, Han-Wei},
	month = feb,
	year = {2009},
	keywords = {Algorithms, animation, Brushes, Cluster Analysis, computer animation, Computer Graphics, Computer Simulation, data visualisation, data visualization, Filter bank, Frequency, Histograms, Information Storage and Retrieval, information visualization, Models, Theoretical, multiscale time activity data exploration, multiscale time series curve sets, Multivariate visualization, pattern clustering, Software, Temperature, temporal clustering visualization spreadsheet, transfer functions, User-Computer Interface, Visualization systems and software, Visualization techniques and methodologies, Wavelet coefficients, wavelet transform, wavelet transforms},
	pages = {123--137},
	file = {IEEE Xplore Abstract Record:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\vuyr0ioc.default\zotero\storage\P9A2PEKN\articleDetails.html:text/html;IEEE Xplore Full Text PDF:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\vuyr0ioc.default\zotero\storage\2NTVR4FT\Woodring å’Shen - 2009 - Multiscale Time Activity Data Exploration via Temp.pdf:application/pdf}
}

@inproceedings{wang_information_2008,
	title = {Information and Knowledge assisted analysis and Visualization of large-scale data},
	doi = {10.1109/ULTRAVIS.2008.5154057},
	abstract = {The ever-increasing sizes of data produced from a variety of scientific studies post a formidable challenge for the subsequent data analysis and visualization tasks. While steady advances in graphics hardware enable faster rendering, achieving interactive visualization of large data must also rely on effective data filtering and organization. In many cases, the best interactivity can only be obtained by taking into account the intrinsic properties of the data and domain knowledge to better reduce and organize the data for visualization. As a result, in recent years, we have seen increasing research and development efforts into the area of information and knowledge assisted visualization ({IKV).} In this paper, we survey research in {IKV} of scientific data and also identify a few directions for further work in this emerging area.},
	booktitle = {Workshop on Ultrascale Visualization, 2008. {UltraVis} 2008},
	author = {Wang, Chaoli and Ma, Kwan-Liu},
	year = {2008},
	keywords = {Computer Graphics, data analysis, data filtering, data mining, data visualisation, data visualization, Focusing, Hardware, Information analysis, information and knowledge assisted visualization, interactive systems, large-scale data visualization, Large-scale systems, Petascale computing, rendering (computer graphics)},
	pages = {1--8},
	file = {IEEE Xplore Abstract Record:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\vuyr0ioc.default\zotero\storage\V8N7HPNE\articleDetails.html:text/html;IEEE Xplore Full Text PDF:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\vuyr0ioc.default\zotero\storage\KWRRPDXP\Wang and Ma - 2008 - Information and Knowledge assisted analysis and Vi.pdf:application/pdf}
}

@inproceedings{ozer_group_2012,
	title = {Group dynamics in scientific visualization},
	doi = {10.1109/LDAV.2012.6378982},
	abstract = {The ability to visually extract and track features is appealing to scientists in many simulations including flow fields. However, as the resolution of the simulation becomes higher, the number of features to track increases and so does the cost in large-scale simulations. Since many of these features act in groups, it seems more cost-effective to follow groups of features rather than individual ones. Very little work has been done for tracking groups of features. In this paper, we present the first full group tracking framework in which we track groups (clusters) of features in time-varying {3D} fluid flow simulations. Our framework uses a clustering algorithm to group interacting features. We demonstrate the use of our framework on data output from a {3D} simulation of wall bounded turbulent flow.},
	booktitle = {2012 {IEEE} Symposium on Large Data Analysis and Visualization ({LDAV)}},
	author = {Ozer, S. and Wei, Jishang and Silver, D. and Ma, Kwan-Liu and Martin, P.},
	year = {2012},
	keywords = {boundary layer turbulence, Clustering, clustering algorithm, Clustering algorithms, data visualisation, data visualization, feature extraction, feature tracking, flow fields, flow simulation, group dynamics, group tracking, group tracking framework, grouping, image color analysis, mechanical engineering computing, packet identification, pattern clustering, Radar tracking, scientific visualization, Shape, time-varying {3D} fluid flow simulations, Tracking, wall bounded turbulent flow},
	pages = {97--104},
	file = {IEEE Xplore Abstract Record:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\vuyr0ioc.default\zotero\storage\ZG2CFIWV\articleDetails.html:text/html;IEEE Xplore Full Text PDF:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\vuyr0ioc.default\zotero\storage\3FIJG258\Ozer et al. - 2012 - Group dynamics in scientific visualization.pdf:application/pdf}
}

@article{lee_visualization_2009,
	title = {Visualization and Exploration of Temporal Trend Relationships in Multivariate Time-Varying Data},
	volume = {15},
	issn = {1077-2626},
	doi = {10.1109/TVCG.2009.200},
	abstract = {We present a new algorithm to explore and visualize multivariate time-varying data sets. We identify important trend relationships among the variables based on how the values of the variables change over time and how those changes are related to each other in different spatial regions and time intervals. The trend relationships can be used to describe the correlation and causal effects among the different variables. To identify the temporal trends from a local region, we design a new algorithm called {SUBDTW} to estimate when a trend appears and vanishes in a given time series. Based on the beginning and ending times of the trends, their temporal relationships can be modeled as a state machine representing the trend sequence. Since a scientific data set usually contains millions of data points, we propose an algorithm to extract important trend relationships in linear time complexity. We design novel user interfaces to explore the trend relationships, to visualize their temporal characteristics, and to display their spatial distributions. We use several scientific data sets to test our algorithm and demonstrate its utilities.},
	number = {6},
	journal = {{IEEE} Transactions on Visualization and Computer Graphics},
	author = {Lee, Teng-Yok and Shen, Han-Wei},
	month = dec,
	year = {2009},
	keywords = {Algorithm design and analysis, Clustering algorithms, computational complexity, Data Exploration, data mining, data visualisation, data visualization, Displays, hurricanes, linear time complexity, multivariate time-varying data, spatial distributions, {SUBDTW}, Temperature, temporal trend relationships, Testing, trend sequence, trend sequence clustering, user interfaces, Wind},
	pages = {1359--1366},
	file = {IEEE Xplore Abstract Record:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\vuyr0ioc.default\zotero\storage\UZZ2X962\articleDetails.html:text/html;IEEE Xplore Full Text PDF:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\vuyr0ioc.default\zotero\storage\2IDUVVHC\Lee å’Shen - 2009 - Visualization and Exploration of Temporal Trend Re.pdf:application/pdf}
}

@inproceedings{muelder_interactive_2009,
	title = {Interactive feature extraction and tracking by utilizing region coherency},
	doi = {10.1109/PACIFICVIS.2009.4906833},
	abstract = {The ability to extract and follow time-varying flow features in volume data generated from large-scale numerical simulations enables scientists to effectively see and validate modeled phenomena and processes. Extracted features often take much less storage space and computing resources to visualize. Most feature extraction and tracking methods first identify features of interest in each time step independently, then correspond these features in consecutive time steps of the data. Since these methods handle each time step separately, they do not use the coherency of the feature along the time dimension in the extraction process. In this paper, we present a prediction-correction method that uses a prediction step to make the best guess of the feature region in the subsequent time step, followed by growing and shrinking the border of the predicted region to coherently extract the actual feature of interest. This method makes use of the temporal-space coherency of the data to accelerate the extraction process while implicitly solving the tedious correspondence problem that previous methods focus on. Our method is low cost with very little storage overhead, and thus facilitates interactive or runtime extraction and visualization, unlike previous methods which were largely suited for batch-mode processing due to high computational cost.},
	booktitle = {Visualization {Symposium}, 2009. {PacificVis} '09. {IEEE} {Pacific}},
	author = {Muelder, C. and Ma, Kwan-Liu},
	year = {2009},
	keywords = {Acceleration, Application software, batch-mode processing, Computational modeling, Computer graphics, computing resources, Costs, data mining, data visualisation, data visualization, feature extraction, Feature extraction, Feature tracking, feature tracking, I.4.6 [Computer Graphics]: Segmentation—Region growing, partitioning, I.4.7 [Computer Graphics]: Feature Measurement—Feature representation, interactive feature extraction, large-scale numerical simulations, Large-scale systems, Numerical simulation, prediction-correction method, storage management, storage overhead, storage space, temporal-space coherency, time-varying flow features, utilizing region coherency},
	pages = {17--24}
}

@book{pylyshyn_seeing_2003,
	title = {Seeing and Visualizing: It's Not what You Think},
	isbn = {0262162172},
	shorttitle = {Seeing and Visualizing},
	abstract = {Winner in the category of Psychology in the 2003 {Professional/Scholarly} Publishing Annual Awards Competition presented by the Association of American Publishers, {Inc.In} Seeing and Visualizing, Zenon Pylyshyn argues that seeing is different from thinking and that to see is not, as it may seem intuitively, to create an inner replica of the world. Pylyshyn examines how we see and how we visualize and why the scientific account does not align with the way these processes seem to us "from the inside." In doing so, he addresses issues in vision science, cognitive psychology, philosophy of mind, and cognitive {neuroscience.First}, Pylyshyn argues that there is a core stage of vision independent from the influence of our prior beliefs and examines how vision can be intelligent and yet essentially knowledge-free. He then proposes that a mechanism within the vision module, called a visual index (or {FINST)}, provides a direct preconceptual connection between parts of visual representations and things in the world, and he presents various experiments that illustrate the operation of this mechanism. He argues that such a deictic reference mechanism is needed to account for many properties of vision, including how mental images attain their apparent spatial character without themselves being laid out in space in our {brains.The} final section of the book examines the "picture theory" of mental imagery, including recent neuroscience evidence, and asks whether any current evidence speaks to the issue of the format of mental images. This analysis of mental imagery brings together many of the themes raised throughout the book and provides a framework for considering such issues as the distinction between the form and the content of representations, the role of vision in thought, and the relation between behavioral, neuroscientific, and phenomenological evidence regarding mental representations.},
	language = {en},
	publisher = {{MIT} Press},
	author = {Pylyshyn, Z. W.},
	year = {2003}
}

@article{rheingans_volume_2001,
	title = {Volume illustration: nonphotorealistic rendering of volume models},
	volume = {7},
	issn = {1077-2626},
	shorttitle = {Volume illustration},
	doi = {10.1109/2945.942693},
	abstract = {Accurately and automatically conveying the structure of a volume model is a problem which has not been fully solved by existing volume rendering approaches. Physics-based volume rendering approaches create images which may match the appearance of translucent materials in nature but may not embody important structural details. Transfer function approaches allow flexible design of the volume appearance but generally require substantial hand-tuning for each new data set in order to be effective. We introduce the volume illustration approach, combining the familiarity of a physics-based illumination model with the ability to enhance important features using non-photorealistic rendering techniques. Since the features to be enhanced are defined on the basis of local volume characteristics rather than volume sample values, the application of volume illustration techniques requires less manual tuning than the design of a good transfer function. Volume illustration provides a flexible unified framework for enhancing the structural perception of volume models through the amplification of features and the addition of illumination effects},
	number = {3},
	journal = {{IEEE} Transactions on Visualization and Computer Graphics},
	author = {Rheingans, P. and Ebert, D.},
	year = {2001},
	keywords = {Art, data visualization, feature amplification, flexible design, image enhancement, important feature enhancement, Isosurfaces, lighting, lighting models, local volume characteristics, manual tuning, Manuals, nonphotorealistic rendering, Optical attenuators, optical transfer function, physics-based illumination model, physics-based volume rendering, rendering (computer graphics), shading, Solid modeling, solid modelling, structural details, structural perception, transfer functions, translucent materials, visualization, volume appearance, Volume illustration, volume model structure, X-rays},
	pages = {253--264},
	file = {IEEE Xplore Abstract Record:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\vuyr0ioc.default\zotero\storage\8WCFHNN5\articleDetails.html:text/html;IEEE Xplore Full Text PDF:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\vuyr0ioc.default\zotero\storage\VREI3X7X\Rheingans and Ebert - 2001 - Volume illustration nonphotorealistic rendering o.pdf:application/pdf}
}

@inproceedings{woodring_chronovolumes_2003,
	address = {New York, {NY}, {USA}},
	series = {{VG} '03},
	title = {Chronovolumes: a direct rendering technique for visualizing time-varying data},
	isbn = {1-58113-745-1},
	shorttitle = {Chronovolumes},
	url = {http://doi.acm.org/10.1145/827051.827054},
	doi = {10.1145/827051.827054},
	abstract = {We present a new method for displaying time varying volumetric data. The core of the algorithm is an integration through time producing a single view volume that captures the essence of multiple time steps in a sequence. The resulting view volume then can be viewed with traditional raycasting techniques. With different time integration functions, we can generate several kinds of resulting chronovolumes, which illustrate differing types of time varying features to the user. By utilizing graphics hardware and texture memory, the integration through time can be sped up, allowing the user interactive control over the temporal transfer function and exploration of the data.},
	urldate = {2013-05-17},
	booktitle = {Proceedings of the 2003 {Eurographics/IEEE} {TVCG} Workshop on Volume graphics},
	publisher = {{ACM}},
	author = {Woodring, Jonathan and Shen, Han-Wei},
	year = {2003},
	pages = {27-34},
	file = {ACM Full Text PDF:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\vuyr0ioc.default\zotero\storage\G5HNWE6B\Woodring and Shen - 2003 - Chronovolumes a direct rendering technique for vi.pdf:application/pdf}
}

@article{wang_efficient_2011,
	title = {Efficient opacity specification based on feature visibilities in direct volume rendering},
	volume = {30},
	copyright = {2011 The Author(s) Computer Graphics Forum Â© 2011 The Eurographics Association and Blackwell Publishing Ltd.},
	issn = {1467-8659},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1467-8659.2011.02045.x/abstract},
	doi = {10.1111/j.1467-8659.2011.02045.x},
	abstract = {Due to {3D} occlusion, the specification of proper opacities in direct volume rendering is a time-consuming and unintuitive process. The visibility histograms introduced by Correa and Ma reflect the effect of occlusion by measuring the influence of each sample in the histogram to the rendered image. However, the visibility is defined on individual samples, while volume exploration focuses on conveying the spatial relationships between features. Moreover, the high computational cost and large memory requirement limits its application in multi-dimensional transfer function {design.In} this paper, we extend visibility histograms to feature visibility, which measures the contribution of each feature in the rendered image. Compared to visibility histograms, it has two distinctive advantages for opacity specification. First, the user can directly specify the visibilities for features and the opacities are automatically generated using an optimization algorithm. Second, its calculation requires only one rendering pass with no additional memory requirement. This feature visibility based opacity specification is fast and compatible with all types of transfer function design. Furthermore, we introduce a two-step volume exploration scheme, in which an automatic optimization is first performed to provide a clear illustration of the spatial relationship and then the user adjusts the visibilities directly to achieve the desired feature enhancement. The effectiveness of this scheme is demonstrated by experimental results on several volumetric datasets.},
	language = {en},
	number = {7},
	urldate = {2013-04-30},
	journal = {Computer Graphics Forum},
	author = {Wang, Yunhai and Zhang, Jian and Chen, Wei and Zhang, Huai and Chi, Xuebin},
	year = {2011},
	pages = {2117-2126}
}

@article{correa_visibility_2011,
	title = {Visibility Histograms and Visibility-Driven Transfer Functions},
	volume = {17},
	issn = {1077-2626},
	url = {http://dx.doi.org/10.1109/TVCG.2010.35},
	doi = {10.1109/TVCG.2010.35},
	abstract = {Direct volume rendering is an important tool for visualizing complex data sets. However, in the process of generating {2D} images from {3D} data, information is lost in the form of attenuation and occlusion. The lack of a feedback mechanism to quantify the loss of information in the rendering process makes the design of good transfer functions a difficult and time consuming task. In this paper, we present the general notion of visibility histograms, which are multidimensional graphical representations of the distribution of visibility in a volume-rendered image. In this paper, we explore the {1D} and {2D} transfer functions that result from intensity values and gradient magnitude. With the help of these histograms, users can manage a complex set of transfer function parameters that maximize the visibility of the intervals of interest and provide high quality images of volume data. We present a semiautomated method for generating transfer functions, which progressively explores the transfer function space toward the goal of maximizing visibility of important structures. Our methodology can be easily deployed in most visualization systems and can be used together with traditional {1D} and {2D} opacity transfer functions based on scalar values, as well as with other more sophisticated rendering algorithms.},
	number = {2},
	urldate = {2013-05-02},
	journal = {{IEEE} Transactions on Visualization and Computer Graphics},
	author = {Correa, Carlos D. and Ma, Kwan-Liu},
	year = {2011},
	keywords = {Attenuation, complex data set visualisation, data visualization, Direct volume rendering, Feedback, feedback mechanism, Histograms, histograms., Image generation, multidimensional graphical representations, Multidimensional systems, Process design, Quality management, rendering (computer graphics), transfer functions, view-point dependent rendering, visibility, visibility driven transfer functions, visibility histograms, volume rendered image, volume rendering},
	pages = {192-204},
	file = {IEEE Xplore Full Text PDF:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\J7JUS8N4\Correa and Ma - 2011 - Visibility Histograms and Visibility-Driven Transf.pdf:application/pdf}
}

@INPROCEEDINGS{correa_visibility-driven_2009, 
	author={C. D. Correa and K. L. Ma}, 
	booktitle={IEEE Pacific Visualization Symposium}, 
	title={Visibility-driven transfer functions}, 
	year={2009}, 
	pages={177-184}, 
	keywords={data visualisation;rendering (computer graphics);transfer functions;1D opacity transfer functions;complex data set visualization;direct volume rendering;visibility-driven transfer functions;Attenuation;Computer graphics;Data visualization;Feedback;Histograms;Image generation;Multidimensional systems;Process design;Rendering (computer graphics);Transfer functions;Transfer functions;volume rendering}, 
	doi={10.1109/PACIFICVIS.2009.4906854}, 
	ISSN={2165-8765}, 
	month={April},}

@inproceedings{correa_visibility-driven_2009---faulty,
	title = {Visibility-driven transfer functions},
	doi = {10.1109/PACIFICVIS.2009.4906854},
	abstract = {Direct volume rendering is an important tool for visualizing complex data sets. However, in the process of generating {2D} images from {3D} data, information is lost in the form of attenuation and occlusion. The lack of a feedback mechanism to quantify the loss of information in the rendering process makes the design of good transfer functions a difficult and time consuming task. In this paper, we present the notion of visibility-driven transfer functions, which are transfer functions that provide a good visibility of features of interest from a given viewpoint. To achieve this, we introduce visibility histograms. These histograms provide graphical cues that intuitively inform the user about the contribution of particular scalar values to the final image. By carefully manipulating the parameters of the opacity transfer function, users can now maximize the visibility of the intervals of interest in a volume data set. Based on this observation, we also propose a semi-automated method for generating transfer functions, which progressively improves a transfer function defined by the user, according to a certain importance metric. Now the user does not have to deal with the tedious task of making small changes to the transfer function parameters, but now he/she can rely on the system to perform these searches automatically. Our methodology can be easily deployed in most visualization systems and can be used together with traditional {1D} opacity transfer functions based on scalar values, as well as with multidimensional transfer functions and other more sophisticated rendering algorithms.},
	booktitle = {Visualization Symposium, 2009. {PacificVis} '09. {IEEE} Pacific},
	author = {Correa, C. and Ma, Kwan-Liu},
	year = {2009},
	keywords = {{1D} opacity transfer functions, Attenuation, complex data set visualization, Computer Graphics, data visualisation, Data visualization, Direct volume rendering, Feedback, Histograms, Image generation, Multidimensional systems, Process design, rendering (computer graphics), transfer functions, visibility-driven transfer functions, volume rendering},
	pages = {177--184},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\t929e2iy.default\\zotero\\storage\\5V4BE2EB\\abs_all.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\t929e2iy.default\\zotero\\storage\\M7FEIITC\\Correa and Ma - 2009 - Visibility-driven transfer functions.pdf:application/pdf}
}

@article{ruiz_automatic_2011,
	title = {Automatic Transfer Functions Based on Informational Divergence},
	volume = {17},
	issn = {1077-2626},
	doi = {10.1109/TVCG.2011.173},
	abstract = {In this paper we present a framework to define transfer functions from a target distribution provided by the user. A target distribution can reflect the data importance, or highly relevant data value interval, or spatial segmentation. Our approach is based on a communication channel between a set of viewpoints and a set of bins of a volume data set, and it supports {1D} as well as {2D} transfer functions including the gradient information. The transfer functions are obtained by minimizing the informational divergence or Kullback-Leibler distance between the visibility distribution captured by the viewpoints and a target distribution selected by the user. The use of the derivative of the informational divergence allows for a fast optimization process. Different target distributions for {1D} and {2D} transfer functions are analyzed together with importance-driven and view-based techniques.},
	number = {12},
	journal = {{IEEE} Transactions on Visualization and Computer Graphics},
	author = {Ruiz, M. and Bardera, A. and Boada, I. and Viola, I. and Feixas, M. and Sbert, M.},
	year = {2011},
	keywords = {{1D} transfer functions, {2D} transfer functions, automatic transfer functions, communication channel, data value interval, data visualization, Information analysis, Information Theory, informational divergence, Kullback-Leibler distance, Kullback-Leibler distance., Mutual information, optical transfer function, optimisation, optimization process, Probability distribution, rendering (computer graphics), spatial segmentation, target distribution, transfer function, transfer functions, visibility, visibility distribution},
	pages = {1932--1941},
	file = {IEEE Xplore Abstract Record:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\7G35HKWI\abs_all.html:text/html;IEEE Xplore Full Text PDF:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\6Q62QI2M\Ruiz et al. - 2011 - Automatic Transfer Functions Based on Informationa.pdf:application/pdf}
}

@article{bramon_information_2013,
	title = {Information Theory-Based Automatic Multimodal Transfer Function Design},
	volume = {17},
	issn = {2168-2194},
	doi = {10.1109/JBHI.2013.2263227},
	abstract = {In this paper, we present a new framework for multimodal volume visualization that combines several information-theoretic strategies to define both colors and opacities of the multimodal transfer function. To the best of our knowledge, this is the first fully automatic scheme to visualize multimodal data. To define the fused color, we set an information channel between two registered input datasets, and afterward, we compute the informativeness associated with the respective intensity bins. This informativeness is used to weight the color contribution from both initial 1-D transfer functions. To obtain the opacity, we apply an optimization process that minimizes the informational divergence between the visibility distribution captured by a set of viewpoints and a target distribution proposed by the user. This distribution is defined either from the dataset features, from manually set importances, or from both. Other problems related to the multimodal visualization, such as the computation of the fused gradient and the histogram binning, have also been solved using new information-theoretic strategies. The quality and performance of our approach are evaluated on different datasets.},
	number = {4},
	journal = {{IEEE} Journal of Biomedical and Health Informatics},
	author = {Bramon, R. and Ruiz, M. and Bardera, A. and Boada, I. and Feixas, M. and Sbert, M.},
	year = {2013},
	keywords = {Information Theory, {Kullback-Leibler} distance, multimodal fusion, multimodal visualization, Transfer function design},
	pages = {870--880},
	file = {IEEE Xplore Abstract Record:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\KN5IQJG5\articleDetails.html:text/html;IEEE Xplore Full Text PDF:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\VF48UATT\Bramon et al. - 2013 - Information Theory-Based Automatic Multimodal Tran.pdf:application/pdf}
}

@inproceedings{bordoloi_view_2005,
	title = {View selection for volume rendering},
	doi = {10.1109/VISUAL.2005.1532833},
	abstract = {In a visualization of a three-dimensional dataset, the insights gained are dependent on what is occluded and what is not. Suggestion of interesting viewpoints can improve both the speed and efficiency of data understanding. This paper presents a view selection method designed for volume rendering. It can be used to find informative views for a given scene, or to find a minimal set of representative views which capture the entire scene. It becomes particularly useful when the visualization process is non-interactive - for example, when visualizing large datasets or time-varying sequences. We introduce a viewpoint "goodness" measure based on the formulation of entropy from information theory. The measure takes into account the transfer function, the data distribution and the visibility of the voxels. Combined with viewpoint properties like view-likelihood and view-stability, this technique can be used as a guide, which suggests "interesting" viewpoints for further exploration. Domain knowledge is incorporated into the algorithm via an importance transfer function or volume. This allows users to obtain view selection behaviors tailored to their specific situations. We generate a view space partitioning, and select one representative view for each partition. Together, this set of views encapsulates the "interesting" and distinct views of the data. Viewpoints in this set can be used as starting points for interactive exploration of the data, thus reducing the human effort in visualization. In non-interactive situations, such a set can be used as a representative visualization of the dataset from all directions.},
	booktitle = {{IEEE} Visualization},
	author = {Bordoloi, {U.D.} and Shen, Han-Wei},
	year = {2005},
	keywords = {Cameras, Data Distribution, data visualisation, data visualization, Design methodology, domain knowledge, entropy, entropy formulation, hidden feature removal, Humans, Information Theory, interactive data exploration, Layout, occlusion, Partitioning algorithms, rendering (computer graphics), transfer function, transfer functions, very large databases, view selection method, view space partitioning, volume rendering, voxel visibility},
	pages = {487--494},
	file = {IEEE Xplore Abstract Record:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\X9PCRU4T\abs_all.html:text/html;IEEE Xplore Full Text PDF:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\DHIBNP53\Bordoloi and Shen - 2005 - View selection for volume rendering.pdf:application/pdf}
}

@article{ruiz_viewpoint_2010,
	title = {Viewpoint information channel for illustrative volume rendering},
	volume = {34},
	issn = {0097-8493},
	shorttitle = {Procedural Methods in Computer Graphics Illustrative Visualization},
	url = {http://www.sciencedirect.com/science/article/pii/S0097849310000439},
	doi = {10.1016/j.cag.2010.01.006},
	abstract = {This paper introduces a volume rendering framework based on the information channel constructed between the volumetric data set and a set of viewpoints. From this channel, the information associated to each voxel can be interpreted as an ambient occlusion value that allows to obtain illustrative volume visualizations. The use of the voxel information combined with the assignation of color to each viewpoint and non-photorealistic effects produces an enhanced visualization of the volume data set. Voxel information is also applied to modulate the transfer function and to select the most informative views.},
	number = {4},
	urldate = {2013-01-22},
	journal = {Computers \& Graphics},
	author = {Ruiz, M. and Boada, I. and Feixas, M. and Sbert, M.},
	month = aug,
	year = {2010},
	keywords = {Ambient occlusion, Illustrative Visualization, Viewpoint selection},
	pages = {351--360},
	file = {ScienceDirect Full Text PDF:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\EBEE3XDD\Ruiz et al. - 2010 - Viewpoint information channel for illustrative vol.pdf:application/pdf;ScienceDirect Snapshot:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\4M4BCII7\Ruiz et al. - 2010 - Viewpoint information channel for illustrative vol.html:text/html}
}

@article{ji_dynamic_2006,
	title = {Dynamic View Selection for Time-Varying Volumes},
	volume = {12},
	issn = {1077-2626},
	doi = {10.1109/TVCG.2006.137},
	abstract = {Animation is an effective way to show how time-varying phenomena evolve over time. A key issue of generating a good animation is to select ideal views through which the user can perceive the maximum amount of information from the time-varying dataset. In this paper, we first propose an improved view selection method for static data. The method measures the quality of a static view by analyzing the opacity, color and curvature distributions of the corresponding volume rendering images from the given view. Our view selection metric prefers an even opacity distribution with a larger projection area, a larger area of salient features' colors with an even distribution among the salient features, and more perceived curvatures. We use this static view selection method and a dynamic programming approach to select time-varying views. The time-varying view selection maximizes the information perceived from the time-varying dataset based on the constraints that the time-varying view should show smooth changes of direction and near-constant speed. We also introduce a method that allows the user to generate a smooth transition between any two views in a given time step, with the perceived information maximized as well. By combining the static and dynamic view selection methods, the users are able to generate a time-varying view that shows the maximum amount of information from a time-varying data set},
	number = {5},
	journal = {{IEEE} Transactions on Visualization and Computer Graphics},
	author = {Ji, Guangfeng and Shen, Han-Wei},
	month = oct,
	year = {2006},
	keywords = {animation, computer animation, curvature distributions, data visualisation, data visualization, Diversity reception, dynamic programming, dynamic programming approach, dynamic view selection, Image analysis, image based method, image color analysis, Information entropy, opacity distribution, Optimization methods, optimization., rendering (computer graphics), salient feature colors, smooth transition generation, static view selection, static view selection method, time-varying dataset, time-varying view selection, time-varying volumes, Volume measurement, volume rendering images},
	pages = {1109--1116},
	file = {IEEE Xplore Abstract Record:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\39SXVHIZ\cookiedetectresponse.html:text/html;IEEE Xplore Full Text PDF:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\CI7ZG68R\Ji and Shen - 2006 - Dynamic View Selection for Time-Varying Volumes.pdf:application/pdf}
}

@inproceedings{takahashi_feature-driven_2005,
	title = {A feature-driven approach to locating optimal viewpoints for volume visualization},
	doi = {10.1109/VISUAL.2005.1532834},
	abstract = {Optimal viewpoint selection is an important task because it considerably influences the amount of information contained in the {2D} projected images of {3D} objects, and thus dominates their first impressions from a psychological point of view. Although several methods have been proposed that calculate the optimal positions of viewpoints especially for {3D} surface meshes, none has been done for solid objects such as volumes. This paper presents a new method of locating such optimal viewpoints when visualizing volumes using direct volume rendering. The major idea behind our method is to decompose an entire volume into a set of feature components, and then find a globally optimal viewpoint by finding a compromise between locally optimal viewpoints for the components. As the feature components, the method employs interval volumes and their combinations that characterize the topological transitions of isosurfaces according to the scalar field. Furthermore, opacity transfer functions are also utilized to assign different weights to the decomposed components so that users can emphasize features of specific interest in the volumes. Several examples of volume datasets together with their optimal positions of viewpoints are exhibited in order to demonstrate that the method can effectively guide naive users to find optimal projections of volumes.},
	booktitle = {{IEEE} Visualization, 2005. {VIS} 05},
	author = {Takahashi, S. and Fujishiro, I. and Takeshima, Y. and Nishita, T.},
	year = {2005},
	keywords = {{3D} surface mesh, Chromium, Computer Graphics, data visualisation, Direct volume rendering, entropy, Hardware, Isosurfaces, level-set graph, mesh generation, optimal viewpoint selection, Psychology, rendering (computer graphics), Solids, surface fitting, transfer function, transfer functions, viewpoint entropy, visualization, Volume Visualization},
	pages = {495--502},
	file = {IEEE Xplore Abstract Record:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\HZG84WEV\abs_all.html:text/html;IEEE Xplore Full Text PDF:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\63Z8VC5U\Takahashi et al. - 2005 - A feature-driven approach to locating optimal view.pdf:application/pdf}
}

@inproceedings{schlegel_visibility-difference_2013,
	title = {Visibility-difference entropy for automatic transfer function generation},
	volume = {8654},
	url = {http://dx.doi.org/10.1117/12.2002971},
	doi = {10.1117/12.2002971},
	abstract = {Direct volume rendering allows for interactive exploration of volumetric data and has become an important tool in many
	visualization domains. But the insight and information that can be obtained are dependent on the transfer function defining
	the transparency of voxels. Constructing good transfer functions is one of the most time consuming and cumbersome tasks
	in volume visualization. We present a novel general purpose method for automatically generating an initial set of best
	transfer function candidates. The generated transfer functions reveal the major structural features within the volume and
	allow for an efficient initial visual analysis, serving as a basis for further interactive exploration in particular of originally
	unknown data. The basic idea is to introduce a metric as a measure of the goodness of a transfer function which indicates the
	information that can be gained from rendered images by interactive visualization. In contrast to prior methods, our approach
	does not require a user feedback-loop, operates exclusively in image space and takes the characteristics of interactive data
	exploration into account. We show how our new transfer function generation method can uncover the major structures of
	an unknown dataset within only a few minutes.},
	urldate = {2016-09-09},
	author = {Schlegel, Philipp and Pajarola, Renato},
	year = {2013},
	pages = {865406--865406--15}
}

@inproceedings{lee_motion_2009,
	address = {Aire-la-Ville, Switzerland, Switzerland},
	series = {{EGSR'09}},
	title = {Motion based painterly rendering},
	url = {http://dx.doi.org/10.1111/j.1467-8659.2009.01498.x},
	doi = {10.1111/j.1467-8659.2009.01498.x},
	abstract = {Previous painterly rendering techniques normally use image gradients for deciding stroke orientations. Image gradients are good for expressing object shapes, but difficult to express the flow or movements of objects. In real painting, the use of brush strokes corresponding to the actual movement of objects allows viewers to recognize objects' motion better and thus to have an impression of the dynamic. In this paper, we propose a novel painterly rendering algorithm to express dynamic objects based on their motion information. We first extract motion information (magnitude, direction, standard deviation) of a scene from a set of consecutive image sequences from the same view. Then the motion directions are used for determining stroke orientations in the regions with significant motions, and image gradients determine stroke orientations where little motion is observed. Our algorithm is useful for realistically and dynamically representing moving objects. We have applied our algorithm for rendering landscapes. We could segment a scene into dynamic and static regions, and express the actual movement of dynamic objects using motion based strokes.},
	urldate = {2013-07-21},
	booktitle = {Proceedings of the Twentieth Eurographics conference on Rendering},
	publisher = {Eurographics Association},
	author = {Lee, H. and Lee, C. H. and Yoon, K.},
	year = {2009},
	pages = {1207-1215},
	file = {EGSR09_MotbasedPR_low.pdf:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\7G83G3EK\EGSR09_MotbasedPR_low.pdf:application/pdf}
}

@inproceedings{hays_image_2004,
	address = {New York, {NY}, {USA}},
	series = {{NPAR} '04},
	title = {Image and video based painterly animation},
	isbn = {1-58113-887-3},
	url = {http://doi.acm.org/10.1145/987657.987676},
	doi = {10.1145/987657.987676},
	abstract = {We present techniques for transforming images and videos into painterly animations depicting different artistic styles. Our techniques rely on image and video analysis to compute appearance and motion properties. We also determine and apply motion information from different (user-specified) sources to static and moving images. These properties that encode spatio-temporal variations are then used to render (or paint) effects of selected styles to generate images and videos with a painted look. Painterly animations are generated using a mesh of brush stroke objects with dynamic spatio-temporal properties. Styles govern the behavior of these brush strokes as well as their rendering to a virtual canvas. We present methods for modifying the properties of these brush strokes according to the input images, videos, or motions. Brush stroke color, length, orientation, opacity, and motion are determined and the brush strokes are regenerated to fill the canvas as the video changes. All brush stroke properties are temporally constrained to guarantee temporally coherent non-photorealistic animations.},
	urldate = {2013-07-21},
	booktitle = {Proceedings of the 3rd international symposium on Non-photorealistic animation and rendering},
	publisher = {{ACM}},
	author = {Hays, James and Essa, Irfan},
	year = {2004},
	pages = {113-120},
	file = {IVBPA_Final.pdf:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\EWPBRNHW\IVBPA_Final.pdf:application/pdf}
}

@article{healey_perceptually_2004,
	title = {Perceptually based brush strokes for nonphotorealistic visualization},
	volume = {23},
	issn = {0730-0301},
	url = {http://doi.acm.org/10.1145/966131.966135},
	doi = {10.1145/966131.966135},
	abstract = {An important problem in the area of computer graphics is the visualization of large, complex information spaces. Datasets of this type have grown rapidly in recent years, both in number and in size. Images of the data stored in these collections must support rapid and accurate exploration and analysis. This article presents a method for constructing visualizations that are both effective and aesthetic. Our approach uses techniques from master paintings and human perception to visualize a multidimensional dataset. Individual data elements are drawn with one or more brush strokes that vary their appearance to represent the element's attribute values. The result is a nonphotorealistic visualization of information stored in the dataset. Our research extends existing glyph-based and nonphotorealistic techniques by applying perceptual guidelines to build an effective representation of the underlying data. The nonphotorealistic properties the strokes employ are selected from studies of the history and theory of Impressionist art. We show that these properties are similar to visual features that are detected by the low-level human visual system. This correspondence allows us to manage the strokes to produce perceptually salient visualizations. Psychophysical experiments confirm a strong relationship between the expressive power of our nonphotorealistic properties and previous findings on the use of perceptual color and texture patterns for data display. Results from these studies are used to produce effective nonphotorealistic visualizations. We conclude by applying our techniques to a large, multidimensional weather dataset to demonstrate their viability in a practical, real-world setting.},
	number = {1},
	urldate = {2013-05-17},
	journal = {ACM Trans. Graph.},
	author = {Healey, Christopher G. and Tateosian, Laura and Enns, James T. and Remple, Mark},
	month = jan,
	year = {2004},
	keywords = {Abstractionism, color, Computer Graphics, human vision, Impressionism, nonphotorealistic rendering, perception, psychophysics, scientific visualization, texture},
	pages = {64--96},
	file = {ACM Full Text PDF:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\JNZP5UJX\\Healey et al. - 2004 - Perceptually based brush strokes for nonphotoreali.pdf:application/pdf}
}

@article{hertzmann_survey_2003,
	title = {A survey of stroke-based rendering},
	volume = {23},
	issn = {0272-1716},
	doi = {10.1109/MCG.2003.1210867},
	abstract = {This tutorial describes several stroke-based rendering ({SBR)} algorithms. {SBR} is an automatic approach to creating nonphotorealistic imagery by placing discrete elements such as paint strokes or stipples.},
	number = {4},
	journal = {{IEEE} Computer Graphics and Applications},
	author = {Hertzmann, A.},
	month = aug,
	year = {2003},
	keywords = {discrete elements, nonphotorealistic imagery, paint strokes, rendering (computer graphics), stipples, stroke-based rendering},
	pages = {70--81},
	file = {IEEE Xplore Abstract Record:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\URTHAV92\abs_all.html:text/html;IEEE Xplore Full Text PDF:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\9NN2I7GB\Hertzmann - 2003 - A survey of stroke-based rendering.pdf:application/pdf}
}

@inproceedings{lopez-moreno_stylized_2010,
	address = {New York, {NY}, {USA}},
	series = {{NPAR} '10},
	title = {Stylized depiction of images based on depth perception},
	isbn = {978-1-4503-0125-1},
	url = {http://doi.acm.org/10.1145/1809939.1809952},
	doi = {10.1145/1809939.1809952},
	abstract = {Recent works in image editing are opening up new possibilities to manipulate and enhance input images. Within this context, we leverage well-known characteristics of human perception along with a simple depth approximation algorithm to creatively relight images for the purpose of generating non-photorealistic renditions that would be difficult to achieve with existing methods. Our realtime implementation on graphics hardware allows the user to efficiently explore artistic possibilities for each image. We show results produced with four different styles proving the versatility of our approach, and validate our assumptions and simplifications by means of a user study.},
	urldate = {2013-07-26},
	booktitle = {Proceedings of the 8th International Symposium on Non-Photorealistic Animation and Rendering},
	publisher = {{ACM}},
	author = {Lopez-Moreno, Jorge and Jimenez, Jorge and Hadap, Sunil and Reinhard, Erik and Anjyo, Ken and Gutierrez, Diego},
	year = {2010},
	keywords = {human visual system, image processing, non-photorealistic rendering, relighting},
	pages = {109-118},
	file = {ACM Full Text PDF:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\ZEPCSAJI\Lopez-Moreno et al. - 2010 - Stylized depiction of images based on depth percep.pdf:application/pdf}
}

@inproceedings{marks_design_1997,
	address = {New York, {NY}, {USA}},
	series = {{SIGGRAPH} '97},
	title = {Design galleries: a general approach to setting parameters for computer graphics and animation},
	isbn = {0-89791-896-7},
	shorttitle = {Design galleries},
	url = {http://dx.doi.org/10.1145/258734.258887},
	doi = {10.1145/258734.258887},
	urldate = {2013-07-31},
	booktitle = {Proceedings of the 24th annual conference on Computer graphics and interactive techniques},
	publisher = {{ACM} {Press/Addison-Wesley} Publishing Co.},
	author = {Marks, J. and Andalman, B. and Beardsley, P. A. and Freeman, W. and Gibson, S. and Hodgins, J. and Kang, T. and Mirtich, B. and Pfister, H. and Ruml, W. and Ryall, K. and Seims, J. and Shieber, S.},
	year = {1997},
	keywords = {animation, Computer-Aided Design, image rendering, lighting, motion synthesis, Particle systems, physical modeling, visualization, volume rendering},
	pages = {389-400},
	file = {TR97-14.pdf:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\JDKKIFG8\TR97-14.pdf:application/pdf}
}

@article{wu_interactive_2007,
	title = {Interactive Transfer Function Design Based on Editing Direct Volume Rendered Images},
	volume = {13},
	issn = {1077-2626},
	doi = {10.1109/TVCG.2007.1051},
	abstract = {Direct volume rendered images ({DVRIs)} have been widely used to reveal structures in volumetric data. However, {DVRIs} generated by many volume visualization techniques can only partially satisfy users' demands. In this paper, we propose a framework for editing {DVRIs}, which can also be used for interactive transfer function ({TF)} design. Our approach allows users to fuse multiple features in distinct {DVRIs} into a comprehensive one, to blend two {DVRIs}, and/or to delete features in a {DVRI.} We further present how these editing operations can generate smooth animations for focus + context visualization. Experimental results on some real volumetric data demonstrate the effectiveness of our method.},
	number = {5},
	journal = {{IEEE} Transactions on Visualization and Computer Graphics},
	author = {Wu, Yingcai and Qu, Huamin},
	year = {2007},
	keywords = {Algorithms, animation, animations, Biomedical imaging, computational fluid dynamics, Computer Graphics, context visualization, data visualisation, data visualization, direct volume rendered images, Direct volume rendering, focus + context, Focusing, Fuses, Graphics, image editing, image enhancement, Image Interpretation, Computer-Assisted, Imaging, Three-Dimensional, Information Storage and Retrieval, interactive transfer function, intuitive user interface, morphing, Numerical Analysis, Computer-Assisted, rendering (computer graphics), Signal Processing, Computer-Assisted, Transfer function design, transfer functions, user interfaces, User-Computer Interface, visualization techniques, Word Processing},
	pages = {1027--1040},
	file = {IEEE Xplore Abstract Record:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\UQ96NDQ5\login.html:text/html;IEEE Xplore Full Text PDF:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\7VDF8FWU\Wu and Qu - 2007 - Interactive Transfer Function Design Based on Edit.pdf:application/pdf}
}

@inproceedings{haidacher_information-based_2008,
	address = {Aire-la-Ville, Switzerland, Switzerland},
	series = {{EG} {VCBM'08}},
	title = {Information-based transfer functions for multimodal visualization},
	isbn = {978-3-905674-13-2},
	url = {http://dx.doi.org/10.2312/VCBM/VCBM08/101-108},
	doi = {10.2312/VCBM/VCBM08/101-108},
	abstract = {Transfer functions are an essential part of volume visualization. In multimodal visualization at least two values exist at every sample point. Additionally, other parameters, such as gradient magnitude, are often retrieved for each sample point. To find a good transfer function for this high number of parameters is challenging because of the complexity of this task. In this paper we present a general information-based approach for transfer function design in multimodal visualization which is independent of the used modality types. Based on information theory, the complex multi-dimensional transfer function space is fused to allow utilization of a well-known {2D} transfer function with a single value and gradient magnitude as parameters. Additionally, a quantity is introduced which enables better separation of regions with complementary information. The benefit of the new method in contrast to other techniques is a transfer function space which is easy to understand and which provides a better separation of different tissues. The usability of the new approach is shown on examples of different modalities.},
	urldate = {2013-05-02},
	booktitle = {Proceedings of the First Eurographics conference on Visual Computing for Biomedicine},
	publisher = {Eurographics Association},
	author = {Haidacher, Martin and Bruckner, Stefan and Kanitsar, Armin and Gr{\"o}ller, M. Eduard},
	year = {2008},
	keywords = {Information Theory, Multimodal Visualization, transfer function},
	pages = {101-108},
	file = {haidacher-2008-vcbm-CT-PET.jpg (JPEG Image, 1444Â Ã—Â 1167 pixels):C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\5RM7B9HE\haidacher-2008-vcbm-CT-PET.jpg:image/jpeg;haidacher-2008-vcbm-CT-PET.jpg (JPEG Image, 1444Â Ã—Â 1167 pixels):C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\BKGZBA9A\haidacher-2008-vcbm-CT-PET.jpg:image/jpeg}
}

@article{meyer-spradow_voreen:_2009,
	title = {Voreen: A Rapid-Prototyping Environment for Ray-Casting-Based Volume Visualizations},
	volume = {29},
	issn = {0272-1716},
	shorttitle = {Voreen},
	doi = {10.1109/MCG.2009.130},
	abstract = {By splitting a complex ray-casting process into different tasks performed on different processors, Voreen provides a lot of flexibility because users can intervene at different points during ray casting. Voreen's object-oriented design lets users easily create customized processor classes that cooperate seamlessly with existing classes. A user-friendly {GUI} supports rapid prototyping of visualization ideas. We've implemented several applications based on our library. In the future, we'd like to further extend Voreen's capabilities to make visualization prototyping even easier on all abstraction levels. Thus, we plan to realize a set of dedicated processor skeletons, which are solely configured through shader programs and can thus be modified at runtime.},
	number = {6},
	journal = {{IEEE} Computer Graphics and Applications},
	author = {Meyer-Spradow, J. and Ropinski, T. and Mensmann, J. and Hinrichs, K.},
	year = {2009},
	keywords = {Casting, {GPU-based} ray casting, graphical user interfaces, {GUI}, Libraries, object-oriented design, object-oriented methods, program visualisation, Prototypes, rapid-prototyping environment, ray-casting-based volume visualizations, Runtime, skeleton, software prototyping, visualization, visualization prototyping, volume rendering, Voreen},
	pages = {6--13},
	file = {IEEE Xplore Abstract Record:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\vuyr0ioc.default\zotero\storage\ZUXEA78N\abs_all.html:text/html;IEEE Xplore Full Text PDF:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\vuyr0ioc.default\zotero\storage\NG8HQGI4\Meyer-Spradow et al. - 2009 - Voreen A Rapid-Prototyping Environment for Ray-Ca.pdf:application/pdf}
}

@article{wang_information_2011,
	title = {Information Theory in Scientific Visualization},
	volume = {13},
	issn = {1099-4300},
	url = {http://www.mdpi.com/1099-4300/13/1/254},
	doi = {10.3390/e13010254},
	number = {12},
	urldate = {2013-07-31},
	journal = {Entropy},
	author = {Wang, Chaoli and Shen, Han-Wei},
	month = jan,
	year = {2011},
	pages = {254--273},
	file = {Entropy | Free Full-Text | Information Theory in Scientific Visualization:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\S9SBUEKJ\254.html:text/html;entropy-13-00254.pdf:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\7QNZVSNQ\entropy-13-00254.pdf:application/pdf}
}

@article{chen_information-theoretic_2010,
	title = {An {Information}-theoretic {Framework} for {Visualization}},
	volume = {16},
	issn = {1077-2626},
	doi = {10.1109/TVCG.2010.132},
	abstract = {In this paper, we examine whether or not information theory can be one of the theoretic frameworks for visualization. We formulate concepts and measurements for qualifying visual information. We illustrate these concepts with examples that manifest the intrinsic and implicit use of information theory in many existing visualization techniques. We outline the broad correlation between visualization and the major applications of information theory, while pointing out the difference in emphasis and some technical gaps. Our study provides compelling evidence that information theory can explain a significant number of phenomena or events in visualization, while no example has been found which is fundamentally in conflict with information theory. We also notice that the emphasis of some traditional applications of information theory, such as data compression or data communication, may not always suit visualization, as the former typically focuses on the efficient throughput of a communication channel, whilst the latter focuses on the effectiveness in aiding the perceptual and cognitive process for data understanding and knowledge discovery. These findings suggest that further theoretic developments are necessary for adopting and adapting information theory for visualization.},
	number = {6},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Chen, M. and J{\"a}enicke, H.},
	month = nov,
	year = {2010},
	keywords = {communication channel, data communication, data compression, data mining, data visualisation, information-theoretic framework, information theory, knowledge discovery, quantitative evaluation, theory of visualization, visual information qualification, visualization techniques},
	pages = {1206--1215}
}

@inproceedings{roettger_spatialized_2005,
	address = {Aire-la-Ville, Switzerland, Switzerland},
	series = {{EUROVIS'05}},
	title = {Spatialized transfer functions},
	isbn = {3-905673-19-3},
	url = {http://dx.doi.org/10.2312/VisSym/EuroVis05/271-278},
	doi = {10.2312/VisSym/EuroVis05/271-278},
	abstract = {Multi-dimensional transfer functions are an efficient way to visualize features in scalar volume data produced by {CT} or {MRI} scanners. However, the optimal transfer function is difficult to find in general. We present an automatic yet powerful method for the automatic setup of multi-dimensional transfer functions by adding spatial information to the histogram of a volume. Using this information we can easily classify the histogram and derive a transfer function by assigning unique colors to each class of the histogram. Each feature can be selected interactively by pointing and clicking at the corresponding class in the transfer function. In order to render the classified volume with adequate quality we propose an extension of the wellknown pre-integration technique. Furthermore, we demonstrate the flexibility of our approach by giving examples for the imaging of segmented, diffusion-tensor and multi-modal data.},
	urldate = {2013-07-09},
	booktitle = {Proceedings of the Seventh Joint Eurographics / {IEEE} {VGTC} conference on Visualization},
	publisher = {Eurographics Association},
	author = {Roettger, Stefan and Bauer, Michael and Stamminger, Marc},
	year = {2005},
	pages = {271-278},
	file = {EUROVIS05.pdf:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\DP3HR2C3\EUROVIS05.pdf:application/pdf;SPATIAL.pdf:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\SEDBU2A7\SPATIAL.pdf:application/pdf}
}

@inproceedings{tappenbeck_distance-based_2006,
	title = {Distance-Based Transfer Function Design: Specification Methods and Applications},
	shorttitle = {Distance-Based Transfer Function Design},
	abstract = {We employ distances as a second dimension for transfer function (hereafter {TF)}  specification. Distances refer to selected reference shapes. When distance-based {TFs}  are applied to medical volume data and anatomic structures as reference shapes, they  can support diagnostic procedures and therapy planning. As an example, distance-based  {TFs} may be used to explore the neighborhood of a tumor which is essential to assess  whether a surgical removal is feasible. In this paper, we discuss methods to specify  2d distance-based {TFs}, the use of predefined but adjustable templates to reduce the  interaction effort and an efficient implementation of these {TFs.}},
	booktitle = {{SimVis}},
	author = {Tappenbeck, Andreas and Preim, Bernhard and Dicken, Volker},
	year = {2006},
	pages = {259--274},
	file = {Citeseer - Full Text PDF:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\QIFNNC98\Tappenbeck et al. - 2006 - Distance-Based Transfer Function Design Specifica.pdf:application/pdf;Citeseer - Snapshot:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\IQBJ59V6\summary.html:text/html}
}

@article{correa_size-based_2008,
	title = {Size-based Transfer Functions: A New Volume Exploration Technique},
	volume = {14},
	issn = {1077-2626},
	shorttitle = {Size-based Transfer Functions},
	doi = {10.1109/TVCG.2008.162},
	abstract = {The visualization of complex {3D} images remains a challenge, a fact that is magnified by the difficulty to classify or segment volume data. In this paper, we introduce size-based transfer functions, which map the local scale of features to color and opacity. Features in a data set with similar or identical scalar values can be classified based on their relative size. We achieve this with the use of scale fields, which are {3D} fields that represent the relative size of the local feature at each voxel. We present a mechanism for obtaining these scale fields at interactive rates, through a continuous scale-space analysis and a set of detection filters. Through a number of examples, we show that size-based transfer functions can improve classification and enhance volume rendering techniques, such as maximum intensity projection. The ability to classify objects based on local size at interactive rates proves to be a powerful method for complex data exploration.},
	number = {6},
	journal = {{IEEE} Transactions on Visualization and Computer Graphics},
	author = {Correa, C. and Ma, Kwan-Liu},
	month = dec,
	year = {2008},
	keywords = {{3D} fields, Algorithms, complex {3D} images, complex data exploration, data visualisation, detection filters, Diagnostic Imaging, {GPU} Techniques, image classification, image enhancement, Image Interpretation, Computer-Assisted, image representation, image segmentation, Imaging, Three-Dimensional, Index Terms\&\#8212, interactive rates, Interactive Visualization, maximum intensity projection, opacity, Pattern Recognition, Automated, rendering (computer graphics), Reproducibility of Results, Scale Space, Sensitivity and Specificity, size-based transfer functions, transfer functions, volume exploration technique, volume rendering},
	pages = {1380--1387},
	file = {IEEE Xplore Abstract Record:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\BVEHDVKE\abs_all.html:text/html;IEEE Xplore Full Text PDF:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\QAAQT4ER\Correa and Ma - 2008 - Size-based Transfer Functions A New Volume Explor.pdf:application/pdf}
}

@article{caban_texture-based_2008,
	title = {Texture-based Transfer Functions for Direct Volume Rendering},
	volume = {14},
	issn = {1077-2626},
	doi = {10.1109/TVCG.2008.169},
	abstract = {Visualization of volumetric data faces the difficult task of finding effective parameters for the transfer functions. Those parameters can determine the effectiveness and accuracy of the visualization. Frequently, volumetric data includes multiple structures and features that need to be differentiated. However, if those features have the same intensity and gradient values, existing transfer functions are limited at effectively illustrating those similar features with different rendering properties. We introduce texture-based transfer functions for direct volume rendering. In our approach, the voxelpsilas resulting opacity and color are based on local textural properties rather than individual intensity values. For example, if the intensity values of the vessels are similar to those on the boundary of the lungs, our texture-based transfer function will analyze the textural properties in those regions and color them differently even though they have the same intensity values in the volume. The use of texture-based transfer functions has several benefits. First, structures and features with the same intensity and gradient values can be automatically visualized with different rendering properties. Second, segmentation or prior knowledge of the specific features within the volume is not required for classifying these features differently. Third, textural metrics can be combined and/or maximized to capture and better differentiate similar structures. We demonstrate our texture-based transfer function for direct volume rendering with synthetic and real-world medical data to show the strength of our technique.},
	number = {6},
	journal = {{IEEE} Transactions on Visualization and Computer Graphics},
	author = {Caban, {J.J.} and Rheingans, P.},
	month = dec,
	year = {2008},
	keywords = {data variability, data visualisation, Direct volume rendering, feature classification, feature extraction, gradient methods, gradient value, image classification, image colour analysis, image texture, Index Terms\&\#8212, medical imaging, rendering (computer graphics), statistical analysis, texture-based transfer function, transfer functions, visualization, volume rendering, volumetric data face visualization},
	pages = {1364--1371},
	file = {IEEE Xplore Abstract Record:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\KRAHM5PT\abs_all.html:text/html;IEEE Xplore Full Text PDF:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\VMESB38F\Caban and Rheingans - 2008 - Texture-based Transfer Functions for Direct Volume.pdf:application/pdf}
}

@article{drebin_volume_1988,
	title = {Volume rendering},
	volume = {22},
	issn = {0097-8930},
	url = {http://doi.acm.org/10.1145/378456.378484},
	doi = {10.1145/378456.378484},
	abstract = {A technique for rendering images of volumes containing mixtures of materials is presented. The shading model allows both the interior of a material and the boundary between materials to be colored. Image projection is performed by simulating the absorption of light along the ray path to the eye. The algorithms used are designed to avoid artifacts caused by aliasing and quantization and can be efficiently implemented on an image computer. Images from a variety of applications are shown.},
	number = {4},
	urldate = {2013-04-14},
	journal = {{SIGGRAPH} Comput. Graph.},
	author = {Drebin, Robert A. and Carpenter, Loren and Hanrahan, Pat},
	month = jun,
	year = {1988},
	keywords = {computer tomography, image processing, magnetic resonance imaging ({MRI)}, medical imaging, non-destructive evaluation ({NDE)}, scientific visualization},
	pages = {65-74},
	file = {ACM Full Text PDF:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\QIQPXZBX\Drebin et al. - 1988 - Volume rendering.pdf:application/pdf}
}

@book{durand_perceptual_2002,
	address = {New York},
	series = {{ACM} {SIGGRAPH} 2002 {Course} {Notes}},
	title = {Perceptual and {Artistic} {Principles} for {Effective} {Computer} {Depiction}},
	volume = {13},
	url = {http://graphics.lcs.mit.edu/ fredo/SIG02_ArtScience/},
	editor = {Durand, Fr{\'e}do},
	year = {2002}
}

@inproceedings{sbert_information_2011,
	address = {New York, {NY}, {USA}},
	series = {{SA} '11},
	title = {Information theory in computer graphics and visualization},
	isbn = {978-1-4503-1135-9},
	url = {http://doi.acm.org/10.1145/2077434.2077443},
	doi = {10.1145/2077434.2077443},
	abstract = {We present a half-day course to review several information theory applications for computer graphics and visualization. Information theory tools, widely used in scientific fields such as engineering, physics, genetics and neuroscience, are also emerging as useful transversal tools in computer graphics and related fields. We introduce the basic concepts of information theory and how they map into application areas. Application areas in computer graphics include viewpoint selection, mesh saliency, scene exploration, ambient occlusion, geometry simplification, radiosity, adaptive ray-tracing and shape descriptors. Application areas in visualization are view selection for volume data, flow visualization, ambient occlusion, time-varying volume visualization, transfer function definition, time-varying volume visualization, iso-surface similarity maps and quality metrics. The applications fall broadly into two categories: the mapping of the problem to an information channel - as in viewpoint applications - and the direct use of measures such as entropy, Kullback-Leibler distance, Jensen-Shannon divergence, and f-divergences. These would be used to evaluate, for instance, the homogeneity of a set of samples being used as metrics. We will also discuss the potential applications of the information bottleneck method that allows us to progressively extract or merge information in a hierarchical structure.},
	urldate = {2013-05-02},
	booktitle = {{SIGGRAPH} Asia 2011 Courses},
	publisher = {{ACM}},
	author = {Sbert, Mateu and Feixas, Miquel and Viola, Ivan and Rigau, Jaume and Chover, Miguel},
	year = {2011},
	pages = {10:1-10:58}
}

@article{pessoa_finding_1998,
	title = {Finding out about filling-in: a guide to perceptual completion for visual science and the philosophy of perception},
	volume = {21},
	issn = {0140-525X},
	shorttitle = {Finding out about filling-in},
	abstract = {In visual science the term filling-in is used in different ways, which often leads to confusion. This target article presents a taxonomy of perceptual completion phenomena to organize and clarify theoretical and empirical discussion. Examples of boundary completion (illusory contours) and featural completion (color, brightness, motion, texture, and depth) are examined, and single-cell studies relevant to filling-in are reviewed and assessed. Filling-in issues must be understood in relation to theoretical issues about neural-perceptual isomorphism and linking propositions. Six main conclusions are drawn: (1) visual filling-in comprises a multitude of different perceptual completion phenomena; (2) certain forms of visual completion seem to involve spatially propagating neural activity (neural filling-in) and so, contrary to Dennett's (1991; 1992) recent discussion of filling-in, cannot be described as results of the brain's "ignoring an absence" or "jumping to a conclusion"; (3) in certain cases perceptual completion seems to have measurable effects that depend on neural signals representing a presence rather than ignoring an absence; (4) neural filling-in does not imply either "analytic isomorphism" or "Cartesian materialism," and thus the notion of the bridge locus--a particular neural stage that forms the immediate substrate of perceptual experience--is problematic and should be abandoned; (5) to reject the representational conception of vision in favor of an "enactive" or "animate" conception reduces the importance of filling-in as a theoretical category in the explanation of vision; and (6) the evaluation of perceptual content should not be determined by "subpersonal" considerations about internal processing, but rather by considerations about the task of vision at the level of the animal or person interacting with the world.},
	language = {eng},
	number = {6},
	journal = {The Behavioral and brain sciences},
	author = {Pessoa, L and Thompson, E and No{\"e}, A},
	month = dec,
	year = {1998},
	pmid = {10191878},
	keywords = {Humans, Optical Illusions, psychophysics, Time Factors, Vision, Ocular, Visual Cortex, Visual Perception},
	pages = {723--748; discussion 748--802}
}

@article{serlie_classifying_2007,
	title = {Classifying {CT} Image Data Into Material Fractions by a Scale and Rotation Invariant Edge Model},
	volume = {16},
	issn = {1057-7149},
	doi = {10.1109/TIP.2007.909407},
	abstract = {A fully automated method is presented to classify 3-D {CT} data into material fractions. An analytical scale-invariant description relating the data value to derivatives around Gaussian blurred step edges - arch model - is applied to uniquely combine robustness to noise, global signal fluctuations, anisotropic scale, noncubic voxels, and ease of use via a straightforward segmentation of 3-D {CT} images through material fractions. Projection of noisy data value and derivatives onto the arch yields a robust alternative to the standard computed Gaussian derivatives. This results in a superior precision of the method. The arch-model parameters are derived from a small, but over-determined, set of measurements (data values and derivatives) along a path following the gradient uphill and downhill starting at an edge voxel. The model is first used to identify the expected values of the two pure materials (named and ) and thereby classify the boundary. Second, the model is used to approximate the underlying noise-free material fractions for each noisy measurement. An iso-surface of constant material fraction accurately delineates the material boundary in the presence of noise and global signal fluctuations. This approach enables straightforward segmentation of 3-D {CT} images into objects of interest for computer-aided diagnosis and offers an easy tool for the design of otherwise complicated transfer functions in high-quality visualizations. The method is applied to segment a tooth volume for visualization and digital cleansing for virtual colonoscopy.},
	number = {12},
	journal = {{IEEE} Transactions on Image Processing},
	author = {Serlie, I. W O and Vos, {F.M.} and Truyen, R. and Post, {F.H.} and van Vliet, {L.J.}},
	year = {2007},
	keywords = {Algorithms, Anisotropic Gaussian point spread function ({PSF)}, Anisotropic magnetoresistance, Artificial Intelligence, Computed tomography, computer-aided diagnosis, computerised tomography, {CT} image data classification, edge detection, Fluctuations, Gaussian blurred step edges, Gaussian noise, Gaussian processes, Image analysis, image classification, image segmentation, Imaging, Three-Dimensional, material fractions, medical image processing, Noise measurement, Noise robustness, noisy data, object segmentation, partial volume effect ({PVE)}, Pattern Recognition, Automated, Radiographic Image Enhancement, Radiographic Image Interpretation, Computer-Assisted, Reproducibility of Results, Rotation, rotation invariant edge model, Sensitivity and Specificity, Signal analysis, Tomography, X-Ray Computed, transfer function for visualization, virtual colonoscopy, visualization, voxel classification},
	pages = {2891--2904},
	file = {IEEE Xplore Abstract Record:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\9TDVPAE3\articleDetails.html:text/html;IEEETIP2007_Serlie.pdf:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\67EWGBZZ\IEEETIP2007_Serlie.pdf:application/pdf}
}

@article{bruckner_style_2007,
	title = {Style Transfer Functions for Illustrative Volume Rendering},
	volume = {26},
	issn = {1467-8659},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1467-8659.2007.01095.x/abstract},
	doi = {10.1111/j.1467-8659.2007.01095.x},
	abstract = {Illustrative volume visualization frequently employs non-photorealistic rendering techniques to enhance important features or to suppress unwanted details. However, it is difficult to integrate multiple non-photorealistic rendering approaches into a single framework due to great differences in the individual methods and their parameters. In this paper, we present the concept of style transfer functions. Our approach enables flexible data-driven illumination which goes beyond using the transfer function to just assign colors and opacities. An image-based lighting model uses sphere maps to represent non-photorealistic rendering styles. Style transfer functions allow us to combine a multitude of different shading styles in a single rendering. We extend this concept with a technique for curvature-controlled style contours and an illustrative transparency model. Our implementation of the presented methods allows interactive generation of high-quality volumetric illustrations.},
	language = {en},
	number = {3},
	urldate = {2012-11-14},
	journal = {Computer Graphics Forum},
	author = {Bruckner, S. and Gr{\"o}ller, M. E.},
	year = {2007},
	keywords = {I.3.3 [Computer Graphics]: {Picture/Image} Generation, I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism, illustrative visualization, Transfer functions, volume rendering},
	pages = {715-724}
}

@inproceedings{tateosian_engaging_2007,
	address = {New York, {NY}, {USA}},
	series = {{NPAR} '07},
	title = {Engaging viewers through nonphotorealistic visualizations},
	isbn = {978-1-59593-624-0},
	url = {http://doi.acm.org/10.1145/1274871.1274886},
	doi = {10.1145/1274871.1274886},
	abstract = {Research in human visual cognition suggests that beautiful images can engage the visual system, encouraging it to linger in certain locations in an image and absorb subtle details. By developing aesthetically pleasing visualizations of data, we aim to engage viewers and promote prolonged inspection, which can lead to new discoveries within the data. We present three new visualization techniques that apply painterly rendering styles to vary interpretational complexity ({IC)}, indication and detail ({ID)}, and visual complexity ({VC)}, image properties that are important to aesthetics. Knowledge of human visual perception and psychophysical models of aesthetics provide the theoretical basis for our designs. Computational geometry and nonphotorealistic algorithms are used to preprocess the data and render the visualizations. We demonstrate the techniques with visualizations of real weather and supernova data.},
	urldate = {2013-04-29},
	booktitle = {Proceedings of the 5th international symposium on Non-photorealistic animation and rendering},
	publisher = {{ACM}},
	author = {Tateosian, Laura G. and Healey, Christopher G. and Enns, James T.},
	year = {2007},
	keywords = {aesthetics, mesh simplification, nonphotorealistic rendering, {NPR} applications, visualization, voronoi diagrams},
	pages = {93-102},
	file = {ACM Full Text PDF:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\URDB9TXZ\Tateosian et al. - 2007 - Engaging viewers through nonphotorealistic visuali.pdf:application/pdf}
}

@article{ebert_designing_2002,
	title = {Designing effective transfer functions for volume rendering from photographic volumes},
	volume = {8},
	issn = {1077-2626},
	doi = {10.1109/2945.998670},
	abstract = {Photographic volumes present a unique, interesting challenge for volume rendering. In photographic volumes, the voxel color is pre-determined, making color selection through transfer functions unnecessary. However, photographic data does not contain a clear mapping from the multi-valued color values to a scalar density or opacity, making projection and compositing much more difficult than with traditional volumes. Moreover, because of the nonlinear nature of color spaces, there is no meaningful norm for the multi-valued voxels. Thus, the individual color channels of photographic data must be treated as incomparable data tuples rather than as vector values. Traditional differential geometric tools, such as intensity gradients, density and Laplacians, are distorted by the nonlinear non-orthonormal color spaces that are the domain of the voxel values. We have developed different techniques for managing these issues while directly rendering volumes from photographic data. We present and justify the normalization of color values by mapping {RGB} values to the {CIE} L*u*v* color space. We explore and compare different opacity transfer functions that map three-channel color values to opacity. We apply these many-to-one mappings to the original {RGB} values as well as to the voxels after conversion to L*u*v* space. Direct rendering using transfer functions allows us to explore photographic volumes without having to commit to an a-priori segmentation that might mask fine variations of interest. We empirically compare the combined effects of each of the two color spaces with our opacity transfer functions using source data from the Visible Human project},
	number = {2},
	journal = {{IEEE} Transactions on Visualization and Computer Graphics},
	author = {Ebert, {D.S.} and Morris, {C.J.} and Rheingans, P. and Yoo, {T.S.}},
	year = {2002},
	keywords = {{CIE} L*u*v* color space, color channels, colour graphics, colour photography, compositing, differential geometry, distortion, fine variations, image colour analysis, incomparable data tuples, intensity gradients, Laplacians, many-to-one mappings, multi-valued color values, nonlinear nonorthonormal color space, normalization, opacity, optical transfer function, photographic volumes, projection, rendering (computer graphics), {RGB} values, scalar density, Transfer function design, transfer functions, Visible Human project, volume rendering, voxel color},
	pages = {183--197},
	file = {IEEE Xplore Abstract Record:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\TSCJCAGE\articleDetails.html:text/html}
}

@article{hauser_two-level_2001,
	title = {Two-level volume rendering},
	volume = {7},
	issn = {1077-2626},
	doi = {10.1109/2945.942692},
	abstract = {Presents a two-level approach for volume rendering, which allows for selectively using different rendering techniques for different subsets of a {3D} data set. Different structures within the data set are rendered locally on an object-by-object basis by either direct volume rendering ({DVR)}, maximum-intensity projection ({MIP)}, surface rendering, value integration (X-ray-like images) or non-photorealistic rendering ({NPR).} All the results of subsequent object renderings are combined globally in a merging step (usually compositing in our case). This allows us to selectively choose the most suitable technique for depicting each object within the data while keeping the amount of information contained in the image at a reasonable level. This is especially useful when inner structures should be visualized together with semi-transparent outer parts, similar to the focus+context approach known from information visualization. We also present an implementation of our approach which allows us to explore volumetric data using two-level rendering at interactive frame rates},
	number = {3},
	journal = {{IEEE} Transactions on Visualization and Computer Graphics},
	author = {Hauser, H. and Mroz, L. and Italo Bischi, G. and Gr{\"o}ller, E.},
	month = sep,
	year = {2001},
	keywords = {2-level volume rendering, {3D} data subsets, Biomedical equipment, Biomedical imaging, compositing, Computer Graphics, Context, data visualisation, data visualization, Direct volume rendering, dynamical systems, focus, Focusing, Humans, information visualization, inner structure visualization, interactive frame rates, maximum-intensity projection, medical applications, Medical services, merging, nonphotorealistic rendering, object depiction techniques, object-by-object local rendering, rendering (computer graphics), semi-transparent outer parts, solid modelling, surface rendering, value integration, volumetric data, X-ray imaging, X-ray-like images},
	pages = {242--252},
	file = {IEEE Xplore Abstract Record:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\32SNWCER\abs_all.html:text/html}
}

@article{corcoran_perceptual_2010,
	title = {Perceptual enhancement of two-level volume rendering},
	volume = {34},
	issn = {0097-8493},
	shorttitle = {Procedural Methods in Computer Graphics Illustrative Visualization},
	url = {http://www.sciencedirect.com/science/article/pii/S0097849310000531},
	doi = {10.1016/j.cag.2010.03.014},
	abstract = {We present a system for interactive visualisation of {3D} volumetric medical datasets combined with perceptual evaluation of how such visualisations can affect a user's interpretation of scenes and attention. Enhancements to traditional volume renderings are provided through a two-level volume rendering strategy which employs fast {GPU-based} direct volume rendering ({DVR)} coupled with an additional layer of perceptual cues derived from various techniques from the non-photorealistic rendering ({NPR)} literature. The two-level approach allows us to successfully separate the most relevant data from peripheral extraneous detail enabling the user to more effectively understand the visual information. Peripheral details are abstracted but sufficiently retained in order to provide spatial reference. We perform a number of perceptual user experiments which test how this approach affects a user's attention and ability to determine the shape of an object. Results indicate that our approach can provide a significant improvement in user perception of shape in complex visualisations, especially when a user has little or no prior knowledge of the data. Our approach would prove extremely useful in technical, medical or scientific visualisations to improve understanding of detailed volumetric datasets.},
	number = {4},
	urldate = {2013-07-08},
	journal = {Computers \& Graphics},
	author = {Corcoran, Andrew and Redmond, Niall and Dingliana, John},
	month = aug,
	year = {2010},
	keywords = {non-photorealistic rendering, Visualisation, volume rendering},
	pages = {388--397},
	file = {ScienceDirect Full Text PDF:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\H5FE6DDG\Corcoran et al. - 2010 - Perceptual enhancement of two-level volume renderi.pdf:application/pdf;ScienceDirect Snapshot:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\CDXFVBJ4\S0097849310000531.html:text/html}
}

@inproceedings{interrante_strategies_1997,
	address = {Los Alamitos, {CA}, {USA}},
	series = {{VIS} '97},
	title = {Strategies for effectively visualizing {3D} flow with volume {LIC}},
	isbn = {1-58113-011-2},
	url = {http://dl.acm.org/citation.cfm?id=266989.267112},
	urldate = {2013-09-10},
	booktitle = {Proceedings of the 8th conference on Visualization '97},
	publisher = {{IEEE} Computer Society Press},
	author = {Interrante, Victoria and Grosch, Chester},
	year = {1997},
	pages = {421-ff.},
	file = {interran.pdf:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\9IZBRUW4\interran.pdf:application/pdf}
}

@article{liu_texture-based_2005,
	title = {A Texture-Based Hardware-Independent Technique for Time-Varying Volume Flow Visualization},
	volume = {8},
	issn = {1343-8875},
	url = {http://dx.doi.org/10.1007/BF03181501},
	doi = {10.1007/BF03181501},
	abstract = {Existing texture-based {3D} flow visualization techniques, e.g., volume Line Integral Convolution ({LIC)}, are either limited to steady flows or dependent on special-purpose graphics cards. In this paper we present a texture-based hardware-independent technique for time-varying volume flow visualization. It is based on our Accelerated Unsteady Flow {LIC} ({AUFLIC)} algorithm (Liu and Moorhead, 2005), which uses a flow-driven seeding strategy and a dynamic seeding controller to reuse pathlines in the value scattering process to achieve fast time-dependent flow visualization with high temporal-spatial coherence. We extend {AUFLIC} to {3D} scenarios for accelerated generation of volume flow textures. To address occlusion, lack of depth cuing, and poor perception of flow directions within a dense volume, we employ magnitude-based transfer functions and cutting planes in volume rendering to clearly show the flow structure and the flow evolution.},
	number = {3},
	urldate = {2013-09-14},
	journal = {J. Vis.},
	author = {Liu, Zhanping and {Moorhead,II}, Robert J.},
	month = aug,
	year = {2005},
	keywords = {{LIC}, texture-based flow visualization, {UFLIC}, unsteady {3D} flow, volume rendering},
	pages = {235-244},
	file = {VAUFLIC.pdf:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\SB4XS66N\VAUFLIC.pdf:application/pdf}
}

@inproceedings{urnessy_techniques_2004,
	address = {Aire-la-Ville, Switzerland, Switzerland},
	series = {{VISSYM'04}},
	title = {Techniques for visualizing multi-valued flow data},
	isbn = {3-905673-07-X},
	url = {http://dx.doi.org/10.2312/VisSym/VisSym04/165-172},
	doi = {10.2312/VisSym/VisSym04/165-172},
	abstract = {In this paper we discuss several techniques to display multiple scalar distributions within an image depicting a {2D} flow field. We first address how internal contrast and mean luminance can effectively be used to represent a scalar distribution in addition to an underlying flow field. Secondly, we expand upon a current technique to more effectively use luminance ramps over dense streamlines to represent direction of flow. Lastly, we present a new method, based on embossing, to encode the out-of-plane component of a {3D} vector field defined over a {2D} domain. Throughout this paper, we limit our focus to the visualization of steady flows.},
	urldate = {2013-09-14},
	booktitle = {Proceedings of the Sixth Joint Eurographics - {IEEE} {TCVG} conference on Visualization},
	publisher = {Eurographics Association},
	author = {Urnessy, Timothy and Interrante, Victoria and Longmire, Ellen and Marusic, Ivan and Ganapathisubramani, Bharathram},
	year = {2004},
	pages = {165-172},
	file = {vissym04.pdf:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\NUP6NMG9\vissym04.pdf:application/pdf}
}

@inproceedings{kaufman_volume_1997,
	title = {Volume Visualization: Principles and Advances},
	shorttitle = {Volume Visualization},
	abstract = {This paper is a survey ofvolume visualization. It includes an introduction to volumetric data; surface rendering techniques for volume data; volume rendering techniques, including image-order,objectorder, and domain techniques; optimization methods for volume rendering; special-purpose volume rendering hardware; global illumination of volumetric data, including volumetric ray tracing and volumetric radiosity; irregular grid rendering; and volume graphics, with several volume modeling techniques, such as voxelization, texture mapping, amorphous phenomena, block operations, constructive solid modeling, and volume sculpting.},
	booktitle = {{ACM} {SIGGRAPH} Course Notes},
	author = {Kaufman, Arie E.},
	year = {1997},
	pages = {23--35},
	file = {Citeseer - Full Text PDF:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\vuyr0ioc.default\zotero\storage\43IDCDFC\Kaufman - 2003 - Volume Visualization Principles and Advances.pdf:application/pdf;Citeseer - Snapshot:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\vuyr0ioc.default\zotero\storage\UHJ4CJVV\summary.html:text/html}
}

@inproceedings{haeberli_paint_1990,
	address = {New York, {NY}, {USA}},
	series = {{SIGGRAPH} '90},
	title = {Paint by numbers: abstract image representations},
	isbn = {0-89791-344-2},
	shorttitle = {Paint by numbers},
	url = {http://doi.acm.org/10.1145/97879.97902},
	doi = {10.1145/97879.97902},
	abstract = {Computer graphics research has concentrated on creating photo-realistic images of synthetic objects. These images communicate surface shading and curvature, as well as the depth relationships of objects in a scene. These renderings are traditionally represented by a rectangular array of pixels that tile the image {plane.As} an alternative to photo-realism, it is possible to create abstract images using an ordered collection of brush strokes. These abstract images filter and refine visual information before it is presented to the viewer. By controlling the color, shape, size, and orientation of individual brush strokes, impressionistic paintings of computer generated or photographic images can easily be created.},
	urldate = {2013-10-14},
	booktitle = {Proceedings of the 17th annual conference on Computer graphics and interactive techniques},
	publisher = {{ACM}},
	author = {Haeberli, Paul},
	year = {1990},
	pages = {207-214},
	file = {download.pdf:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\JDEK58AV\download.pdf:application/pdf}
}

@article{tory_human_2004,
	title = {Human factors in visualization research},
	volume = {10},
	issn = {1077-2626},
	doi = {10.1109/TVCG.2004.1260759},
	abstract = {Visualization can provide valuable assistance for data analysis and decision making tasks. However, how people perceive and interact with a visualization tool can strongly influence their understanding of the data as well as the system's usefulness. Human factors therefore contribute significantly to the visualization process and should play an important role in the design and evaluation of visualization tools. Several research initiatives have begun to explore human factors in visualization, particularly in perception-based design. Nonetheless, visualization work involving human factors is in its infancy, and many potentially promising areas have yet to be explored. Therefore, we aim to 1) review known methodology for doing human factors research, with specific emphasis on visualization, 2) review current human factors research in visualization to provide a basis for future investigation, and 3) identify promising areas for future research.},
	number = {1},
	journal = {{IEEE} Transactions on Visualization and Computer Graphics},
	author = {Tory, M. and M{\"o}ller, T.},
	year = {2004},
	keywords = {Algorithms, Biomedical imaging, cognition, cognitive support, Computer Graphics, data analysis, data visualisation, data visualization, decision making, decision making task, Fluid flow, Geographic Information Systems, Human Engineering, human factors, Image Interpretation, Computer-Assisted, Imaging, Three-Dimensional, Information Storage and Retrieval, Medical simulation, Online Systems, Pattern Recognition, Automated, perception, research, Research initiatives, Signal Processing, Computer-Assisted, Terminology, User-Computer Interface, Visual Perception, visualization},
	pages = {72--84},
	file = {IEEE Xplore Abstract Record:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\BMR52CM5\articleDetails.html:text/html;tvcg04.pdf:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\HKW6AX8T\tvcg04.pdf:application/pdf}
}

@inproceedings{schumann_assessing_1996,
	address = {New York, {NY}, {USA}},
	series = {{CHI} '96},
	title = {Assessing the effect of non-photorealistic rendered images in {CAD}},
	isbn = {0-89791-777-4},
	url = {http://doi.acm.org/10.1145/238386.238398},
	doi = {10.1145/238386.238398},
	urldate = {2013-11-03},
	booktitle = {Proceedings of the {SIGCHI} Conference on Human Factors in Computing Systems},
	publisher = {{ACM}},
	author = {Schumann, Jutta and Strothotte, Thomas and Laser, Stefan and Raab, Andreas},
	year = {1996},
	keywords = {architectural presentation, {CAD}, non-photorealistic rendering, preliminary drafts, sketches},
	pages = {35-41}
}

@article{shannon_mathematical_1948,
	title = {A Mathematical Theory of Communication},
	volume = {27},
	url = {http://cm.bell-labs.com/cm/ms/what/shannonday/shannon1948.pdf},
	number = {3},
	journal = {The Bell System Technical Journal},
	author = {Shannon, Claude E.},
	month = oct,
	year = {1948},
	keywords = {communication, entropy, information, shannon, toread},
	pages = {379--423},
	file = {shannon1948.pdf:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\SSBZHFXA\shannon1948.pdf:application/pdf}
}

@misc{website:Roettger_volume_2013,
      author = {Roettger, Stefan},
      title = {The Volume Library},
      month = {January},
      year = {2006},
      url = {http://lgdv.cs.fau.de/External/vollib/},
      howpublished = "\url{http://lgdv.cs.fau.de/External/vollib/}",
      note = {Accessed: 2013-01-20},
      doi = {}
}

@misc{website:Ma_repository_2013,
      author = {Ma, Kwan-Liu},
      title = {Time-Varying Data Repository},
      month = {October},
      year = {2003},
      url = {http://www.cs.ucdavis.edu/~ma/ITR/},
      howpublished = "\url{http://www.cs.ucdavis.edu/~ma/ITR/}",
      note = {Accessed: 2013-02-10},
      doi = {}
}


@misc{website:Voreen_datasets_2013,
	author = {Pra{\ss}ni, J{\"o}rg-Stefan},
	title = {Voreen Data Sets},
	month = {December},
	year = {2013},
	url = {http://www.uni-muenster.de/Voreen/download/workspaces_and_data_sets.html},
	howpublished = "\url{http://www.uni-muenster.de/Voreen/download/workspaces_and_data_sets.html}",
	note = {Accessed: 2013-12-20},
	doi = {}
}


@misc{website:stanford_volume_data,
	author = {Levoy, Marc},
	title = {The Stanford volume data archive},
	year = {2001},
	url = {https://graphics.stanford.edu/data/voldata/},
	howpublished = "\url{https://graphics.stanford.edu/data/voldata/}",
	note = {Accessed: 2014-08-20},
	doi = {}
}

@phdthesis{redmond_influencing_2010,
	address = {Dublin, Ireland},
	type = {{PhD} {Thesis}},
	title = {Influencing user perception using real-time adaptive abstraction},
	school = {Trinity College Dublin},
	author = {Redmond, Niall},
	year = {2010},
	keywords = {Computer Studies, Ph.D, Dissertations, Ph.D. thesis Trinity College Dublin, Trinity College (Dublin, Ireland)},
	file = {redmond2011.pdf:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\UEK9748W\\redmond2011.pdf:application/pdf}
}

@article{isenberg_systematic_2013,
	title = {A Systematic Review on the Practice of Evaluating Visualization},
	volume = {19},
	issn = {1077-2626},
	doi = {10.1109/TVCG.2013.126},
	abstract = {We present an assessment of the state and historic development of evaluation practices as reported in papers published at the {IEEE} Visualization conference. Our goal is to reflect on a meta-level about evaluation in our community through a systematic understanding of the characteristics and goals of presented evaluations. For this purpose we conducted a systematic review of ten years of evaluations in the published papers using and extending a coding scheme previously established by Lam et al. [2012]. The results of our review include an overview of the most common evaluation goals in the community, how they evolved over time, and how they contrast or align to those of the {IEEE} Information Visualization conference. In particular, we found that evaluations specific to assessing resulting images and algorithm performance are the most prevalent (with consistently 80-90\% of all papers since 1997). However, especially over the last six years there is a steady increase in evaluation methods that include participants, either by evaluating their performances and subjective feedback or by evaluating their work practices and their improved analysis and reasoning capabilities using visual tools. Up to 2010, this trend in the {IEEE} Visualization conference was much more pronounced than in the {IEEE} Information Visualization conference which only showed an increasing percentage of evaluation through user performance and experience testing. Since 2011, however, also papers in {IEEE} Information Visualization show such an increase of evaluations of work practices and analysis as well as reasoning using visual tools. Further, we found that generally the studies reporting requirements analyses and domain-specific work practices are too informally reported which hinders cross-comparison and lowers external validity.},
	number = {12},
	journal = {{IEEE} Transactions on Visualization and Computer Graphics},
	author = {Isenberg, Tobias and Isenberg, Petra and Chen, Jian and Sedlmair, Michael and M{\"o}ller, Torsten},
	year = {2013},
	keywords = {data visualization, encoding, evaluation, History, information visualization, Mathematical model, scientific visualization, systematic review, Systematics, validation, visualization},
	pages = {2818--2827},
	file = {IEEE Xplore Abstract Record:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\CGP6DAII\articleDetails.html:text/html;Isenberg_2013_SRP.pdf:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\6JMU8FGR\Isenberg_2013_SRP.pdf:application/pdf}
}

@article{lam_empirical_2012,
	title = {Empirical Studies in Information Visualization: Seven Scenarios},
	volume = {18},
	issn = {1077-2626},
	shorttitle = {Empirical Studies in Information Visualization},
	doi = {10.1109/TVCG.2011.279},
	abstract = {We take a new, scenario-based look at evaluation in information visualization. Our seven scenarios, evaluating visual data analysis and reasoning, evaluating user performance, evaluating user experience, evaluating environments and work practices, evaluating communication through visualization, evaluating visualization algorithms, and evaluating collaborative data analysis were derived through an extensive literature review of over 800 visualization publications. These scenarios distinguish different study goals and types of research questions and are illustrated through example studies. Through this broad survey and the distillation of these scenarios, we make two contributions. One, we encapsulate the current practices in the information visualization research community and, two, we provide a different approach to reaching decisions about what might be the most effective evaluation of a given information visualization. Scenarios can be used to choose appropriate research questions and goals and the provided examples can be consulted for guidance on how to design one's own study.},
	number = {9},
	journal = {{IEEE} Transactions on Visualization and Computer Graphics},
	author = {Lam, H. and Bertini, E. and Isenberg, P. and Plaisant, C. and Carpendale, S.},
	year = {2012},
	keywords = {data analysis, data visualisation, data visualization, Electronic mail, empirical studies, encoding, evaluating collaborative data analysis, evaluating communication, evaluating environments, evaluating user experience, evaluating visualization algorithms, evaluation., information visualization, seven scenarios, Systematics, Taxonomy, user performance evaluation, visual data analysis, visual data reasoning, visualization, visualization publications},
	pages = {1520--1536},
	file = {IEEE Xplore Abstract Record:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\3IUS28QT\login.html:text/html;tvcg2011-seven-scenarios.pdf:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\U46X2DVB\tvcg2011-seven-scenarios.pdf:application/pdf}
}

@techreport{chen_what_2013,
	type = {{arXiv} e-print},
	title = {What is Visualization Really for?},
	url = {http://arxiv.org/abs/1305.5670},
	abstract = {Whenever a visualization researcher is asked about the purpose of visualization, the phrase "gaining insight" by and large pops out instinctively. However, it is not absolutely factual that all uses of visualization are for gaining a deep understanding, unless the term insight is broadened to encompass all types of thought. Even when insight is the focus of a visualization task, it is rather difficult to know what insight is gained, how much, or how accurate. In this paper, we propose that "saving time" in accomplishing a user's task is the most fundamental objective. By giving emphasis to saving time, we can establish a concrete metric, alleviate unnecessary contention caused by different interpretations of insight, and stimulate new research efforts in some aspects of visualization, such as empirical studies, design optimisation and theories of visualization.},
	number = {1305.5670},
	urldate = {2013-11-06},
	author = {Chen, Min and Floridi, Luciano and Borgo, Rita},
	month = may,
	year = {2013},
	keywords = {Computer Science - Human-Computer Interaction},
	file = {1305.5670 PDF:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\VSAKTRKT\Chen et al. - 2013 - What is Visualization Really for.pdf:application/pdf;arXiv.org Snapshot:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\MK5VRHKT\1305.html:text/html}
}

@inproceedings{anderson_evaluating_2012,
	address = {New York, {NY}, {USA}},
	series = {{BELIV} '12},
	title = {Evaluating visualization using cognitive measures},
	isbn = {978-1-4503-1791-7},
	url = {http://doi.acm.org/10.1145/2442576.2442581},
	doi = {10.1145/2442576.2442581},
	abstract = {In this position paper, we discuss the problems and advantages of using physiological measurements to to estimate cognitive load in order to evaluate scientific visualization methods. We will present various techniques and technologies designed to measure cognitive load and how they may be leveraged in the context of user evaluation studies for scientific visualization. We also discuss the challenges of experiments designed to use these physiological measurements.},
	urldate = {2013-11-06},
	booktitle = {Proceedings of the 2012 {BELIV} Workshop: Beyond Time and Errors - Novel Evaluation Methods for Visualization},
	publisher = {{ACM}},
	author = {Anderson, Erik W.},
	year = {2012},
	keywords = {evaluation, human-computer interfaces, scientific visualization},
	pages = {5:1-5:4},
	file = {paper05.pdf:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\DURHC8P4\paper05.pdf:application/pdf}
}

@article{christopher_thoughts_2003,
	title = {Thoughts on User Studies: Why, How, and When},
	volume = {23},
	shorttitle = {Thoughts on User Studies},
	abstract = {This article describes our experiences with user studies. We offer some examples of our own studies, talk about the pitfalls and problems we encountered, and show how the results were applied to produce successful visualizations. Although our main goal is to encourage the use of studies in visualization, we recognize that other disciplines also offer important insights into visualization design, for example, the areas of visual design or the visual arts. We conclude by discussing when knowledge from these areas might be preferable to a traditional user study},
	number = {4},
	journal = {{IEEE} Computer Graphics and Applications},
	author = {Christopher, Robert Kosara and Healey, Christopher G. and Interrante, Victoria and Laidlaw, David H. and Ware, Colin},
	year = {2003},
	pages = {20--25},
	file = {Citeseer - Full Text PDF:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\XSM8W3TG\Christopher et al. - 2003 - Thoughts on User Studies Why, How, and When.pdf:application/pdf;Citeseer - Snapshot:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\3SN4D95S\summary.html:text/html}
}

@inproceedings{laidlaw_quantitative_2001,
	title = {Quantitative comparative evaluation of {2D} vector field visualization methods},
	doi = {10.1109/VISUAL.2001.964505},
	abstract = {Presents results from a user study that compared six visualization methods for {2D} vector data. Two methods used different distributions of short arrows, two used different distributions of integral curves, one used wedges located to suggest flow lines, and the final one was line-integral convolution ({LIC).} We defined three simple but representative tasks for users to perform using visualizations from each method: (1) locating all critical points in an image, (2) identifying critical point types, and (3) advecting a particle. The results show different strengths and weaknesses for each method. We found that users performed better with methods that: (1) showed the sign of vectors within the vector field, (2) visually represented integral curves, and (3) visually represented the locations of critical points. These results provide quantitative support for some of the anecdotal evidence concerning visualization methods. The tasks and testing framework also provide a basis for comparing other visualization methods, for creating more effective methods and for defining additional tasks to further understand tradeoffs among methods. They may also be useful for evaluating {2D} vectors on {2D} surfaces embedded in {3D} and for defining analogous tasks for {3D} visualization methods.},
	booktitle = {Visualization, 2001. {VIS} '01. Proceedings},
	author = {Laidlaw, {D.H.} and Kirby, {R.M.} and Davidson, {J.S.} and Miller, {T.S.} and da Silva, M. and Warren, {W.H.} and Tarr, M.},
	year = {2001},
	keywords = {{2D} surfaces, {2D} vector field visualization methods, Application software, Chromium, computational fluid dynamics, Computer applications, Computer Graphics, Computer Science, Convolution, critical point location, critical point types, critical points, data visualisation, data visualization, flow lines, flow visualisation, Fluid dynamics, graphical user interfaces, human factors, iconic textures, image texture, image-guided streamlines, integral curve distributions, jittered grid icons, line-integral convolution, mathematics, particle advection, quantitative comparative evaluation, scientific visualization, short arrow distributions, Streaming media, Testing, testing framework, tradeoffs, user performance study, vector sign, vectors, Visual Representation, wedges},
	pages = {143--150},
	file = {IEEE Xplore Abstract Record:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\CH3WTUPT\login.html:text/html}
}

@inproceedings{lombaert_spatio-temporal_2010,
	title = {Spatio-temporal segmentation of the heart in {4D} {MRI} images using graph cuts with motion cues},
	doi = {10.1109/ISBI.2010.5490303},
	abstract = {With the increasing availability of {4D} cardiac imaging technologies, the need for efficient spatio-temporal segmentation algorithms for the heart is growing. We propose a new method for heart segmentation in {4D} data sets. We efficiently use the established graph cut method for the segmentation of the heart by simultaneously exploiting motion and region cues. We construct a {4D} graph designed to find a moving object with a uniform intensity from a static background. This method has useful applications ranging from qualitative tasks such as direct visualization of the heart by removing its surrounding structures, to quantitative tasks such as measurements and analysis of the total heart volume. The method has been tested on cardiac {MRI} sequences with successful results.},
	booktitle = {2010 {IEEE} International Symposium on Biomedical Imaging: From Nano to Macro},
	author = {Lombaert, H. and Cheriet, F.},
	year = {2010},
	keywords = {4-D cardiac imaging, 4-D {MRI} image, biomedical {MRI}, cardiology, diseases, Echocardiography, graph cut method, Heart, Image motion analysis, image segmentation, image sequences, Magnetic analysis, Magnetic Resonance Imaging, medical image processing, Motion cues, motion picture, Motion pictures, Spatiotemporal phenomena, spatiotemporal segmentation, static background, Testing, total heart volume, uniform intensity, visualization, Volume measurement},
	pages = {492--495},
	file = {IEEE Xplore Abstract Record:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\M8IKEUSN\login.html:text/html;IEEE Xplore Full Text PDF:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\E4IEJ67G\Lombaert and Cheriet - 2010 - Spatio-temporal segmentation of the heart in 4D MR.pdf:application/pdf}
}

@article{boykov_graph_2006,
	title = {Graph Cuts and Efficient N-D Image Segmentation},
	volume = {70},
	issn = {0920-5691, 1573-1405},
	url = {http://link.springer.com/article/10.1007/s11263-006-7934-5},
	doi = {10.1007/s11263-006-7934-5},
	abstract = {Combinatorial graph cut algorithms have been successfully applied to a wide range of problems in vision and graphics. This paper focusses on possibly the simplest application of graph-cuts: segmentation of objects in image data. Despite its simplicity, this application epitomizes the best features of combinatorial graph cuts methods in vision: global optima, practical efficiency, numerical robustness, ability to fuse a wide range of visual cues and constraints, unrestricted topological properties of segments, and applicability to N-D problems. Graph cuts based approaches to object extraction have also been shown to have interesting connections with earlier segmentation methods such as snakes, geodesic active contours, and level-sets. The segmentation energies optimized by graph cuts combine boundary regularization with region-based properties in the same fashion as Mumford-Shah style functionals. We present motivation and detailed technical description of the basic combinatorial optimization framework for image segmentation via s/t graph cuts. After the general concept of using binary graph cut algorithms for object segmentation was first proposed and tested in Boykov and Jolly (2001), this idea was widely studied in computer vision and graphics communities. We provide links to a large number of known extensions based on iterative parameter re-estimation and learning, multi-scale or hierarchical approaches, narrow bands, and other techniques for demanding photo, video, and medical applications.},
	language = {en},
	number = {2},
	urldate = {2013-11-16},
	journal = {International Journal of Computer Vision},
	author = {Boykov, Yuri and Funka-Lea, Gareth},
	month = nov,
	year = {2006},
	keywords = {Artificial Intelligence (incl. Robotics), Computer Imaging, Graphics and Computer Vision, image processing, Pattern Recognition},
	pages = {109--131},
	file = {ijcv06.pdf:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\AU8CDC7Q\ijcv06.pdf:application/pdf;Snapshot:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\7J4HJKT3\10.html:text/html}
}

@article{burns_line_2005,
	title = {Line Drawings from Volume Data},
	volume = {24},
	issn = {0730-0301},
	url = {http://doi.acm.org/10.1145/1073204.1073222},
	doi = {10.1145/1073204.1073222},
	abstract = {Renderings of volumetric data have become an important data analysis tool for applications ranging from medicine to scientific simulation. We propose a volumetric drawing system that directly extracts sparse linear features, such as silhouettes and suggestive contours, using a temporally coherent seed-and-traverse framework. In contrast to previous methods based on isosurfaces or nonrefractive transparency, producing these drawings requires examining an asymptotically smaller subset of the data, leading to efficiency on large data sets. In addition, the resulting imagery is often more comprehensible than standard rendering styles, since it focuses attention on important features in the data. We test our algorithms on datasets up to 5123, demonstrating interactive extraction and rendering of line drawings in a variety of drawing styles.},
	number = {3},
	urldate = {2013-11-17},
	journal = {{ACM} Trans. Graph.},
	author = {Burns, Michael and Klawe, Janek and Rusinkiewicz, Szymon and Finkelstein, Adam and {DeCarlo}, Doug},
	month = jul,
	year = {2005},
	keywords = {isosurface, {NPR}, silhouettes, suggestive contours, visualization, Volume},
	pages = {512-518},
	file = {volines.pdf:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\DH75GP6X\volines.pdf:application/pdf}
}

@inproceedings{kirby_visualizing_1999,
	address = {Los Alamitos, {CA}, {USA}},
	series = {{VIS} '99},
	title = {Visualizing multivalued data from {2D} incompressible flows using concepts from painting},
	isbn = {0-7803-5897-X},
	url = {http://dl.acm.org/citation.cfm?id=319351.319429},
	abstract = {We present a new visualization method for 2d flows which allows us to combine multiple data values in an image for simultaneous viewing. We utilize concepts from oil painting, art, and design as introduced in [1] to examine problems within fluid mechanics. We use a combination of discrete and continuous visual elements arranged in multiple layers to visually represent the data. The representations are inspired by the brush strokes artists apply in layers to create an oil painting. We display commonly visualized quantities such as velocity and vorticity together with three additional mathematically derived quantities: the rate of strain tensor (defined in section 4), and the turbulent charge and turbulent current (defined in section 5). We describe the motivation for simultaneously examining these quantities and use the motivation to guide our choice of visual representation for each particular quantity. We present visualizations of three flow examples and observations concerning some of the physical relationships made apparent by the simultaneous display technique that we employed.},
	urldate = {2013-03-17},
	booktitle = {Proceedings of the conference on Visualization '99: celebrating ten years},
	publisher = {{IEEE} Computer Society Press},
	author = {Kirby, R. M. and Marmanis, H. and Laidlaw, David H.},
	year = {1999},
	pages = {333-340},
	file = {ACM Full Text PDF:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\X66VEUQJ\Kirby et al. - 1999 - Visualizing multivalued data from 2D incompressibl.pdf:application/pdf}
}

@article{kalnins_coherent_2003,
	title = {Coherent Stylized Silhouettes},
	volume = {22},
	issn = {0730-0301},
	url = {http://doi.acm.org/10.1145/882262.882355},
	doi = {10.1145/882262.882355},
	abstract = {We describe a way to render stylized silhouettes of animated {3D} models with temporal coherence. Coherence is one of the central challenges for non-photorealistic rendering. It is especially difficult for silhouettes, because they may not have obvious correspondences between frames. We demonstrate various coherence effects for stylized silhouettes with a robust working system. Our method runs in real-time for models of moderate complexity, making it suitable for both interactive applications and offline animation.},
	number = {3},
	urldate = {2013-11-17},
	journal = {{ACM} Trans. Graph.},
	author = {Kalnins, Robert D. and Davidson, Philip L. and Markosian, Lee and Finkelstein, Adam},
	month = jul,
	year = {2003},
	keywords = {animation, non-photorealistic rendering, silhouettes},
	pages = {856-861},
	file = {kalnins2003css.pdf:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\RC84VKVV\kalnins2003css.pdf:application/pdf}
}

@incollection{kruger_focus_2010,
	series = {{IFMBE} {Proceedings}},
	title = {Focus and {Context}-{Visualization} without the {Complexity}},
	copyright = {©2010 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-642-03894-5 978-3-642-03895-2},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-03895-2_14},
	abstract = {Attempting to display the entirety of a large volumetric dataset at one time would result in an overwhelming amount of information. Furthermore, visualization tools based on volume rendering present the user with a host of confusing options. We present ClearView, which provides a simplified volume visualization tool with a focus on doing what matters most: looking at your data. Users frequently want to direct the viewer’s attention to a particular region of their volumes. With many volume rendering tools, this means setting up complex transfer functions to highlight the region of interest, with the unfortunate side effect of potentially affecting the larger image. ClearView allows the user to focus their visualization efforts on the area of their choice, while separating parameters for visualizing of surrounding data. This provides not only a simplified user interface, but finergrained control over the final publication-quality visualization. Through advanced GPU rendering techniques, ClearView presents all of this to the user at highly interactive frame rates.},
	number = {25/13},
	urldate = {2013-11-17},
	booktitle = {World {Congress} on {Medical} {Physics} and {Biomedical} {Engineering}, {September} 7 - 12, 2009, {Munich}, {Germany}},
	publisher = {Springer Berlin Heidelberg},
	author = {Kr{\"u}ger, J. and Fogal, T.},
	editor = {D{\"o}ssel, Olaf and Schlegel, Wolfgang C.},
	month = jan,
	year = {2010},
	keywords = {Biomedical Engineering, Biophysics and Biological Physics, Context, focus, Visualization, Volume},
	pages = {45--48}
}

@inproceedings{xu_stylized_2004,
	address = {New York, {NY}, {USA}},
	series = {{NPAR} '04},
	title = {Stylized Rendering of {3D} Scanned Real World Environments},
	isbn = {1-58113-887-3},
	url = {http://doi.acm.org/10.1145/987657.987662},
	doi = {10.1145/987657.987662},
	abstract = {This paper presents an interactive non-photorealistic rendering ({NPR)} system that stylizes and renders outdoor scenes captured by {3D} laser scanning. In order to deal with the large size, complexity and inherent incompleteness of data obtained from outdoor scans, our system represents outdoor scenes using points instead of traditional polygons. Algorithms are then developed to extract, stylize and render features from this point representation. In addition to conveying various {NPR} styles, our system also promises consistency in animation by maintaining stroke coherence and density. We achieve {NPR} of large data at interactive rates by designing novel data structures and algorithms as well as leveraging new features of commodity graphics hardware.},
	urldate = {2013-11-18},
	booktitle = {Proceedings of the 3rd International Symposium on Non-photorealistic Animation and Rendering},
	publisher = {{ACM}},
	author = {Xu, Hui and Chen, Baoquan},
	year = {2004},
	keywords = {{3D} scanning, interactive {3D} graphics, multi-resolution, non-photorealistic rendering, point-based rendering},
	pages = {25-34},
	file = {2005_26.pdf:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\SUTSANWS\2005_26.pdf:application/pdf}
}

@inproceedings{boye_population_2013,
	title = {Population based modeling of respiratory lung motion and prediction from partial information},
	volume = {8669},
	url = {http://dx.doi.org/10.1117/12.2007076},
	doi = {10.1117/12.2007076},
	abstract = {Treatment of tumor sites affected by respiratory motion requires knowledge of the position and the shape of
the tumor and the surrounding organs during breathing. As not all structures of interest can be observed in
real-time, their position needs to be predicted from partial information (so-called surrogates) like motion of
diaphragm, internal markers or patients surface. Here, we present an approach to model respiratory lung motion
and predict the position and shape of the lungs from surrogates. 4D-MRI lung data of 10 healthy subjects was
acquired and used to create a model based on Principal Component Analysis (PCA). The mean RMS motion
ranged from 1.88 mm to 9.66 mm. Prediction was done using a Bayesian approach and an average RMSE of
1.44 mm was achieved.},
	urldate = {2013-11-08},
	booktitle = {{SPIE} {Medical} {Imaging}},
	author = {Boye, Dirk and Samei, Golnoosh and Schmidt, Johannes and Sz{\'e}kely, Gabor and Tanner, Christine},
	year = {2013},
	pages = {86690U--86690U--7}
}

@article{tsoumpas_fast_2011,
	title = {Fast generation of {4D} {PET-MR} data from real dynamic {MR} acquisitions},
	volume = {56},
	issn = {0031-9155},
	url = {http://iopscience.iop.org/0031-9155/56/20/005},
	doi = {10.1088/0031-9155/56/20/005},
	abstract = {We have implemented and evaluated a framework for simulating simultaneous dynamic {PET-MR} data using the anatomic and dynamic information from real {MR} acquisitions. {PET} radiotracer distribution is simulated by assigning typical {FDG} uptake values to segmented {MR} images with manually inserted additional virtual lesions. {PET} projection data and images are simulated using analytic forward projections (including attenuation and Poisson statistics) implemented within the image reconstruction package {STIR.} {PET} image reconstructions are also performed with {STIR.} The simulation is validated with numerical simulation based on Monte Carlo ({GATE)} which uses more accurate physical modelling, but has 150Ã— slower computation time compared to the analytic method for ten respiratory positions and is 7000Ã— slower when performing multiple realizations. Results are validated in terms of region of interest mean values and coefficients of variation for 65 million coincidences including scattered events. Although some discrepancy is observed, agreement between the two different simulation methods is good given the statistical noise in the data. In particular, the percentage difference of the mean values is 3.1\% for tissue, 17\% for the lungs and 18\% for a small lesion. The utility of the procedure is demonstrated by simulating realistic {PET-MR} datasets from multiple volunteers with different breathing patterns. The usefulness of the toolkit will be shown for performance investigations of the reconstruction, motion correction and attenuation correction algorithms for dynamic {PET-MR} data.},
	language = {en},
	number = {20},
	urldate = {2013-11-14},
	journal = {Physics in Medicine and Biology},
	author = {Tsoumpas, C. and Buerger, C. and King, A. P. and Mollet, P. and Keereman, V. and Vandenberghe, S. and Schulz, V. and Schleyer, P. and Schaeffter, T. and Marsden, P. K.},
	month = oct,
	year = {2011},
	pages = {6597},
	file = {Full Text PDF:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\GC8WSKW5\Tsoumpas et al. - 2011 - Fast generation of 4D PET-MR data from real dynami.pdf:application/pdf;Snapshot:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\NB2HMDHI\005.html:text/html}
}

@article{zhou_automatic_2009,
	title = {Automatic Transfer Function Generation Using Contour Tree Controlled Residue Flow Model and Color Harmonics},
	volume = {15},
	issn = {1077-2626},
	doi = {10.1109/TVCG.2009.120},
	abstract = {Transfer functions facilitate the volumetric data visualization by assigning optical properties to various data features and scalar values. Automation of transfer function specifications still remains a challenge in volume rendering. This paper presents an approach for automating transfer function generations by utilizing topological attributes derived from the contour tree of a volume. The contour tree acts as a visual index to volume segments, and captures associated topological attributes involved in volumetric data. A residue flow model based on Darcy's Law is employed to control distributions of opacity between branches of the contour tree. Topological attributes are also used to control color selection in a perceptual color space and create harmonic color transfer functions. The generated transfer functions can depict inclusion relationship between structures and maximize opacity and color differences between them. The proposed approach allows efficient automation of transfer function generations, and exploration on the data to be carried out based on controlling of opacity residue flow rate instead of complex low-level transfer function parameter adjustments. Experiments on various data sets demonstrate the practical use of our approach in transfer function generations.},
	number = {6},
	journal = {{IEEE} Transactions on Visualization and Computer Graphics},
	author = {Zhou, Jianlong and Takatsuka, Masahiro},
	year = {2009},
	keywords = {Algorithms, Australia, Automatic control, Automatic generation control, automatic transfer function generation, Automation, Color, color harmonics, color selection, colour graphics, Computer Graphics, contour tree, Darcy's law, data visualisation, data visualization, Foot, harmonic color., Head, Humans, Image motion analysis, Image Processing, Computer-Assisted, Knee, Models, Theoretical, opacity, Optical control, Optical harmonic generation, optical properties, perceptual color space, rendering (computer graphics), residue flow model, residueflow, Topology, transfer function, transfer functions, trees (mathematics), volume rendering, volumetric data visualization},
	pages = {1481--1488},
	file = {IEEE Xplore Abstract Record:C:\Users\JoeShengzhou\AppData\Roaming\Zotero\Zotero\Profiles\0m7eflhb.default\zotero\storage\GQ5J3G5I\abs_all.html:text/html;IEEE Xplore Full Text PDF:C:\Users\JoeShengzhou\AppData\Roaming\Zotero\Zotero\Profiles\0m7eflhb.default\zotero\storage\CC7GP4B7\Zhou and Takatsuka - 2009 - Automatic Transfer Function Generation Using Conto.pdf:application/pdf}
}

@article{ip_hierarchical_2012,
	title = {Hierarchical Exploration of Volumes Using Multilevel Segmentation of the Intensity-Gradient Histograms},
	volume = {18},
	issn = {1077-2626},
	doi = {10.1109/TVCG.2012.231},
	abstract = {Visual exploration of volumetric datasets to discover the embedded features and spatial structures is a challenging and tedious task. In this paper we present a semi-automatic approach to this problem that works by visually segmenting the intensity-gradient {2D} histogram of a volumetric dataset into an exploration hierarchy. Our approach mimics user exploration behavior by analyzing the histogram with the normalized-cut multilevel segmentation technique. Unlike previous work in this area, our technique segments the histogram into a reasonable set of intuitive components that are mutually exclusive and collectively exhaustive. We use information-theoretic measures of the volumetric data segments to guide the exploration. This provides a data-driven coarse-to-fine hierarchy for a user to interactively navigate the volume in a meaningful manner.},
	number = {12},
	journal = {{IEEE} Transactions on Visualization and Computer Graphics},
	author = {Ip, Cheuk Yiu and Varshney, A. and Jaja, J.},
	year = {2012},
	keywords = {data visualisation, data-driven coarse-to-fine hierarchy, entropy, exploration hierarchy, gradient methods, hierarchical volume exploration, histogram segmentation, Histograms, Image segmentation, Information Theory, Information-guided exploration, information-theoretic measures, intensity-gradient {2D} histogram, intensity-gradient histogram, normalized cut, normalized-cut multilevel segmentation, shape analysis, transfer functions, user exploration behavior, visual exploration, visual segmentation, visualization, Volume classification, volume exploration, Volume measurement, volumetric data segment, volumetric dataset},
	pages = {2355--2363},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\t929e2iy.default\\zotero\\storage\\J4VWTHUG\\articleDetails.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\t929e2iy.default\\zotero\\storage\\V6CI4GF8\\Ip et al. - 2012 - Hierarchical Exploration of Volumes Using Multilev.pdf:application/pdf}
}

@article{rodriguez_state---art_2014,
	title = {State-of-the-art in Compressed {GPU-Based} Direct Volume Rendering},
	volume = {33},
	url = {http://vic.crs4.it/vic/cgi-bin/bib-page.cgi?id='Balsa:2014:SCG'},
	abstract = {Great advancements in commodity graphics hardware have favored {GPU-based} volume rendering as the main adopted solution for interactive exploration of rectilinear scalar volumes on commodity platforms. Nevertheless, long data transfer times and {GPU} memory size limitations are often the main limiting factors, especially for massive, time-varying or multi-volume visualization, as well as for networked visualization on the emerging mobile devices. To address this issue, a variety of level-of-detail data representations and compression techniques have been introduced. In order to improve capabilities and performance over the entire storage, distribution and rendering pipeline, the encoding/decoding process is typically highly asymmetric, and systems should ideally compress at data production time and decompress on demand at rendering time. Compression and level-of-detail pre-computation does not have to adhere to real-time constraints and can be performed off-line for high quality results. In contrast, adaptive real-time rendering from compressed representations requires fast, transient, and spatially independent decompression. In this report, we review the existing compressed {GPU} volume rendering approaches, covering sampling grid layouts, compact representation models, compression techniques, {GPU} ren- dering architectures and fast decoding techniques.},
	journal = {Computer Graphics Forum},
	author = {Rodriguez, Marcos Balsa and Gobbetti, Enrico and Guiti{\'a}n, Jos{\'e} Antonio Iglesias and Makhinya, Maxim and Marton, Fabio and Pajarola, Renato and Suter, Susanne},
	year = {2014},
	note = {To appear},
	keywords = {compression models, compression-domain direct volume rendering, Computer Graphics [I.3.3]: {Picture/Image} Generation Computer Graphics [I.3.7]: Three-dimensional graphics and realism Coding and Information Theory [E.4]: Data compaction and compression Compression (Coding) [I.4.2]: Approximate methods, decoding strategies, decompression architectures, {GPU}, large volume data visualization, level-of-detail representations, preprocessing and encoding, sampling grid layouts, time-varying volume data visualization},
	file = {cgf2014-star_compressed_gpu_dvr.pdf:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\t929e2iy.default\\zotero\\storage\\SJEZC43X\\cgf2014-star_compressed_gpu_dvr.pdf:application/pdf;Snapshot:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\t929e2iy.default\\zotero\\storage\\V5KKW7IT\\abstract.html:text/html}
}

@article{mindek_visual_2013,
	title = {Visual {Parameter} {Exploration} in {GPU} {Shader} {Space}},
	volume = {21},
	issn = {1213-6972},
	url = {http://www.cg.tuwien.ac.at/research/publications/2013/mindek-2013-pel/},
	abstract = {The wide availability of high-performance GPUs has made the use of shader programs in visualization ubiquitous. Understanding shaders is a challenging task. Frequently it is dif?cult to mentally reconstruct the nature and types of transformations applied to the underlying data during the visualization process. We propose a method for the visual analysis of GPU shaders, which allows the ?exible exploration and investigation of algorithms, parameters, and their effects. We introduce a method for extracting feature vectors composed of several attributes of the shader, as well as a direct manipulation interface for assigning semantics to them. The user interactively classi?es pixels of images which are rendered with the investigated shader. The two resulting classes, a positive class and a negative one, are employed to steer the visualization. Based on this information, we can extract a wide variety of additional attributes and visualize their relation to this classi?cation. Our system allows an interactive exploration of shader space and we demonstrate its utility for several different applications.},
	number = {3},
	journal = {Journal of WSCG},
	author = {Mindek, Peter and Bruckner, Stefan and Rautek, Peter and Gr{\"o}ller, Meister Eduard},
	year = {2013},
	keywords = {parameter space exploration, shader augmentation},
	pages = {225--234}
}

@article{maciejewski_abstracting_2013,
	title = {Abstracting Attribute Space for Transfer Function Exploration and Design},
	volume = {19},
	issn = {1077-2626},
	doi = {10.1109/TVCG.2012.105},
	abstract = {Currently, user centered transfer function design begins with the user interacting with a one or two-dimensional histogram of the volumetric attribute space. The attribute space is visualized as a function of the number of voxels, allowing the user to explore the data in terms of the attribute size/magnitude. However, such visualizations provide the user with no information on the relationship between various attribute spaces (e.g., density, temperature, pressure, x, y, z) within the multivariate data. In this work, we propose a modification to the attribute space visualization in which the user is no longer presented with the magnitude of the attribute; instead, the user is presented with an information metric detailing the relationship between attributes of the multivariate volumetric data. In this way, the user can guide their exploration based on the relationship between the attribute magnitude and user selected attribute information as opposed to being constrained by only visualizing the magnitude of the attribute. We refer to this modification to the traditional histogram widget as an abstract attribute space representation. Our system utilizes common one and two-dimensional histogram widgets where the bins of the abstract attribute space now correspond to an attribute relationship in terms of the mean, standard deviation, entropy, or skewness. In this manner, we exploit the relationships and correlations present in the underlying data with respect to the dimension(s) under examination. These relationships are often times key to insight and allow us to guide attribute discovery as opposed to automatic extraction schemes which try to calculate and extract distinct attributes a priori. In this way, our system aids in the knowledge discovery of the interaction of properties within volumetric data.},
	number = {1},
	journal = {{IEEE} Transactions on Visualization and Computer Graphics},
	author = {Maciejewski, R. and Jang, Yun and Woo, Insoo and J{\"a}nicke, H. and Gaither, K.P. and Ebert, D.S.},
	month = jan,
	year = {2013},
	keywords = {abstract attribute space representation, attribute discovery, attribute information, attribute magnitude, attribute space visualization, automatic extraction schemes, data mining, data visualisation, Data visualization, entropy, histogram widget, Histograms, image color analysis, information metric, Information Theory, knowledge discovery, mean, Measurement, multivariate volumetric data, one-dimensional histogram, rendering (computer graphics), skewness, standard deviation, Transfer function design, transfer function exploration, transfer functions, two-dimensional histogram, user centered transfer function design, user centred design, volume rendering, volumetric attribute space abstraction},
	pages = {94--107},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\t929e2iy.default\\zotero\\storage\\MUU3MT2E\\articleDetails.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\t929e2iy.default\\zotero\\storage\\JK69SZNU\\Maciejewski et al. - 2013 - Abstracting Attribute Space for Transfer Function .pdf:application/pdf}
}

@inproceedings{csebfalvi_illumination-driven_2012,
	address = {Magdeburg, Germany},
	title = {Illumination-{Driven} {Opacity} {Modulation} for {Expressive} {Volume} {Rendering}},
	url = {http://www.cg.tuwien.ac.at/research/publications/2012/Csebfalvi-2012-IOM/},
	abstract = {Using classical volume visualization, typically a couple of isosurface layers are rendered semi-transparently to show the internal structures contained in the data. However, the opacity transfer function is often difficult to specify such that all the isosurfaces are of high contrast and sufficiently perceivable. In this paper, we propose a volumerendering technique which ensures that the different layers contribute to fairly different regions of the image space. Since the overlapping between the effected regions is reduced, an outer translucent isosurface does not decrease significantly the contrast of a partially hidden inner isosurface. Therefore, the layers of the data become visually well separated. Traditional transfer functions assign color and opacity values to the voxels depending on the density and the gradient. In contrast, we assign also different illumination directions to different materials, and modulate the opacities view-dependently based on the surface normals and the directions of the light sources, which are fixed to the viewing angle. We will demonstrate that this model allows an expressive visualization of volumetric data.},
	booktitle = {Proceedings of {Vision}, {Modeling} \& {Visualization} 2012},
	author = {Csebfalvi, Bal{\'a}zs and T{\'o}th, Bal{\'a}zs and Bruckner, Stefan and Gr{\"o}ller, Meister Eduard},
	month = nov,
	year = {2012},
	keywords = {illumination, illustrative visualization, volume rendering},
	pages = {103--109}
}

@article{marchesin_per-pixel_2010,
	title = {Per-{Pixel} {Opacity} {Modulation} for {Feature} {Enhancement} in {Volume} {Rendering}},
	volume = {16},
	issn = {1077-2626},
	url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5406522},
	doi = {10.1109/TVCG.2010.30},
	number = {4},
	urldate = {2012-07-17},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Marchesin, St{\'e}phane and Dischler, Jean-Michel and Mongenet, Catherine},
	month = jul,
	year = {2010},
	keywords = {adaptive rendering, Algorithms, alpha blending, Computer graphics, Computer Simulation, feature enhancement, feature extraction, Feature extraction, image enhancement, Image Interpretation, Computer-Assisted, Imaging, Three-Dimensional, Models, Theoretical, nonphotorealistic rendering., opacity, opacity transfer function, per-pixel opacity modulation, relevance function, rendering (computer graphics), Rendering (computer graphics), User-Computer Interface, Volume rendering, volume rendering equation},
	pages = {560--570}
}

@article{bruckner_illustrative_2006,
	title = {Illustrative {Context}-{Preserving} {Exploration} of {Volume} {Data}},
	volume = {12},
	issn = {1077-2626},
	url = {http://dx.doi.org/10.1109/TVCG.2006.96},
	doi = {10.1109/TVCG.2006.96},
	abstract = {In volume rendering, it is very difficult to simultaneously visualize interior and exterior structures while preserving clear shape cues. Highly transparent transfer functions produce cluttered images with many overlapping structures, while clipping techniques completely remove possibly important context information. In this paper, we present a new model for volume rendering, inspired by techniques from illustration. It provides a means of interactively inspecting the interior of a volumetric data set in a feature-driven way which retains context information. The context-preserving volume rendering model uses a function of shading intensity, gradient magnitude, distance to the eye point, and previously accumulated opacity to selectively reduce the opacity in less important data regions. It is controlled by two user-specified parameters. This new method represents an alternative to conventional clipping techniques, sharing their easy and intuitive user control, but does not suffer from the drawback of missing context information.},
	number = {6},
	urldate = {2013-05-09},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Bruckner, Stefan and Grimm, Soren and Kanitsar, Armin and Groller, M. Eduard},
	month = nov,
	year = {2006},
	keywords = {Algorithms, Attenuation, blood vessels, clipping techniques, cluttered images, Computer graphics, Context modeling, context-preserving volume rendering model, Databases, Factual, data visualisation, data visualization, Displays, Focus+Context Techniques, focus+context techniques, Focusing, gradient magnitude, illustrative context-preserving exploration, illustrative visualization, Illustrative visualization, image enhancement, Image Interpretation, Computer-Assisted, image texture, Imaging, Three-Dimensional, Information Storage and Retrieval, Pattern Recognition, Automated, rendering (computer graphics), shading intensity, Shape, Transfer functions, transparent transfer functions, User-Computer Interface, volume rendering., volume rendering, volumetric data set},
	pages = {1559--1569}
}

@inproceedings{kindlmann_curvature-based_2003,
	address = {Washington, {DC}, {USA}},
	series = {{VIS} '03},
	title = {Curvature-Based Transfer Functions for Direct Volume Rendering: Methods and Applications},
	isbn = {0-7695-2030-8},
	shorttitle = {Curvature-Based Transfer Functions for Direct Volume Rendering},
	url = {http://dx.doi.org/10.1109/VISUAL.2003.1250414},
	doi = {10.1109/VISUAL.2003.1250414},
	abstract = {Direct volume rendering of scalar fields uses a transfer function to map locally measured data properties to opacities and colors. The domain of the transfer function is typically the one-dimensional space of scalar data values. This paper advances the use of curvature information in multi-dimensional transfer functions, with a methodology for computing high-quality curvature measurements. The proposed methodology combines an implicit formulation of curvature with convolution-based reconstruction of the field. We give concrete guidelines for implementing the methodology, and illustrate the importance of choosing accurate filters for computing derivatives with convolution. Curvature-based transfer functions are shown to extend the expressivity and utility of volume rendering through contributions in three different application areas: non-photorealistic volume rendering, surface smoothing via anisotropic diffusion, and visualization of isosurface uncertainty.},
	urldate = {2013-03-12},
	booktitle = {Proceedings of the 14th {IEEE} Visualization 2003 ({VIS'03)}},
	publisher = {{IEEE} Computer Society},
	author = {Kindlmann, Gordon and Whitaker, Ross and Tasdizen, Tolga and M{\"o}ller, Torsten},
	year = {2003},
	keywords = {convolution-based differentiation, flowline curvature, implicit surface curvature, non-photorealistic rendering, surface processing, uncertainty visualization, volume rendering},
	pages = {513--520},
	file = {ACM Full Text PDF:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\vuyr0ioc.default\zotero\storage\M6ZDWZ3Z\Kindlmann et al. - 2003 - Curvature-Based Transfer Functions for Direct Volu.pdf:application/pdf}
}

@inproceedings{borland_volumetric_2006,
	title = {Volumetric depth peeling for medical image display},
	volume = {6060},
	url = {http://dx.doi.org/10.1117/12.641497},
	doi = {10.1117/12.641497},
	abstract = {Volumetric depth peeling  ({VDP)} is an extension to volume rendering that enables display of otherwise occluded features in volume data sets.  {VDP} decouples occlusion calculation from the volume rendering transfer function, enabling independent optimization of settings for rendering and occlusion.  The algorithm is flexible enough to handle multiple regions occluding the object of interest, as well as object self-occlusion, and requires no pre-segmentation of the data set.  {VDP} was developed as an improvement for virtual arthroscopy for the diagnosis of shoulder-joint trauma, and has been generalized for use in other simple and complex joints, and to enable non-invasive urology studies.  In virtual arthroscopy, the surfaces in the joints often occlude each other, allowing limited viewpoints from which to evaluate these surfaces.  In urology studies, the physician would like to position the virtual camera outside the kidney collecting system and see inside it.  By rendering invisible all voxels between the observer's point of view and objects of interest, {VDP} enables viewing from unconstrained positions.  In essence, {VDP} can be viewed as a technique for automatically defining an optimal data- and task-dependent clipping surface.  Radiologists using {VDP} display have been able to perform evaluations of pathologies more easily and more rapidly than with clinical arthroscopy, standard volume rendering, or standard {MRI/CT} slice viewing.},
	urldate = {2014-06-17},
	booktitle = {Proceedings of {SPIE}},
	author = {Borland, David and Clarke, John P. and Fielding, Julia R. and TaylorII, Russell M.},
	year = {2006},
	pages = {606004--606004-11},
	file = {VDPforMed_VDA2006.pdf:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\0m7eflhb.default\\zotero\\storage\\MTM39I6E\\VDPforMed_VDA2006.pdf:application/pdf}
}

@INPROCEEDINGS{ma_high_2000, 
author={Kwan-Liu Ma and Camp, D.M.}, 
booktitle={Supercomputing, ACM/IEEE 2000 Conference}, 
title={High Performance Visualization of Time-Varying Volume Data over a Wide-Area Network}, 
year={2000}, 
month={Nov}, 
pages={29-29}, 
keywords={Image Compression;Keywords: High Performance Computing;Parallel Volume Rendering;Pipelining;Remote Visualization;Sci-entific;Time-Varying Data;Visualization;Wide-Area Network;Computer displays;Computer networks;Concurrent computing;Costs;Data visualization;High-speed networks;Image coding;Pipeline processing;Rendering (computer graphics);Testing;Image Compression;Keywords: High Performance Computing;Parallel Volume Rendering;Pipelining;Remote Visualization;Sci-entific;Time-Varying Data;Visualization;Wide-Area Network}, 
doi={10.1109/SC.2000.10000}, 
ISSN={1063-9535},}

@techreport{konig_mastering_2000,
	opt_address = {Favoritenstrasse 9-11/186, A-1040 Vienna, Austria},
	type = {Technical Report},
	title = {Mastering Transfer Function Specification by using {VolumePro} Technology},
	url = {http://www.cg.tuwien.ac.at/research/publications/2000/Koenig-2000-ATFS/},
	abstract = {A new user-interface paradigm for the specification of transfer functions is presented. The specification is usually a difficult task as mapping information for a number of different domains (data range, color, opacity, etc.) has to be defined. In the presented approach, the definition of the mapping information can be realized independently for each property domain. A set of specification tools is provided for each domain, enabling users with different levels of experience or demanding time restrictions to choose an appropriate approach for their needs. Real-time feedback during the manipulation of parameters has been proven to be crucial to the specification. An interactive direct-volume-rendering display is realized by utilizing dedicated hardware acceleration.},
	number = {{TR-186-2-00-07}},
	institution = {Institute of Computer Graphics and Algorithms, Vienna University of Technology},
	author = {K{\"o}nig, Andreas and Gr{\"o}ller, Meister Eduard},
	month = mar,
	year = {2000},
	opt_note = {human contact: technical-report@cg.tuwien.ac.at},
	keywords = {Transfer Function Specification, volume visualization, {VolumePro} ray-casting system},
	file = {TR-186-2-00-07Paper.pdf:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\0m7eflhb.default\\zotero\\storage\\4ZU5DVVV\\TR-186-2-00-07Paper.pdf:application/pdf}
}

@book{feeman_mathematics_2009,
	title = {The Mathematics of Medical Imaging: A Beginner's Guide},
	isbn = {9780387927114},
	shorttitle = {The Mathematics of Medical Imaging},
	abstract = {In 1979, the Nobel Prize for Medicine and Physiology was awarded jointly to Allan {McLeod} Cormack and Godfrey Newbold Houns eld, the two pioneering scienti- engineers primarily responsible for the development, in the 1960s and early 1970s, of computerized axial tomography, popularly known as the {CAT} or {CT} scan. In his papers [13], Cormack, then a Professor at Tufts University, in Massachusetts, dev- oped certain mathematical algorithms that, he envisioned, could be used to create an image from X-ray data. Working completely independently of Cormack and at about the same time, Houns eld, a research scientist at {EMI} Central Research Laboratories in the United Kingdom, designed the rst operational {CT} scanner as well as the rst commercially available model. (See [22] and [23]. ) Since 1980, the number of {CT} scans performed each year in the United States has risen from about 3 million to over 67 million. What few people who have had {CT} scans probably realize is that the fundamental problem behind this procedure is essentially mathematical: If we know the values of the integral of a two- or three-dimensional fu- tion along all possible cross-sections, then how can we reconstruct the function itself? This particular example of what is known as an inverse problem was studied by Johann Radon, an Austrian mathematician, in the early part of the twentieth century.},
	language = {en},
	publisher = {Springer},
	author = {Feeman, Timothy G.},
	month = dec,
	year = {2009},
	keywords = {Computers / Computer Vision \& Pattern Recognition, Computers / Data Processing, Mathematics / Calculus, Mathematics / Discrete Mathematics, Mathematics / Functional Analysis, Mathematics / Mathematical Analysis, Medical / Allied Health Services / Radiological \& Ultrasound Technology, Medical / Biochemistry, Medical / Radiology \& Nuclear Medicine, Technology \& Engineering / Biomedical, Technology \& Engineering / Engineering (General)},
	file = {Feeman T.G. The mathematics of medical imaging.. A beginners guide (Springer, 2010)(ISBN 0387927115)(O)(150s)_CsIp_.pdf:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\0m7eflhb.default\\zotero\\storage\\7ZG6NRIH\\Feeman T.G. The mathematics of medical imaging.. A beginners guide (Springer, 2010)(ISBN 0387927115)(O)(150s)_CsIp_.pdf:application/pdf}
}

@inproceedings{fang_visualization_2007,
	address = {New York, {NY}, {USA}},
	series = {{GI} '07},
	title = {Visualization and exploration of time-varying medical image data sets},
	isbn = {978-1-56881-337-0},
	url = {http://doi.acm.org/10.1145/1268517.1268563},
	doi = {10.1145/1268517.1268563},
	abstract = {In this work, we propose and compare several methods for the visualization and exploration of time-varying volumetric medical images based on the temporal characteristics of the data. The principle idea is to consider a time-varying data set as a {3D} array where each voxel contains a time-activity curve ({TAC).} We define and appraise three different {TAC} similarity measures. Based on these measures we introduce three methods to analyze and visualize time-varying data. The first method relates the whole data set to one template {TAC} and creates a {1D} histogram. The second method extends the {1D} histogram into a {2D} histogram by taking the Euclidean distance between voxels into account. The third method does not rely on a template {TAC} but rather creates a {2D} scatter-plot of all {TAC} data points via multi-dimensional scaling. These methods allow the user to specify transfer functions on the {1D} and {2D} histograms and on the scatter plot, respectively. We validate these methods on synthetic dynamic {SPECT} and {PET} data sets and a dynamic planar Gamma camera image of a patient. These techniques are designed to offer researchers and health care professionals a new tool to study the time-varying medical imaging data sets.},
	urldate = {2012-11-05},
	booktitle = {Proceedings of Graphics Interface 2007},
	publisher = {{ACM}},
	author = {Fang, Zhe and M{\"o}ller, Torsten and Hamarneh, Ghassan and Celler, Anna},
	year = {2007},
	keywords = {medical imaging, multi-dimensional scaling, time-varying data, transfer function, volume rendering},
	pages = {281--288},
	file = {ACM Full Text PDF:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\t929e2iy.default\\zotero\\storage\\SP3RP6UG\\Fang et al. - 2007 - Visualization and exploration of time-varying medi.pdf:application/pdf}
}

@article{janicke_multifield_2007,
	title = {Multifield Visualization Using Local Statistical Complexity},
	volume = {13},
	issn = {1077-2626},
	doi = {10.1109/TVCG.2007.70615},
	abstract = {Modern unsteady (multi-)field visualizations require an effective reduction of the data to be displayed. From a huge amount of information the most informative parts have to be extracted. Instead of the fuzzy application dependent notion of feature, a new approach based on information theoretic concepts is introduced in this paper to detect important regions. This is accomplished by extending the concept of local statistical complexity from finite state cellular automata to discretized (multi-)fields. Thus, informative parts of the data can be highlighted in an application-independent, purely mathematical sense. The new measure can be applied to unsteady multifields on regular grids in any application domain. The ability to detect and visualize important parts is demonstrated using diffusion, flow, and weather simulations.},
	number = {6},
	journal = {{IEEE} Transactions on Visualization and Computer Graphics},
	author = {J{\"a}nicke, Heike and Wiebel, Alexander and Scheuermann, Gerik and Kollmann, Wolfgang},
	year = {2007},
	keywords = {coherent structures, feature detection, flow visualization., information theory, local statistical complexity, multifield visualization, time-dependent},
	pages = {1384--1391},
	file = {IEEE Computer Snapshot:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\t929e2iy.default\\zotero\\storage\\XQFMXA83\\JÃ¤nicke et al. - 2007 - Multifield Visualization Using Local Statistical C.html:text/html;jaenicke_vis07.pdf:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\t929e2iy.default\\zotero\\storage\\RVZ3BS2R\\jaenicke_vis07.pdf:application/pdf}
}

@article{xu_information-theoretic_2010,
	title = {An {Information}-{Theoretic} {Framework} for {Flow} {Visualization}},
	volume = {16},
	issn = {1077-2626},
	doi = {10.1109/TVCG.2010.131},
	abstract = {The process of visualization can be seen as a visual communication channel where the input to the channel is the raw data, and the output is the result of a visualization algorithm. From this point of view, we can evaluate the effectiveness of visualization by measuring how much information in the original data is being communicated through the visual communication channel. In this paper, we present an information-theoretic framework for flow visualization with a special focus on streamline generation. In our framework, a vector field is modeled as a distribution of directions from which Shannon's entropy is used to measure the information content in the field. The effectiveness of the streamlines displayed in visualization can be measured by first constructing a new distribution of vectors derived from the existing streamlines, and then comparing this distribution with that of the original data set using the conditional entropy. The conditional entropy between these two distributions indicates how much information in the original data remains hidden after the selected streamlines are displayed. The quality of the visualization can be improved by progressively introducing new streamlines until the conditional entropy converges to a small value. We describe the key components of our framework with detailed analysis, and show that the framework can effectively visualize 2D and 3D flow data.},
	number = {6},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Xu, Lijie and Lee, Teng-Yok and Shen, Han-Wei},
	month = nov,
	year = {2010},
	keywords = {2D flow data visualization, 3D flow data.visualization, computational fluid dynamics, conditional entropy, data visualisation, data visualization, entropy, Flow field visualization, flow visualisation, Histograms, information-theoretic framework, information theory, Shannon's entropy, Streaming media, streamline generation., streamline generation, Three dimensional displays, vector field, visual communication channel, Visualization},
	pages = {1216--1224},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\PHK4IWQP\\abs_all.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\RBW2ZNPH\\Xu et al. - 2010 - An Information-Theoretic Framework for Flow Visual.pdf:application/pdf}
}

@article{bramon_information-theoretic_2013,
	title = {An {Information}-{Theoretic} {Observation} {Channel} for {Volume} {Visualization}},
	volume = {32},
	copyright = {© 2013 The Author(s) Computer Graphics Forum © 2013 The Eurographics Association and Blackwell Publishing Ltd.},
	issn = {1467-8659},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/cgf.12128/abstract},
	doi = {10.1111/cgf.12128},
	abstract = {Different quality metrics have been proposed in the literature to evaluate how well a visualization represents the underlying data. In this paper, we present a new information-theoretic framework that quantifies the information transfer between the source data set and the rendered image. This approach is based on the definition of an observation channel whose input and output are given by the intensity values of the volumetric data set and the pixel colors, respectively. From this channel, the mutual information, a measure of information transfer or correlation between the input and the output, is used as a metric to evaluate the visualization quality. The usefulness of the proposed observation channel is illustrated with three fundamental visualization applications: selection of informative viewpoints, transfer function design, and light positioning.},
	language = {en},
	number = {3pt4},
	urldate = {2013-08-06},
	journal = {Computer Graphics Forum},
	author = {Bramon, R. and Ruiz, M. and Bardera, A. and Boada, I. and Feixas, M. and Sbert, M.},
	year = {2013},
	keywords = {Algorithms, [Computer, Generation—Viewing, Graphics]:, I.3.3, I.3.3 [Computer Graphics]: Picture/Image Generation—Viewing algorithms, Picture/Image},
	pages = {411--420}
}

@article{chan_perception-based_2009,
	title = {Perception-Based Transparency Optimization for Direct Volume Rendering},
	volume = {15},
	issn = {1077-2626},
	doi = {10.1109/TVCG.2009.172},
	abstract = {The semi-transparent nature of direct volume rendered images is useful to depict layered structures in a volume. However, obtaining a semi-transparent result with the layers clearly revealed is difficult and may involve tedious adjustment on opacity and other rendering parameters. Furthermore, the visual quality of layers also depends on various perceptual factors. In this paper, we propose an auto-correction method for enhancing the perceived quality of the semi-transparent layers in direct volume rendered images. We introduce a suite of new measures based on psychological principles to evaluate the perceptual quality of transparent structures in the rendered images. By optimizing rendering parameters within an adaptive and intuitive user interaction process, the quality of the images is enhanced such that specific user requirements can be met. Experimental results on various datasets demonstrate the effectiveness and robustness of our method.},
	number = {6},
	journal = {{IEEE} Transactions on Visualization and Computer Graphics},
	author = {Chan, Ming-Yuen and Wu, Yingcai and Mak, Wai-Ho and Chen, Wei and Qu, Huamin},
	month = dec,
	year = {2009},
	keywords = {auto-correction method, Computer Graphics, Diagnostic Imaging, direct volume rendered images, Direct volume rendering, image enhancement, Image enhancement, Image Processing, Computer-Assisted, Image quality, Image resolution, intuitive user interaction process, layer perception., perception-based transparency optimization, psychological principles, Psychology, rendering (computer graphics), Rendering (computer graphics), Robustness, Shape measurement, Transfer functions, user interfaces, User-Computer Interface, Visual perception, visual quality, Visualization},
	pages = {1283 --1290},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\0m7eflhb.default\\zotero\\storage\\P39H2TC9\\articleDetails.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\0m7eflhb.default\\zotero\\storage\\MBKFMXJ3\\Chan et al. - 2009 - Perception-Based Transparency Optimization for Dir.pdf:application/pdf}
}

@inproceedings{rezk-salama_automatic_2000,
	title = {Automatic Adjustment of Transfer Functions for 3D Volume Visualization},
	abstract = {In most volume rendering scenarios implicit classification is performed manually by specification data values to visual attributes. An appropriate classification requires both specialized knowledge of the interesting structures within the data set as well as the technical knowhow of the computer scientist. Recent automatic data-driven techniques are verywell capable of separating different regions in the data set. However, their applicability in practice is limited, since they do not contain any information about the critical structures which are of interest. In this scenario we propose an efficient and reproducible way to automatically assign transfer function templates, which include individual knowledge as well as personal taste. The presented approach is based on dynamic programming and was successfully applied in medical environment. 1},
	booktitle = {In Proc. Workshop Vision, Modeling, and Visualization ({VMV}},
	publisher = {a},
	author = {Rezk-Salama, Christof and Hastreiter, Peter and Scherer, J{\"o}rg and Greiner, G{\"u}nther},
	year = {2000},
	pages = {357--364}
}

@article{kindlmann_transfer_2002,
	title = {Transfer Functions in Direct Volume Rendering: Design Interface Interaction},
	shorttitle = {Transfer Functions in Direct Volume Rendering},
	journal = {{SIGGRAPH} Course Notes},
	author = {Kindlmann, Gordon},
	year = {2002},
	keywords = {Direct volume rendering, transfer function},
	file = {PDF from www.cs.utah.edu:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\t929e2iy.default\\zotero\\storage\\FHBM7FGV\\Kindlmann - Transfer Functions in Direct Volume Rendering Des.pdf:application/pdf}
}

@inproceedings{akiba_simultaneous_2006,
	address = {Aire-la-Ville, Switzerland, Switzerland},
	series = {{EUROVIS}'06},
	title = {Simultaneous classification of time-varying volume data based on the time histogram},
	isbn = {3-905673-31-2},
	url = {http://dx.doi.org/10.2312/VisSym/EuroVis06/171-178},
	doi = {10.2312/VisSym/EuroVis06/171-178},
	abstract = {An important challenge in the application of direct volume rendering to time-varying data is the specification of transfer functions for all time steps. Very little research has been devoted to this problem, however. To address this issue we propose an approach which allows simultaneous classification of the entire time series. We explore options for transfer function specification that are based, either directly or indirectly, on the time histogram. Furthermore, we consider how to effectively provide feedback for interactive classification by exploring options for simultaneous rendering of the time series, again based on the time histogram. Finally, we apply this approach to several large time-varying data sets where we show that the important features at all times are captured with about the same effort it takes to classify one time step using conventional classification.},
	urldate = {2012-11-05},
	booktitle = {Proceedings of the {Eighth} {Joint} {Eurographics} / {IEEE} {VGTC} conference on {Visualization}},
	publisher = {Eurographics Association},
	author = {Akiba, Hiroshi and Fout, Nathaniel and Ma, Kwan-Liu},
	year = {2006},
	pages = {171--178},
	file = {submitted_akiba.pdf:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\0m7eflhb.default\\zotero\\storage\\NTJFEVTX\\submitted_akiba.pdf:application/pdf}
}

@article{woodring_multi-variate_2006,
	title = {Multi-variate, {Time} {Varying}, and {Comparative} {Visualization} with {Contextual} {Cues}},
	volume = {12},
	issn = {1077-2626},
	doi = {10.1109/TVCG.2006.164},
	abstract = {Time-varying, multi-variate, and comparative data sets are not easily visualized due to the amount of data that is presented to the user at once. By combining several volumes together with different operators into one visualized volume, the user is able to compare values from different data sets in space over time, run, or field without having to mentally switch between different renderings of individual data sets. In this paper, we propose using a volume shader where the user is given the ability to easily select and operate on many data volumes to create comparison relationships. The user specifies an expression with set and numerical operations and her data to see relationships between data fields. Furthermore, we render the contextual information of the volume shader by converting it to a volume tree. We visualize the different levels and nodes of the volume tree so that the user can see the results of suboperations. This gives the user a deeper understanding of the final visualization, by seeing how the parts of the whole are operationally constructed},
	number = {5},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Woodring, J. and Shen, Han-Wei},
	year = {2006},
	keywords = {Animation, comparative, Computer science, contextual information, data visualisation, Data visualization, Filling, focus + context, Humans, multi-variate, multivariate time-varying comparative visualization, numerical operation, rendering (computer graphics), Rendering (computer graphics), rendering technique, Switches, time-varying, Transfer functions, transfer functions, volume shader, volume tree},
	pages = {909--916},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\0m7eflhb.default\\zotero\\storage\\5TRRDTBF\\abs_all.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\0m7eflhb.default\\zotero\\storage\\XNK9Q3AE\\Woodring and Shen - 2006 - Multi-variate, Time Varying, and Comparative Visua.pdf:application/pdf}
}

@article{guo_scalable_2012,
	title = {Scalable {Multivariate} {Volume} {Visualization} and {Analysis} {Based} on {Dimension} {Projection} and {Parallel} {Coordinates}},
	volume = {18},
	issn = {1077-2626},
	url = {http://dx.doi.org/10.1109/TVCG.2012.80},
	doi = {10.1109/TVCG.2012.80},
	abstract = {In this paper, we present an effective and scalable system for multivariate volume data visualization and analysis with a novel transfer function interface design that tightly couples parallel coordinates plots (PCP) and MDS-based dimension projection plots. In our system, the PCP visualizes the data distribution of each variate (dimension) and the MDS plots project features. They are integrated seamlessly to provide flexible feature classification without context switching between different data presentations during the user interaction. The proposed interface enables users to identify relevant correlation clusters and assign optical properties with lassos, magic wand, and other tools. Furthermore, direct sketching on the volume rendered images has been implemented to probe and edit features. With our system, users can interactively analyze multivariate volumetric data sets by navigating and exploring feature spaces in unified PCP and MDS plots. To further support large-scale multivariate volume data visualization and analysis, Scalable Pivot MDS (SPMDS), parallel adaptive continuous PCP rendering, as well as parallel rendering techniques are developed and integrated into our visualization system. Our experiments show that the system is effective in multivariate volume data visualization and its performance is highly scalable for data sets with different sizes and number of variates.},
	number = {9},
	urldate = {2013-05-01},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Guo, Hanqi and Xiao, He and Yuan, Xiaoru},
	month = sep,
	year = {2012},
	keywords = {Algorithm design and analysis, context switching, Correlation, data presentations, data visualisation, Data visualization, dimension projection, direct sketching, flexible feature classification, lassos, magic wand, MDS-based dimension projection plots, MDS plots, multivariate volume, multivariate volume data visualization, multivariate volumetric data sets, optical properties, parallel adaptive continuous PCP rendering, Parallel coordinates, parallel coordinates plots, parallel visualization., pattern classification, PCP plots, rendering (computer graphics), Rendering (computer graphics), scalable multivariate volume visualization, scalable pivot MDS, Transfer Function, transfer function interface design, Transfer functions, transfer functions, user interaction, user-interface design, Vegetation, volume rendered images},
	pages = {1397--1410},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\0m7eflhb.default\\zotero\\storage\\GA6ZPI8E\\articleDetails.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\0m7eflhb.default\\zotero\\storage\\8NPAVS9B\\articleDetails.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\0m7eflhb.default\\zotero\\storage\\CDSS99AI\\Guo et al. - 2012 - Scalable Multivariate Volume Visualization and Ana.pdf:application/pdf;IEEE Xplore Full Text PDF:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\0m7eflhb.default\\zotero\\storage\\QITCES5E\\Guo et al. - 2012 - Scalable Multivariate Volume Visualization and Ana.pdf:application/pdf}
}

@inproceedings{liu_multivariate_2014,
	title = {Multivariate volume visualization through dynamic projections},
	doi = {10.1109/LDAV.2014.7013202},
	abstract = {We propose a multivariate volume visualization framework that tightly couples dynamic projections with a high-dimensional transfer function design for interactive volume visualization. We assume that the complex, high-dimensional data in the attribute space can be well-represented through a collection of low-dimensional linear subspaces, and embed the data points in a variety of 2D views created as projections onto these subspaces. Through dynamic projections, we present animated transitions between different views to help the user navigate and explore the attribute space for effective transfer function design. Our framework not only provides a more intuitive understanding of the attribute space but also allows the design of the transfer function under multiple dynamic views, which is more flexible than being restricted to a single static view of the data. For large volumetric datasets, we maintain interactivity during the transfer function design via intelligent sampling and scalable clustering. Using examples in combustion and climate simulations, we demonstrate how our framework can be used to visualize interesting structures in the volumetric space.},
	booktitle = {2014 {IEEE} 4th {Symposium} on {Large} {Data} {Analysis} and {Visualization} ({LDAV})},
	author = {Liu, Shusen and Wang, Bei and Thiagarajan, J.J. and Bremer, P.-T. and Pascucci, V.},
	month = nov,
	year = {2014},
	keywords = {2D view, animated transitions, attribute space, computer animation, data visualisation, Data visualization, dynamic projections, high-dimensional data, high-dimensional transfer function design, Hurricanes, hurricanes, Image color analysis, intelligent sampling, interactive systems, interactive volume visualization, low-dimensional linear subspaces, multiple dynamic view, multivariate volume visualization, Navigation, pattern clustering, principal component analysis, scalable clustering, Space exploration, Transfer functions},
	pages = {35--42},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\0m7eflhb.default\\zotero\\storage\\VIKJNJHJ\\articleDetails.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\0m7eflhb.default\\zotero\\storage\\4AHC5CTZ\\articleDetails.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\0m7eflhb.default\\zotero\\storage\\WP54JTEJ\\Liu et al. - 2014 - Multivariate volume visualization through dynamic .pdf:application/pdf;IEEE Xplore Full Text PDF:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\0m7eflhb.default\\zotero\\storage\\AG3NHCDS\\Liu et al. - 2014 - Multivariate volume visualization through dynamic .pdf:application/pdf}
}

@article{akiba_visualizing_2007,
	title = {Visualizing {Multivariate} {Volume} {Data} from {Turbulent} {Combustion} {Simulations}},
	volume = {9},
	issn = {1521-9615},
	doi = {10.1109/MCSE.2007.42},
	abstract = {To understand dynamic mechanisms, scientists need intuitive and convenient ways to validate known relationships and reveal hidden ones among multiple variables},
	number = {2},
	journal = {Computing in Science Engineering},
	author = {Akiba, Hiroshi and Ma, Kwan-Liu and Chen, Jacqueline H. and Hawkes, Evatt R.},
	month = apr,
	year = {2007},
	keywords = {Biomedical Engineering, Biomedical imaging, chemistry computing, combustion, Computational modeling, data rendering, data visualisation, Data visualization, dynamic mechanisms, dynamics, Fires, heat of combustion, Magnetic resonance imaging, multivariate, multivariate volume data visualization, rendering (computer graphics), Rendering (computer graphics), turbulence, turbulent, turbulent combustion simulations, Ultrasonic imaging, visualization, X-ray imaging},
	pages = {76 --83},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\0m7eflhb.default\\zotero\\storage\\W7SGPANQ\\abstractAuthors.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\0m7eflhb.default\\zotero\\storage\\7HW2URBD\\articleDetails.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\0m7eflhb.default\\zotero\\storage\\K2D2J3SU\\Akiba et al. - 2007 - Visualizing Multivariate Volume Data from Turbulen.pdf:application/pdf;IEEE Xplore Full Text PDF:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\0m7eflhb.default\\zotero\\storage\\8SABM3EX\\Akiba et al. - 2007 - Visualizing Multivariate Volume Data from Turbulen.pdf:application/pdf}
}

@inproceedings{kniss_volume_2002,
 author = {Kniss, Joe and Hansen, Charles and Grenier, Michel and Robinson, Tom},
 title = {Volume Rendering Multivariate Data to Visualize Meteorological Simulations: A Case Study},
 booktitle = {Proceedings of the Symposium on Data Visualisation 2002},
 series = {VISSYM '02},
 year = {2002},
 isbn = {1-58113-536-X},
 location = {Barcelona, Spain},
 pages = {189--ff},
 url = {http://dl.acm.org/citation.cfm?id=509740.509770},
 acmid = {509770},
 publisher = {Eurographics Association},
 address = {Aire-la-Ville, Switzerland, Switzerland},
}

@inproceedings{tominski_task-driven_2008,
	title = {Task-{Driven} {Color} {Coding}},
	doi = {10.1109/IV.2008.24},
	abstract = {Color coding is a widely used visualization method for scalar data. To generate expressive and effective visual representations, it is extremely important to carefully design the mapping from data to color. In this paper, we describe a color coding approach that accounts for the different tasks users might pursue when analyzing data. Our task description is based on the task model of Andrienko \& Andrienko. We apply different color scales and introduce strategies to adapt the color mapping function to support tasks like comparison, localization, or identification of data values.},
	booktitle = {Information {Visualisation}, 2008. {IV} '08. 12th {International} {Conference}},
	author = {Tominski, C. and Fuchs, G. and Schumann, H.},
	month = jul,
	year = {2008},
	keywords = {Cathode ray tubes, Color, color mapping function, color scales, Computer science, data analysis, data visualisation, data visualization, Frequency, Guidelines, Humans, image colour analysis, Liquid crystal displays, Task, task-driven color coding, Thin film transistors, visualization, visualization method},
	pages = {373--380},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\0m7eflhb.default\\zotero\\storage\\ZTCXP7J3\\abs_all.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\0m7eflhb.default\\zotero\\storage\\VTEFDK5V\\Tominski et al. - 2008 - Task-Driven Color Coding.pdf:application/pdf}
}

@article{harrower_colorbrewer.org:_2003,
	title = {{ColorBrewer}.org: {An} {Online} {Tool} for {Selecting} {Colour} {Schemes} for {Maps}},
	volume = {40},
	issn = {0008-7041},
	shorttitle = {{ColorBrewer}.org},
	url = {http://www.maneyonline.com/doi/abs/10.1179/000870403235002042},
	doi = {10.1179/000870403235002042},
	number = {1},
	urldate = {2015-03-30},
	journal = {The Cartographic Journal},
	author = {Harrower, Mark and Brewer, Cynthia A.},
	month = jun,
	year = {2003},
	pages = {27--37},
	file = {harrower2003.pdf:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\0m7eflhb.default\\zotero\\storage\\B4HDJ8FA\\harrower2003.pdf:application/pdf;Snapshot:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\0m7eflhb.default\\zotero\\storage\\IMB4UDAZ\\000870403235002042.html:text/html}
}

@inproceedings{hastreiter_integrated_1998,
	title = {Integrated registration and visualization of medical image data},
	doi = {10.1109/CGI.1998.694253},
	abstract = {Different imaging modalities give insight to vascular, anatomical and functional information which assist diagnosis and therapy planning in medicine. Registration and consecutive visualization allow to combine the image data and thereby convey more meaningful images to the clinician. Applying a voxel based approach based on mutual information, accurate and retrospective registration is provided. However, optimization and consecutive visualization procedures require a huge amount of trilinear interpolation operations to re-sample the data. Ensuring fast performance which is fundamental for medical routine, we suggest an integrated approach which takes advantage of the imaging and texture mapping subsystem of graphics computers. All trilinear interpolation is completely performed with hardware assisted 3D texture mapping. The 1D and 2D histograms of the datasets which are necessary for the calculation of mutual information are obtained with different hardware accelerated imaging operations. For the simultaneous and interactive visualization of the registered datasets a new approach was developed which allows for versatile fusion operations. Using similar procedures supported by hardware, contributes considerably to accelerate registration and visualization. Implementing our approach within a previously presented framework (Hastreiter et al., 1996) (Sommer et al., 1998) based on OpenInventor and OpenGL provides intuitive manipulation. Clinical examples show the value of our approach in practice},
	booktitle = {Computer {Graphics} {International}, 1998. {Proceedings}},
	author = {Hastreiter, P. and Ertl, T.},
	month = jun,
	year = {1998},
	keywords = {1D histograms, 2D histograms, 3D texture mapping, Acceleration, anatomical information, Biomedical imaging, Computer Graphics, data visualisation, data visualization, functional information, graphics computers, Hardware, hardware accelerated imaging, Histograms, image registration, image texture, interpolation, medical diagnosis, medical diagnostic imaging, medical image data visualization, medical image processing, medical image registration, medical treatment, mutual information, OpenGL, OpenInventor, optimisation, Optimization, performance, therapy planning, three dimensional texture mapping, trilinear interpolation operations, vascular information, voxel based approach},
	pages = {78--85},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\0m7eflhb.default\\zotero\\storage\\GWTXDQ7G\\abs_all.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\0m7eflhb.default\\zotero\\storage\\R9TBCQ9S\\Hastreiter and Ertl - 1998 - Integrated registration and visualization of medic.pdf:application/pdf}
}

@inproceedings{fedkiw_visual_2001,
	address = {New York, NY, USA},
	series = {{SIGGRAPH} '01},
	title = {Visual {Simulation} of {Smoke}},
	isbn = {1-58113-374-X},
	url = {http://doi.acm.org/10.1145/383259.383260},
	doi = {10.1145/383259.383260},
	abstract = {In this paper, we propose a new approach to numerical smoke simulation for computer graphics applications. The method proposed here exploits physics unique to smoke in order to design a numerical method that is both fast and efficient on the relatively coarse grids traditionally used in computer graphics applications (as compared to the much finer grids used in the computational fluid dynamics literature). We use the inviscid Euler equations in our model, since they are usually more appropriate for gas modeling and less computationally intensive than the viscous Navier-Stokes equations used by others. In addition, we introduce a physically consistent vorticity confinement term to model the small scale rolling features characteristic of smoke that are absent on most coarse grid simulations. Our model also correctly handles the inter-action of smoke with moving objects.},
	urldate = {2015-02-25},
	booktitle = {Proceedings of the 28th {Annual} {Conference} on {Computer} {Graphics} and {Interactive} {Techniques}},
	publisher = {ACM},
	author = {Fedkiw, Ronald and Stam, Jos and Jensen, Henrik Wann},
	year = {2001},
	keywords = {computational fluid dynamics, Euler equations, Navier-Stokes equations, participating media, semi-Lagrangian methods, smoke, stable fluids, vorticity confinement},
	pages = {15--22},
	file = {ACM Full Text PDF:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\0m7eflhb.default\\zotero\\storage\\CSXGMVWE\\Fedkiw et al. - 2001 - Visual Simulation of Smoke.pdf:application/pdf}
}

@article{cui_measuring_2006,
	title = {Measuring {Data} {Abstraction} {Quality} in {Multiresolution} {Visualizations}},
	volume = {12},
	issn = {1077-2626},
	doi = {10.1109/TVCG.2006.161},
	abstract = {Data abstraction techniques are widely used in multiresolution visualization systems to reduce visual clutter and facilitate analysis from overview to detail. However, analysts are usually unaware of how well the abstracted data represent the original dataset, which can impact the reliability of results gleaned from the abstractions. In this paper, we define two data abstraction quality measures for computing the degree to which the abstraction conveys the original dataset: the histogram difference measure and the nearest neighbor measure. They have been integrated within XmdvTool, a public-domain multiresolution visualization system for multivariate data analysis that supports sampling as well as clustering to simplify data. Several interactive operations are provided, including adjusting the data abstraction level, changing selected regions, and setting the acceptable data abstraction quality level. Conducting these operations, analysts can select an optimal data abstraction level. Also, analysts can compare different abstraction methods using the measures to see how well relative data density and outliers are maintained, and then select an abstraction method that meets the requirement of their analytic tasks},
	number = {5},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Cui, Q. and Ward, M.O. and Rundensteiner, E.A. and Yang, J.},
	month = sep,
	year = {2006},
	keywords = {Bioinformatics, Clustering, Coordinate measuring machines, data abstraction quality measures, data analysis, data clustering, Data structures, data structures, data visualisation, Data visualization, Delay, Density measurement, Displays, histogram difference measure, Histograms, Metrics, Multiresolution Visualization Authors 1:, multivariate data analysis, nearest neighbor measure, Nearest neighbor searches, pattern clustering, public-domain multiresolution visualization system, sampling, sampling methods, very large databases, XmdvTool},
	pages = {709--716},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\0m7eflhb.default\\zotero\\storage\\EX32V6F4\\abs_all.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\0m7eflhb.default\\zotero\\storage\\CJQKXCQU\\Cui et al. - 2006 - Measuring Data Abstraction Quality in Multiresolut.pdf:application/pdf}
}

@inproceedings{rundensteiner_xmdvtoolq::_2007,
	address = {New York, NY, USA},
	series = {{SIGMOD} '07},
	title = {{XmdvtoolQ}:: {Quality}-aware {Interactive} {Data} {Exploration}},
	isbn = {978-1-59593-686-8},
	shorttitle = {{XmdvtoolQ}},
	url = {http://doi.acm.org/10.1145/1247480.1247623},
	doi = {10.1145/1247480.1247623},
	abstract = {In this work, we describe our approach for making the interactive data exploration system, called XmdvTool, quality-aware to assure informed decision-making. XmdvToolQ, makes quality or lack thereof explicit for all stages of the data exploration process from raw data, to abstracted data, to the final visual displays, allowing users to query and navigate through data-, structure- and quality-spaces.},
	urldate = {2015-06-09},
	booktitle = {Proceedings of the 2007 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Rundensteiner, Elke A. and Ward, Matthew O. and Xie, Zaixian and Cui, Qingguang and Wad, Charudatta V. and Yang, Di and Huang, Shiping},
	year = {2007},
	keywords = {abstraction quality, data quality, display quality},
	pages = {1109--1112},
	file = {ACM Full Text PDF:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\0m7eflhb.default\\zotero\\storage\\NEDQNCNU\\Rundensteiner et al. - 2007 - XmdvtoolQ Quality-aware Interactive Data Explora.pdf:application/pdf}
}

@inproceedings{chen_measuring_2005,
	title = {Measuring the quality of network visualization},
	doi = {10.1145/1065385.1065509},
	abstract = {A quantitative method is developed for measuring the quality of network visualizations in terms of log-likelihood metrics resulted from expectation maximization (EM) clustering intrinsic and extrinsic attributes of network nodes},
	booktitle = {Proceedings of the 5th {ACM}/{IEEE}-{CS} {Joint} {Conference} on {Digital} {Libraries}, 2005. {JCDL} '05},
	author = {Chen, C.},
	month = jun,
	year = {2005},
	keywords = {Chaos, Clustering algorithms, data visualisation, Data visualization, Educational institutions, EM clustering, expectation maximization clustering, graphical user interfaces, Information science, Information systems, learning (artificial intelligence), log-likelihood metrics, Machine Learning, Machine learning algorithms, network nodes, network visualization, network visualizations, pattern clustering, quality measurement, quality metrics of quality, User interfaces},
	pages = {405--405},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\0m7eflhb.default\\zotero\\storage\\TF4CDTIX\\abs_all.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\0m7eflhb.default\\zotero\\storage\\ZGI3757B\\Chen - 2005 - Measuring the quality of network visualization.pdf:application/pdf}
}

@inproceedings{van_wijk_value_2005,
	title = {The value of visualization},
	doi = {10.1109/VISUAL.2005.1532781},
	abstract = {The field of visualization is getting mature. Many problems have been solved, and new directions are sought for. In order to make good choices, an understanding of the purpose and meaning of visualization is needed. Especially, it would be nice if we could assess what a good visualization is. In this paper an attempt is made to determine the value of visualization. A technological viewpoint is adopted, where the value of visualization is measured based on effectiveness and efficiency. An economic model of visualization is presented, and benefits and costs are established. Next, consequences (brand limitations of visualization are discussed (including the use of alternative methods, high initial costs, subjective/less, and the role of interaction), as well as examples of the use of the model for the judgement of existing classes of methods and understanding why they are or are not used in practice. Furthermore, two alternative views on visualization are presented and discussed: viewing visualization as an art or as a scientific discipline. Implications and future directions are identified.},
	booktitle = {{IEEE} {Visualization}, 2005. {VIS} 05},
	author = {van Wijk, J.J.},
	month = oct,
	year = {2005},
	keywords = {Application software, Art, Chromium, computer graphics, Computer science, Costs, data visualisation, Data visualization, Explosions, Mathematics, User interfaces, visualization economic model, visualization value},
	pages = {79--86},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\0m7eflhb.default\\zotero\\storage\\5WEK9XMC\\abs_all.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\0m7eflhb.default\\zotero\\storage\\5XG8VDCA\\van Wijk - 2005 - The value of visualization.pdf:application/pdf}
}

@inproceedings{filonik_measuring_2009,
	title = {Measuring {Aesthetics} for {Information} {Visualization}},
	doi = {10.1109/IV.2009.94},
	abstract = {Aesthetics is an unsolved problem of information visualization, because there is no satisfactory understanding of what constitutes aesthetic effect. This survey paper gives an overview of approaches to model aesthetics, starting with Birkhoffpsilas aesthetic measure and continuing to recent ones based on mathematical and information theoretical concepts. Common concepts in the different models are highlighted, such as the effects of order and complexity. Further, practical techniques for generating aesthetic visualizations are shown together with examples of recent work in this field. Finally, the paper discusses some of the key issues regarding aesthetics and the human factor in the visualization process. Empirical studies have shown a correlation between perceived aesthetics and usability, meaning that a better understanding of aesthetics could improve the usability of visualizations.},
	booktitle = {Information {Visualisation}, 2009 13th {International} {Conference}},
	author = {Filonik, D. and Baur, D.},
	month = jul,
	year = {2009},
	keywords = {aesthetics, Art, art-beauty philosophy, Birkhoff aesthetics measurement, Chaos, data visualisation, human factor, Human factors, Image generation, Informatics, information visualization, Mathematical model, Measures, philosophical aspects, Pressing, Robustness, Usability, Visualization},
	pages = {579--584},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\0m7eflhb.default\\zotero\\storage\\QF4JEBU4\\abs_all.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\0m7eflhb.default\\zotero\\storage\\N8H7FJCC\\Filonik and Baur - 2009 - Measuring Aesthetics for Information Visualization.pdf:application/pdf}
}

@article{janicke_salience-based_2010,
	title = {A {Salience}-based {Quality} {Metric} for {Visualization}},
	volume = {29},
	copyright = {Â© 2010 The Author(s) Journal compilation Â© 2010 The Eurographics Association and Blackwell Publishing Ltd.},
	issn = {1467-8659},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1467-8659.2009.01667.x/abstract},
	doi = {10.1111/j.1467-8659.2009.01667.x},
	abstract = {Salience detection is a principle mechanism to facilitate visual attention. A good visualization guides the observer's attention to the relevant aspects of the representation. Hence, the distribution of salience over a visualization image is an essential measure of the quality of the visualization. We describe a method for computing such a metric for a visualization image in the context of a given dataset. We show how this technique can be used to analyze a visualization's salience, improve an existing visualization, and choose the best representation from a set of alternatives. The usefulness of this proposed metric is illustrated using examples from information visualization, volume visualization and flow visualization.},
	language = {en},
	number = {3},
	urldate = {2014-09-09},
	journal = {Computer Graphics Forum},
	author = {J{\"a}nicke, H. and Chen, M.},
	month = jun,
	year = {2010},
	keywords = {I.3.3 [Computer Graphics]: Picture/Image Generation—Line and curve generation},
	pages = {1183--1192}
}

@inproceedings{wu_quantitative_2010,
	title = {Quantitative effectiveness measures for direct volume rendered images},
	doi = {10.1109/PACIFICVIS.2010.5429623},
	abstract = {With the rapid development in graphics hardware and volume rendering techniques, many volumetric datasets can now be rendered in real time on a standard PC equipped with a commodity graphics board. However, the effectiveness of the results, especially direct volume rendered images, is difficult to validate and users may not be aware of ambiguous or even misleading information in the results. This limits the applications of volume visualization. In this paper, we introduce four quantitative effectiveness measures: distinguishability, contour clarity, edge consistency, and depth coherence measures, which target different effectiveness issues for direct volume rendered images. Based on the measures, we develop a visualization system with automatic effectiveness assessment, providing users with instant feedback on the effectiveness of the results. The case study and user evaluation have demonstrated the high potential of our system.},
	booktitle = {2010 {IEEE} {Pacific} {Visualization} {Symposium} ({PacificVis})},
	author = {Wu, Yingcai and Qu, Huamin and Chung, Ka-Kei and Chan, Ming-Yuen and Zhou, Hong},
	year = {2010},
	keywords = {automatic effectiveness assessment, commodity graphics board, computer graphics, contour clarity measure, data analysis, data visualisation, Data visualization, depth coherence measure, direct volume rendered images, distinguishability measure, edge consistency measure, Feedback, graphics hardware, Hardware, Image generation, Measurement standards, PC, quantitative effectiveness measure, rendering (computer graphics), Rendering (computer graphics), Standards development, volume measurement, volume rendering technique, volumetric datasets, volume visualization},
	pages = {1--8},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\0m7eflhb.default\\zotero\\storage\\3VGTT3K5\\abstractCitations.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\0m7eflhb.default\\zotero\\storage\\7MQ7WDAS\\Wu et al. - 2010 - Quantitative effectiveness measures for direct vol.pdf:application/pdf}
}

@article{cai_automatic_2013,
	title = {Automatic transfer function design for medical visualization using visibility distributions and projective color mapping},
	volume = {37},
	issn = {0895-6111},
	url = {http://www.medicalimagingandgraphics.com/article/S0895-6111(13)00141-9/abstract},
	doi = {10.1016/j.compmedimag.2013.08.008},
	abstract = {Transfer functions play a key role in volume rendering of medical data, but transfer function manipulation is unintuitive and can be time-consuming; achieving an optimal visualization of patient anatomy or pathology is difficult. To overcome this problem, we present a system for automatic transfer function design based on visibility distribution and projective color mapping. Instead of assigning opacity directly based on voxel intensity and gradient magnitude, the opacity transfer function is automatically derived by matching the observed visibility distribution to a target visibility distribution. An automatic color assignment scheme based on projective mapping is proposed to assign colors that allow for the visual discrimination of different structures, while also reflecting the degree of similarity between them. When our method was tested on several medical volumetric datasets, the key structures within the volume were clearly visualized with minimal user intervention.},
	number = {7},
	urldate = {2014-07-18},
	journal = {Computerized Medical Imaging and Graphics},
	author = {Cai, Lile and Tay, Wei-Liang and Nguyen, Binh P. and Chui, Chee-Kong and Ong, Sim-Heng},
	month = oct,
	year = {2013},
	keywords = {Color mapping, Transfer function design, Visibility distribution, volume visualization, Volume visualization},
	pages = {450--458},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\0m7eflhb.default\\zotero\\storage\\P387H37N\\Cai et al. - 2013 - Automatic transfer function design for medical vis.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\0m7eflhb.default\\zotero\\storage\\K6HGS37E\\S0895611113001419.html:text/html}
}

@article{giesen_conjoint_2007,
	title = {Conjoint {Analysis} to {Measure} the {Perceived} {Quality} in {Volume} {Rendering}},
	volume = {13},
	issn = {1077-2626},
	doi = {10.1109/TVCG.2007.70542},
	abstract = {Visualization algorithms can have a large number of parameters, making the space of possible rendering results rather high-dimensional. Only a systematic analysis of the perceived quality can truly reveal the optimal setting for each such parameter. However, an exhaustive search in which all possible parameter permutations are presented to each user within a study group would be infeasible to conduct. Additional complications may result from possible parameter co-dependencies. Here, we will introduce an efficient user study design and analysis strategy that is geared to cope with this problem. The user feedback is fast and easy to obtain and does not require exhaustive parameter testing. To enable such a framework we have modified a preference measuring methodology, conjoint analysis, that originated in psychology and is now also widely used in market research. We demonstrate our framework by a study that measures the perceived quality in volume rendering within the context of large parameter spaces.},
	number = {6},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Giesen, J. and Mueller, K. and Schuberth, E. and Wang, Lujin and Zolliker, P.},
	month = nov,
	year = {2007},
	keywords = {Algorithms, computer graphics, Conjoint Analysis, data visualisation, Data visualization, Design engineering, Feedback, Focusing, Humans, image enhancement, Image Interpretation, Computer-Assisted, Imaging, Three-Dimensional, market research, Parameterized Algorithms, perceived quality, psychology, Quality Control, rendering (computer graphics), Reproducibility of Results, Sensitivity and Specificity, Testing, user feedback, visualization algorithms, Visual Perception, volume measurement, Volume rendering, volume visualization},
	pages = {1664--1671},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\0m7eflhb.default\\zotero\\storage\\CTUTPXRG\\abs_all.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\0m7eflhb.default\\zotero\\storage\\TPG3J7XJ\\Giesen et al. - 2007 - Conjoint Analysis to Measure the Perceived Quality.pdf:application/pdf}
}

@article{itti_model_1998,
	title = {A model of saliency-based visual attention for rapid scene analysis},
	volume = {20},
	issn = {0162-8828},
	doi = {10.1109/34.730558},
	abstract = {A visual attention system, inspired by the behavior and the neuronal architecture of the early primate visual system, is presented. Multiscale image features are combined into a single topographical saliency map. A dynamical neural network then selects attended locations in order of decreasing saliency. The system breaks down the complex problem of scene understanding by rapidly selecting, in a computationally efficient manner, conspicuous locations to be analyzed in detail},
	number = {11},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Itti, L. and Koch, C. and Niebur, E.},
	month = nov,
	year = {1998},
	keywords = {Biological system modeling, Brain modeling, Computer architecture, Computer vision, dynamical neural network, feature extraction, Feature extraction, Hardware, Image analysis, image recognition, Layout, neural nets, neural networks, object detection, rapid scene analysis, Saliency, scene understanding, target detection, target tracking, topographical saliency map, Visual attention, visual search, Visual System},
	pages = {1254--1259},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\0m7eflhb.default\\zotero\\storage\\3X2XCTW7\\abs_all.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\0m7eflhb.default\\zotero\\storage\\ESCDGV9G\\Itti et al. - 1998 - A model of saliency-based visual attention for rap.pdf:application/pdf}
}

@article{kim_saliency-guided_2006,
	title = {Saliency-guided {Enhancement} for {Volume} {Visualization}},
	volume = {12},
	issn = {1077-2626},
	url = {http://dx.doi.org/10.1109/TVCG.2006.174},
	doi = {10.1109/TVCG.2006.174},
	abstract = {Recent research in visual saliency has established a computational measure of perceptual importance. In this paper we present a visual-saliency-based operator to enhance selected regions of a volume. We show how we use such an operator on a user-specified saliency field to compute an emphasis field. We further discuss how the emphasis field can be integrated into the visualization pipeline through its modifications of regional luminance and chrominance. Finally, we validate our work using an eye-tracking-based user study and show that our new saliency enhancement operator is more effective at eliciting viewer attention than the traditional Gaussian enhancement operator.},
	number = {5},
	urldate = {2013-03-07},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Kim, Youngmin and Varshney, Amitabh},
	month = sep,
	year = {2006},
	keywords = {Algorithms, Art, Attention, Biomedical imaging, computer graphics, data visualisation, Data visualization, Eye Movements, Fixation, Ocular, Geometry, Humans, image enhancement, Image Interpretation, Computer-Assisted, Imaging, Three-Dimensional, Large-scale systems, Mouth, non-photorealistic rendering, Optical attenuators, perceptual enhancement, Pipelines, rendering (computer graphics), Rendering (computer graphics), Saliency, saliency-guided enhancement, Transfer functions, transfer functions, User-Computer Interface, Visual attention, visual-saliency-based operator, Volume rendering, volume rendering, volume visualization},
	pages = {925--932},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\0m7eflhb.default\\zotero\\storage\\7S3EG4U5\\articleDetails.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\0m7eflhb.default\\zotero\\storage\\GQZFMBRX\\Kim and Varshney - 2006 - Saliency-guided Enhancement for Volume Visualizati.pdf:application/pdf;saliency_guided_enhancement.ppt:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\0m7eflhb.default\\zotero\\storage\\2XCJEI6T\\saliency_guided_enhancement.ppt:application/msword}
}

@book{palmer_vision_1999,
	title = {Vision {Science}: {Photons} to {Phenomenology}},
	isbn = {9780262161831},
	shorttitle = {Vision {Science}},
	abstract = {This book revolutionizes how vision can be taught to undergraduate and graduate students in cognitive science, psychology, and optometry. It is the first comprehensive textbook on vision to reflect the integrated computational approach of modern research scientists. This new interdisciplinary approach, called "vision science," integrates psychological, computational, and neuroscientific perspectives.The book covers all major topics related to vision, from early neural processing of image structure in the retina to high-level visual attention, memory, imagery, and awareness. The presentation throughout is theoretically sophisticated yet requires minimal knowledge of mathematics. There is also an extensive glossary, as well as appendices on psychophysical methods, connectionist modeling, and color technology. The book will serve not only as a comprehensive textbook on vision, but also as a valuable reference for researchers in cognitive science, psychology, neuroscience, computer science, optometry, and philosophy.},
	language = {en},
	publisher = {MIT Press},
	author = {Palmer, Stephen E.},
	year = {1999},
	keywords = {Medical / Neuroscience, Psychology / Cognitive Psychology, Psychology / Cognitive Psychology \& Cognition, Science / General, Science / Life Sciences / Anatomy \& Physiology}
}

@article{koch_predicting_1999,
	title = {Predicting the visual world: silence is golden},
	volume = {2},
	copyright = {Â© 1999 Nature Publishing Group},
	issn = {1097-6256},
	shorttitle = {Predicting the visual world},
	url = {http://www.nature.com/neuro/journal/v2/n1/abs/nn0199_9.html},
	doi = {10.1038/4511},
	abstract = {In predictive coding, only unexpected input features are signaled to the next stage of processing. Rao and Ballard use this approach to model extraâˆ’classical receptive field effects.},
	language = {en},
	number = {1},
	urldate = {2015-06-10},
	journal = {Nature Neuroscience},
	author = {Koch, Christof and Poggio, Tomaso},
	month = jan,
	year = {1999},
	pages = {9--10},
	file = {Full Text PDF:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\0m7eflhb.default\\zotero\\storage\\BHH8T47W\\Koch and Poggio - 1999 - Predicting the visual world silence is golden.pdf:application/pdf;Snapshot:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\0m7eflhb.default\\zotero\\storage\\85JTEKFR\\nn0199_9.html:text/html}
}

@inproceedings{viola_importance-driven_2004,
	address = {Washington, DC, USA},
	series = {{VIS} '04},
	title = {Importance-{Driven} {Volume} {Rendering}},
	isbn = {0-7803-8788-0},
	url = {http://dx.doi.org/10.1109/VISUAL.2004.48},
	doi = {10.1109/VISUAL.2004.48},
	abstract = {This paper introduces importance-driven volume rendering as a novel technique for automatic focus and context display of volumetric data. Our technique is a generalization of cut-away views, which Â¿ depending on the viewpoint Â¿ remove or suppress less important parts of a scene to reveal more important underlying information. We automatize and apply this idea to volumetric data. Each part of the volumetric data is assigned an object importance which encodes visibility priority. This property determines which structures should be readily discernible and which structures are less important. In those image regions, where an object occludes more important structures it is displayed more sparsely than in those areas where no occlusion occurs. Thus the objects of interest are clearly visible. For each object several representations, i.e., levels of sparseness, are specified. The display of an individual object may incorporate different levels of sparseness. The goal is to emphasize important structures and to maximize the information content in the final image. This paper also discusses several possible schemes for level of sparseness specification and different ways how object importance can be composited to determine the final appearance of a particular object.},
	urldate = {2013-07-27},
	booktitle = {Proceedings of the conference on {Visualization} '04},
	publisher = {IEEE Computer Society},
	author = {Viola, Ivan and Kanitsar, Armin and Groller, Meister Eduard},
	year = {2004},
	keywords = {Biomedical equipment, computer graphics, data visualisation, Data visualization, Focusing, Image generation, image representation, importance-driven volume rendering, Lesions, level-of-details, levels of sparseness, Liver neoplasms, medical diagnostic computing, Medical services, nonphotorealistic techniques, object importance, occlusion, rendering (computer graphics), Shape, view-dependent visualization},
	pages = {139--146},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\0m7eflhb.default\\zotero\\storage\\ESH65NZJ\\abs_all.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\0m7eflhb.default\\zotero\\storage\\FEQVS9MJ\\abs_all.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\0m7eflhb.default\\zotero\\storage\\FNGBEAWG\\Viola et al. - 2004 - Importance-driven volume rendering.pdf:application/pdf;IEEE Xplore Full Text PDF:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\0m7eflhb.default\\zotero\\storage\\TCPZ5ED3\\Viola et al. - 2004 - Importance-driven volume rendering.pdf:application/pdf}
}

@article{shen_spatiotemporal_2015,
	title = {Spatiotemporal volume saliency},
	issn = {1343-8875, 1875-8975},
	url = {http://link.springer.com/article/10.1007/s12650-015-0293-y},
	doi = {10.1007/s12650-015-0293-y},
	abstract = {Abstract This paper proposes spatiotemporal volume saliency to detect and explore salient regions in time-varying volume data. Based on the center-surround hypothesis that the salient region stands out from its surroundings, we extend the spatial saliency to time domain and introduce temporal volume saliency. It is defined as a center-surround operator on Gaussian-weighted mean attribute gradient between steps in a scale-independent manner. By combing spatial saliency and temporal saliency together, our spatiotemporal volume saliency is effective in detecting changes of salient regions. We demonstrate its utility in this regard by automating transfer function design and selecting key frames for time-varying volume data. Graphical abstract},
	language = {en},
	urldate = {2015-05-24},
	journal = {Journal of Visualization},
	author = {Shen, Enya and Wang, Yunhai and Li, Sikun},
	month = apr,
	volume = {19},
	number = {1},
	year = {2015},
	keywords = {Classical Continuum Physics, Computer Imaging, Vision, Pattern Recognition and Graphics, Engineering Fluid Dynamics, Engineering Thermodynamics, Heat and Mass Transfer, Human perception, Spatiotemporal volume saliency, Time-varying data, Volume exploration},
	pages = {1--12},
	file = {Full Text PDF:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\0m7eflhb.default\\zotero\\storage\\S6PQXBFW\\Shen et al. - 2015 - Spatiotemporal volume saliency.pdf:application/pdf;Snapshot:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\0m7eflhb.default\\zotero\\storage\\ADICUBI6\\10.html:text/html}
}

@inproceedings{harel_graph-based_2006,
	title = {Graph-{Based} {Visual} {Saliency}},
	abstract = {A new bottom-up visual saliency model, Graph-Based Visual Saliency (GBVS), is proposed. It consists of two steps: ï¿½rst forming activation maps on certain feature channels, and then normalizing them in a way which highlights conspicuity and admits combination with other maps. The model is simple, and biologically plau- sible insofar as it is naturally parallelized. This model powerfully predicts human ï¿½xations on 749 variations of 108 natural images, achieving 98\% of the ROC area of a human-based control, whereas the classical algorithms of Itti \& Koch ((2), (3), (4)) achieve only 84\%.Read \& annotate PDFRead, annotate and save this article using the colwiz Interactive PDF Reader Add to colwizSave this article to your colwiz library to read and reference anywhere},
	booktitle = {Proceedings of {Neural} {Information} {Processing} {Systems} ({NIPS})},
	author = {Harel, Jonathan and Koch, Christof and Perona, Pietro},
	year = {2006},
	keywords = {Bottom Up, natural images},
	pages = {545--552},
	file = {Citeseer - Full Text PDF:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\0m7eflhb.default\\zotero\\storage\\EGRFHAN3\\Harel et al. - 2007 - Graph-based visual saliency.pdf:application/pdf}
}

@article{lee_mesh_2005,
	author = {Lee, Chang Ha and Varshney, Amitabh and Jacobs, David W.},
	title = {Mesh Saliency},
	journal = {ACM Trans. Graph.},
	issue_date = {July 2005},
	volume = {24},
	number = {3},
	month = jul,
	year = {2005},
	issn = {0730-0301},
	pages = {659--666},
	numpages = {8},
	url = {http://doi.acm.org/10.1145/1073204.1073244},
	doi = {10.1145/1073204.1073244},
	acmid = {1073244},
	publisher = {ACM},
	address = {New York, NY, USA},
	keywords = {perception, saliency, simplification, viewpoint selection, visual attention},
} 


@inproceedings{lee_mesh_2005---faulty,
	address = {New York, NY, USA},
	series = {{SIGGRAPH} '05},
	title = {Mesh {Saliency}},
	url = {http://doi.acm.org/10.1145/1186822.1073244},
	doi = {10.1145/1186822.1073244},
	abstract = {Research over the last decade has built a solid mathematical foundation for representation and analysis of 3D meshes in graphics and geometric modeling. Much of this work however does not explicitly incorporate models of low-level human visual attention. In this paper we introduce the idea of mesh saliency as a measure of regional importance for graphics meshes. Our notion of saliency is inspired by low-level human visual system cues. We define mesh saliency in a scale-dependent manner using a center-surround operator on Gaussian-weighted mean curvatures. We observe that such a definition of mesh saliency is able to capture what most would classify as visually interesting regions on a mesh. The human-perception-inspired importance measure computed by our mesh saliency operator results in more visually pleasing results in processing and viewing of 3D meshes. compared to using a purely geometric measure of shape. such as curvature. We discuss how mesh saliency can be incorporated in graphics applications such as mesh simplification and viewpoint selection and present examples that show visually appealing results from using mesh saliency.},
	urldate = {2014-10-08},
	booktitle = {{ACM} {SIGGRAPH} 2005 {Papers}},
	publisher = {ACM},
	author = {Lee, Chang Ha and Varshney, Amitabh and Jacobs, David W.},
	year = {2005},
	keywords = {perception, Saliency, simplification, Viewpoint selection, visual attention},
	pages = {659--666},
	file = {ACM Full Text PDF:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\0m7eflhb.default\\zotero\\storage\\DHAVHDHN\\Lee et al. - 2005 - Mesh Saliency.pdf:application/pdf}
}

@book{fairchild_color_2013,
	title = {Color {Appearance} {Models}},
	isbn = {1118653106},
	abstract = {The essential resource for readers needing to understand visual perception and for those trying to produce, reproduce and measure color appearance in various applications such as imaging, entertainment, materials, design, architecture and lighting. This book builds upon the success of previous editions, and will continue to serve the needs of those professionals working in the field to solve practical problems or looking for background for on-going research projects. It would also act as a good course text for senior undergraduates and postgraduates studying color science. The 3rd Edition of Color Appearance Models contains numerous new and expanded sections providing an updated review of color appearance and includes many of the most widely used models to date, ensuring its continued success as the comprehensive resource on color appearance models. Key features:  Presents the fundamental concepts and phenomena of color appearance (what objects look like in typical viewing situations) and practical techniques to measure, model and predict those appearances. Includes the clear explanation of fundamental concepts that makes the implementation of mathematical models very easy to understand. Explains many different types of models, and offers a clear context for the models, their use, and future directions in the field.},
	language = {en},
	publisher = {John Wiley \& Sons},
	author = {Fairchild, Mark D.},
	month = jun,
	year = {2013},
	keywords = {Technology \& Engineering / Electrical, Technology \& Engineering / Imaging Systems}
}

@phdthesis{emsenhuber_visibility_2008,
	opt_address = {Favoritenstrasse 9-11/186, A-1040 Vienna, Austria},
	type = {Master's {Thesis}},
	title = {Visibility {Histograms} in {Direct} {Volume} {Rendering}},
	url = {http://www.cg.tuwien.ac.at/research/publications/2008/emsenhuber-2008-vhd/},
	abstract = {This thesis introduces visibility histograms as a method for analyzing volumetric datasets. These histograms show how much the data points within a 3D dataset that have the same scalar value influence the image which is created by rendering the dataset with a particular transfer function and from a particular viewing direction. These histograms can be used to gain insights into the internal structure of volumetric datasets, in particular information about occlusions. Furthermore, the possibility of automatically calculating transfer functions which generate a particular visibility histogram when applied to a dataset from a particular viewing direction is explored. Two methods which can be used to calculate a matching transfer function for a visibility histogram are explained, one of which is based on a genetic algorithm approach, while the other is an heuristic.},
	school = {Institute of Computer Graphics and Algorithms, Vienna University of Technology},
	author = {Emsenhuber, Gerlinde},
	month = nov,
	year = {2008},
	file = {emsenhuber-2008-vhd-paper.pdf:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\0m7eflhb.default\\zotero\\storage\\89MVZ5WT\\emsenhuber-2008-vhd-paper.pdf:application/pdf}
}

@article{laramee_using_2010,
	title = {Using {Visualization} to {Debug} {Visualization} {Software}},
	volume = {30},
	issn = {0272-1716},
	doi = {10.1109/MCG.2009.154},
	abstract = {This article provides useful strategies for debugging visualization software.The key to debugging visualization software is to exploit the strengths of computer graphics and visualization.},
	number = {6},
	journal = {IEEE Computer Graphics and Applications},
	author = {Laramee, R.S.},
	month = nov,
	year = {2010},
	keywords = {Application software, Computer bugs, computer graphics, Computer graphics, computer graphics applications, Computer science, computer visualization, computing methodologies, Data structures, data visualisation, Data visualization, graphics and multimedia, Guidelines, methodology and techniques, program debugging, Programming profession, program visualisation, Software algorithms, software debugging, Software debugging, visualization software},
	pages = {67--73},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\0m7eflhb.default\\zotero\\storage\\JAJ7KX6J\\Laramee - 2010 - Using Visualization to Debug Visualization Softwar.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\0m7eflhb.default\\zotero\\storage\\V2D83J3M\\Laramee - 2010 - Using Visualization to Debug Visualization Softwar.pdf:application/pdf;laramee09debuggingSlides.pdf:C\:\\Users\\JoeShengzhou\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\0m7eflhb.default\\zotero\\storage\\2W375RI7\\laramee09debuggingSlides.pdf:application/pdf}
}

@book{hadwiger_real-time_2006,
	author = {Hadwiger, Markus and Kniss, Joe M. and Rezk-salama, Christof and Weiskopf, Daniel and Engel, Klaus},
	title = {Real-time Volume Graphics},
	year = {2006},
	isbn = {1568812663},
	publisher = {A. K. Peters, Ltd.},
	address = {Natick, MA, USA}
} 

@article{duan_visual_2011,
	author={Lijuan Duan and Chunpeng Wu and Jun Miao and Bovik, A.C.}, 
	journal={Signal Processing Letters, IEEE}, 
	title={Visual Conspicuity Index: Spatial Dissimilarity, Distance, and Central Bias}, 
	year={2011}, 
	month={Nov}, 
	volume={18}, 
	number={11}, 
	pages={690-693}, 
	keywords={image processing;principal component analysis;Kullback-Leibler distance metric;central bias;color image datasets;image conspicuity index;image patches;image processing;receiver operator characteristics analysis;reduced dimensional principal component space;spatial dissimilarity;spatial distance;visual conspicuity index;Analytical models;Bars;Color;Computational modeling;Humans;Indexes;Visualization;central bias;conspicuity;dissimilarity;spatial distance;visual saliency}, 
	doi={10.1109/LSP.2011.2167752}, 
	ISSN={1070-9908}
}

@book{chong_introduction_2013,
	address = {Hoboken, New Jersey},
	edition = {Fourth edition},
	series = {Wiley series in discrete mathematics and optimization},
	title = {An introduction to optimization},
	isbn = {978-1-118-27901-4},
	publisher = {Wiley},
	author = {Chong, Edwin Kah Pin and Zak, Stanislaw H.},
	year = {2013},
	keywords = {Mathematical optimization},
	file = {An Introduction to Optimization-Wiley (2013).pdf:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\SEJED5PU\\An Introduction to Optimization-Wiley (2013).pdf:application/pdf}
}

@article{yuan_step-sizes_2008,
	title = {Step-sizes for the gradient method},
	volume = {42},
	url = {ftp://159.226.92.9/pub/yyx/papers/p0504.pdf},
	number = {2},
	urldate = {2015-10-10},
	journal = {AMS IP Studies in Advanced Mathematics},
	author = {Yuan, Ya-xiang},
	year = {2008},
	pages = {785},
	file = {Step-Sizes for the Gradient Method.pdf:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\59P9T9ZA\\Step-Sizes for the Gradient Method.pdf:application/pdf}
}

@article{zhou_gradient_2006,
	title = {Gradient {Methods} with {Adaptive} {Step}-{Sizes}},
	volume = {35},
	issn = {0926-6003, 1573-2894},
	url = {http://link.springer.com/article/10.1007/s10589-006-6446-0},
	doi = {10.1007/s10589-006-6446-0},
	language = {en},
	number = {1},
	urldate = {2015-10-10},
	journal = {Computational Optimization and Applications},
	author = {Zhou, Bin and Gao, Li and Dai, Yu-Hong},
	month = mar,
	year = {2006},
	keywords = {adaptive step-size, Barzilai-Borwein method, Convex and Discrete Geometry, Gradient method, linear system, Operations Research/Decision Theory, Operations Research, Mathematical Programming, Optimization, Statistics, general, superlinear behavior, trust-region approach},
	pages = {69--86},
	file = {Full Text PDF:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\AIRXQ833\\Zhou et al. - 2006 - Gradient Methods with Adaptive Step-Sizes.pdf:application/pdf;Snapshot:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\KNGKX8Q8\\10.html:text/html}
}

@article{vrahatis_class_2000,
	title = {A class of gradient unconstrained minimization algorithms with adaptive stepsize},
	volume = {114},
	issn = {0377-0427},
	url = {http://www.sciencedirect.com/science/article/pii/S0377042799002769},
	doi = {10.1016/S0377-0427(99)00276-9},
	abstract = {In this paper the development, convergence theory and numerical testing of a class of gradient unconstrained minimization algorithms with adaptive stepsize are presented. The proposed class comprises four algorithms: the first two incorporate techniques for the adaptation of a common stepsize for all coordinate directions and the other two allow an individual adaptive stepsize along each coordinate direction. All the algorithms are computationally efficient and possess interesting convergence properties utilizing estimates of the Lipschitz constant that are obtained without additional function or gradient evaluations. The algorithms have been implemented and tested on some well-known test cases as well as on real-life artificial neural network applications and the results have been very satisfactory.},
	number = {2},
	urldate = {2015-10-10},
	journal = {Journal of Computational and Applied Mathematics},
	author = {Vrahatis, M. N. and Androulakis, G. S. and Lambrinos, J. N. and Magoulas, G. D.},
	month = feb,
	year = {2000},
	keywords = {Armijo's method, Artificial neural network, Globally convergent method, Gradient method, Line search strategies, Lipschitz constant, Steepest descent, Training algorithm, unconstrained optimization},
	pages = {367--386},
	file = {1-s2.0-S0377042799002769-main.pdf:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\VU8UUQW5\\1-s2.0-S0377042799002769-main.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\DJ8IK2VN\\S0377042799002769.html:text/html}
}

@article{armijo_minimization_1966,
	title = {Minimization of functions having {Lipschitz} continuous first partial derivatives.},
	volume = {16},
	issn = {0030-8730},
	url = {http://projecteuclid.org/euclid.pjm/1102995080},
	abstract = {Project Euclid - mathematics and statistics online},
	number = {1},
	urldate = {2015-10-10},
	journal = {Pacific Journal of Mathematics},
	author = {Armijo, Larry},
	year = {1966},
	mrnumber = {MR0191071},
	zmnumber = {0202.46105},
	pages = {1--3},
	file = {pjm-v16-n1-p01-p.pdf:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\ZPZKZX8G\\pjm-v16-n1-p01-p.pdf:application/pdf;Snapshot:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\PFPJGPH3\\1102995080.html:text/html}
}

@techreport{shewchuk_introduction_1994,
	address = {Pittsburgh, PA, USA},
	title = {An {Introduction} to the {Conjugate} {Gradient} {Method} {Without} the {Agonizing} {Pain}},
	abstract = {The Conjugate Gradient Method is the most prominent iterative method for solving sparse systems of linear equations. Unfortunately, many textbook treatments of the topic are written so that even their own authors would be mystified, if they bothered to read their own writing. For this reason, an understanding of the method has been reserved for the elite brilliant few who have painstakingly decoded the mumblings of their forebears. Nevertheless, the Conjugate Gradient Method is a composite of simple, elegant ideas that almost anyone can understand. Of course, a reader as intelligent as yourself will learn them almost effortlessly. The idea of quadratic forms is introduced and used to derive the methods of Steepest Descent, Conjugate Directions, and Conjugate Gradients. Eigenvectors are explained and used to examine the convergence of the Jacobi Method, Steepest Descent, and Conjugate Gradients. Other topics include preconditioning and the nonlinear Conjugate Gradient Method. I have taken pains to make this article easy to read. Sixty-two illustrations are provided. Dense prose is avoided. Concepts are explained in several different ways. Most equations are coupled with an intuitive interpretation.},
	institution = {Carnegie Mellon University},
	author = {Shewchuk, Jonathan R},
	year = {1994},
	file = {painless-conjugate-gradient.pdf:C\:\\Users\\joe\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\z4ailp3m.default\\zotero\\storage\\4RN9S5ID\\painless-conjugate-gradient.pdf:application/pdf}
}

@article{jung_visibility-driven_2013,
	title = {Visibility-driven {PET}-{CT} visualisation with region of interest ({ROI}) segmentation},
	volume = {29},
	issn = {0178-2789, 1432-2315},
	url = {http://link.springer.com/article/10.1007/s00371-013-0833-1},
	doi = {10.1007/s00371-013-0833-1},
	abstract = {Multi-modality (MM) positron emission tomography-computed tomography (PET-CT) visualises biological and physiological functions (from PET) as region of interests (ROIs) within a higher resolution anatomical reference frame (from CT). The need to efficiently assess and assimilate the information from these co-aligned volumes simultaneously has stimulated new visualisation techniques that combine 3D volume rendering with interactive transfer functions to enable efficient manipulation of these volumes. However, in typical MM volume rendering visualisation, the transfer functions for the volumes are manipulated in isolation with the resulting volumes being fused, thus failing to exploit the spatial correlation that exists between the aligned volumes. Such lack of feedback makes MM transfer function manipulation complex and time consuming. Further, transfer function alone is often insufficient to select the ROIs when they have similar voxel properties to those of non-relevant regions. In this study, we propose a new ROI-based MM visibility-driven transfer function (m 2-vtf) for PET-CT visualisation. We present a novel ‘visibility’ metric, a fundamental optical property that represents how much of the ROIs are visible to the users, and use it to measure the visibility of the ROIs in PET in relation to how it is affected by transfer function manipulations to its counterpart CT. To overcome the difficulty in ROI selection, we provide an intuitive ROI selection tool based on automated PET segmentation. We further present a MM transfer function automation where the visibility metrics from the PET ROIs are used to automate its CT’s transfer function. Our GPU implementation achieved an interactive visualisation of PET-CT with efficient and intuitive transfer function manipulations.},
	language = {en},
	number = {6-8},
	urldate = {2013-07-25},
	journal = {The Visual Computer},
	author = {Jung, Younhyun and Kim, Jinman and Eberl, Stefan and Fulham, Micheal and Feng, David Dagan},
	month = jun,
	year = {2013},
	keywords = {Artificial Intelligence (incl. Robotics), Computer graphics, Computer Science, general, Image Processing and Computer Vision, image segmentation, Multi-modality volume rendering, PET-CT imaging, transfer function, Visibility histogram},
	pages = {805--815}
}

@inproceedings{zheng_visibility_2013,
	title = {Visibility guided multimodal volume visualization},
	doi = {10.1109/BIBM.2013.6732506},
	abstract = {With the advances in dual medical imaging, the requirements for multimodal and multifield volume visualization begin to emerge. One of the challenges in multimodal visualization is how to simplify the process of generating informative pictures from complementary data. In this paper we present an automatic technique that makes use of dual modality information, such as CT and PET, to produce effective focus+context volume visualization. With volume ray casting, per-ray visibility histograms summarize the contribution of samples along each ray to the final image. By quantifying visibility for the region of interest, indicated by the PET data, occluding tissues can be made just transparent enough to give a clear view of the features in that region while preserving some context. Unlike most previous methods relying on costly-preprocessing and tedious manual tuning, our technique achieves comparable and better results based on on-the-fly processing that still enables interactive visualization. Our work thus offers a powerful visualization technique for examining multimodal volume data. We demonstrate the technique with scenarios for the detection and diagnosis of cancer and other pathologies.},
	booktitle = {2013 {IEEE} {International} {Conference} on {Bioinformatics} and {Biomedicine} ({BIBM})},
	author = {Zheng, Lin and Correa, C. and Ma, Kwan-Liu},
	month = dec,
	year = {2013},
	keywords = {cancer, Cancer detection, cancer diagnosis, Computed tomography, Context, CT, data visualisation, data visualization, dual modality information, Histograms, interactive systems, Interactive visualization, medical image processing, on-the-fly processing, per-ray visibility histograms, PET, Positron emission tomography, rendering (computer graphics), Transfer functions, visibility guided multimodal volume visualization, volume ray casting},
	pages = {297--304},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\V2JN3H3G\\articleDetails.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\SSSP3RH9\\Zheng et al. - 2013 - Visibility guided multimodal volume visualization.pdf:application/pdf}
}

@incollection{healey_combining_2001,
	address = {New York},
	series = {{SIGGRAPH} 2001 {Course} {Notes}},
	title = {Combining {Perception} and {Impressionist} {Techniques} for {Nonphotorealistic} {Visualization} of {Multidimensional} {Data}},
	booktitle = {Non-{Photorelistic} {Rendering} in {Scientific} {Visualization}},
	publisher = {ACM Press},
	author = {Healey, Christopher G.},
	editor = {Healey, Christopher G.},
	year = {2001},
	file = {sig-course.01.pdf:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\ANNP3S3Z\\sig-course.01.pdf:application/pdf}
}

@inproceedings{hall_trainable_2004,
	title = {A trainable low-level feature detector},
	volume = {1},
	doi = {10.1109/ICPR.2004.1334279},
	abstract = {We introduce a trainable system that simultaneously filters and classifies low-level features into types specified by the user. The system operates over full colour images, and outputs a vector at each pixel indicating the probability that the pixel belongs to each feature type. We explain how common features such as edge, corner, and ridge can all be detected within a single framework, and how we combine these detectors using simple probability theory. We show its efficacy, using stereo-matching as an example.},
	booktitle = {Proceedings of the 17th {International} {Conference} on {Pattern} {Recognition}, 2004. {ICPR} 2004},
	author = {Hall, P. and Owen, M. and Collomosse, J.},
	month = aug,
	year = {2004},
	keywords = {Artificial Intelligence, Computer science, Computer vision, Detectors, feature extraction, Filters, Humans, image classification, image colour analysis, Image edge detection, image matching, Learning systems, Particle measurements, Pixel, probability, probability theory, sampling methods, Stereo image processing, stereo matching, trainable low level feature detector},
	pages = {708--711 Vol.1},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\DKQ9JKJ7\\abs_all.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\9CKID2M5\\Hall et al. - 2004 - A trainable low-level feature detector.pdf:application/pdf}
}

@inproceedings{carmi_causal_2006,
	address = {New York, NY, USA},
	series = {{ETRA} '06},
	title = {Causal {Saliency} {Effects} {During} {Natural} {Vision}},
	isbn = {1-59593-305-0},
	url = {http://doi.acm.org/10.1145/1117309.1117313},
	doi = {10.1145/1117309.1117313},
	abstract = {Salient stimuli, such as color or motion contrasts, attract human attention, thus providing a fast heuristic for focusing limited neural resources on behaviorally relevant sensory inputs. Here we address the following questions: What types of saliency attract attention and how do they compare to each other during natural vision? We asked human participants to inspect scene-shuffled video clips, tracked their instantaneous eye-position, and quantified how well a battery of computational saliency models predicted overt attentional selections (saccades). Saliency effects were measured as a function of total viewing time, proximity to abrupt scene transitions (jump cuts), and inter-participant consistency. All saliency models predicted overall attentional selection well above chance, with dynamic models being equally predictive to each other, and up to 3.6 times more predictive than static models. The prediction accuracy of all dynamic models was twice higher than their average for saccades that were initiated immediately after jump cuts, and led to maximal inter-participant consistency. Static models showed mixed results in these circumstances, with some models having weaker prediction accuracy than their average. These results demonstrate that dynamic visual cues play a dominant causal role in attracting attention, while static visual cues correlate with attentional selection mostly due to top-down causes.},
	urldate = {2015-10-17},
	booktitle = {Proceedings of the 2006 {Symposium} on {Eye} {Tracking} {Research} \& {Applications}},
	publisher = {ACM},
	author = {Carmi, Ran and Itti, Laurent},
	year = {2006},
	keywords = {Attention, Computational modeling, eye-movements, natural vision, Saliency},
	pages = {11--18},
	file = {ACM Full Text PDF:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\Z9Q8J7R9\\Carmi and Itti - 2006 - Causal Saliency Effects During Natural Vision.pdf:application/pdf}
}

@article{treue_visual_2003,
	title = {Visual attention: the where, what, how and why of saliency},
	volume = {13},
	issn = {0959-4388},
	shorttitle = {Visual attention},
	url = {http://www.sciencedirect.com/science/article/pii/S0959438803001053},
	doi = {10.1016/S0959-4388(03)00105-3},
	abstract = {Attention influences the processing of visual information even in the earliest areas of primate visual cortex. There is converging evidence that the interaction of bottom-up sensory information and top-down attentional influences creates an integrated saliency map, that is, a topographic representation of relative stimulus strength and behavioral relevance across visual space. This map appears to be distributed across areas of the visual cortex, and is closely linked to the oculomotor system that controls eye movements and orients the gaze to locations in the visual scene characterized by a high salience.},
	number = {4},
	urldate = {2015-10-17},
	journal = {Current Opinion in Neurobiology},
	author = {Treue, Stefan},
	month = aug,
	year = {2003},
	pages = {428--432},
	file = {ScienceDirect Snapshot:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\72EHCRET\\S0959438803001053.html:text/html}
}

@article{kadir_saliency_2001,
	title = {Saliency, {Scale} and {Image} {Description}},
	volume = {45},
	issn = {0920-5691, 1573-1405},
	url = {http://link.springer.com/article/10.1023/A%3A1012460413855},
	doi = {10.1023/A:1012460413855},
	abstract = {Many computer vision problems can be considered to consist of two main tasks: the extraction of image content descriptions and their subsequent matching. The appropriate choice of type and level of description is of course task dependent, yet it is generally accepted that the low-level or so called early vision layers in the Human Visual System are context independent. This paper concentrates on the use of low-level approaches for solving computer vision problems and discusses three inter-related aspects of this: saliency; scale selection and content description. In contrast to many previous approaches which separate these tasks, we argue that these three aspects are intrinsically related. Based on this observation, a multiscale algorithm for the selection of salient regions of an image is introduced and its application to matching type problems such as tracking, object recognition and image retrieval is demonstrated.},
	language = {en},
	number = {2},
	urldate = {2015-10-17},
	journal = {International Journal of Computer Vision},
	author = {Kadir, Timor and Brady, Michael},
	month = nov,
	year = {2001},
	keywords = {Artificial Intelligence (incl. Robotics), Automation and Robotics, Computer Imaging, Graphics and Computer Vision, entropy, feature extraction, image content descriptors, image database, Image processing, salient features, scale selection, scale-space, visual saliency},
	pages = {83--105},
	file = {Full Text PDF:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\MMKMVC7H\\Kadir and Brady - 2001 - Saliency, Scale and Image Description.pdf:application/pdf;Snapshot:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\VGUSPRIW\\10.html:text/html}
}

@article{lathen_automatic_2012,
	title = {Automatic {Tuning} of {Spatially} {Varying} {Transfer} {Functions} for {Blood} {Vessel} {Visualization}},
	volume = {18},
	issn = {1077-2626},
	doi = {10.1109/TVCG.2012.203},
	abstract = {Computed Tomography Angiography (CTA) is commonly used in clinical routine for diagnosing vascular diseases. The procedure involves the injection of a contrast agent into the blood stream to increase the contrast between the blood vessels and the surrounding tissue in the image data. CTA is often visualized with Direct Volume Rendering (DVR) where the enhanced image contrast is important for the construction of Transfer Functions (TFs). For increased efficiency, clinical routine heavily relies on preset TFs to simplify the creation of such visualizations for a physician. In practice, however, TF presets often do not yield optimal images due to variations in mixture concentration of contrast agent in the blood stream. In this paper we propose an automatic, optimization-based method that shifts TF presets to account for general deviations and local variations of the intensity of contrast enhanced blood vessels. Some of the advantages of this method are the following. It computationally automates large parts of a process that is currently performed manually. It performs the TF shift locally and can thus optimize larger portions of the image than is possible with manual interaction. The method is based on a well known vesselness descriptor in the definition of the optimization criterion. The performance of the method is illustrated by clinically relevant CT angiography datasets displaying both improved structural overviews of vessel trees and improved adaption to local variations of contrast concentration.},
	number = {12},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Lathen, G. and Lindholm, S. and Lenz, R. and Persson, A. and Borga, M.},
	month = dec,
	year = {2012},
	keywords = {automatic tuning, Biomedical imaging, blood stream, blood vessels, blood vessel visualization, clinical routine, computed tomography angiography, computerised tomography, CT angiography datasets, data visualisation, data visualization, direct volume rendering, enhanced image contrast, image data, medical image processing, mixture concentration, optimisation, Optimization, optimization criterion, rendering (computer graphics), Rendering (computer graphics), spatially varying transfer functions, Transfer functions, transfer functions, vascular diseases, vesselness descriptor, vessel visualization},
	pages = {2345--2354},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\ZQ7B7B8R\\articleDetails.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\D2KMVV8H\\Lathen et al. - 2012 - Automatic Tuning of Spatially Varying Transfer Fun.pdf:application/pdf}
}

@inproceedings{peng_optimal_2011,
	title = {An {Optimal} {Color} {Mapping} {Strategy} {Based} on {Energy} {Minimization} for {Time}-{Varying} {Data}},
	doi = {10.1109/CAD/Graphics.2011.68},
	abstract = {Color mapping plays a critical role in visualization of time-varying data and also sets a challenge for researchers due to the consistency of mapping and great changes in time-varying data. In order to solve this problem and generate feature-prominent animation, we present a two phase optimization technique using bilateral filtering and global energy functions. In the first phase, for each time step, we use a weighted mapping function which combines linear mapping and feature histogram. In the second phase, an optimization function taking global color mapping and minimum color difference into consideration is designed. So users can clearly distinguish between data in variable time steps and easily understand the corresponding relationships between different structures. The experiments' results show that our method can achieve high quality visualization for both static and time-varying data.},
	booktitle = {2011 12th {International} {Conference} on {Computer}-{Aided} {Design} and {Computer} {Graphics} ({CAD}/{Graphics})},
	author = {Peng, Yi and Dong, Jie and Chen, Li and Chu, Haiyang and Yong, Junhai},
	year = {2011},
	keywords = {bilateral filtering, computer animation, Correlation, data visualisation, data visualization, energy function, energy minimization, feature extraction, Feature extraction, feature histogram, feature-prominent animation, Histograms, Image color analysis, image colour analysis, linear mapping, mapping consistency, Minimization, optimal color mapping, optimal color mapping strategy, optimisation, Optimization, Time-varying data, time-varying data visualization, Transfer functions, transfer functions, two-phase optimization technique, Volume rendering, volume rendering, weighted mapping function},
	pages = {411--417},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\HSJHD9MU\\abs_all.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\5X74SAG7\\Peng et al. - 2011 - An Optimal Color Mapping Strategy Based on Energy .pdf:application/pdf}
}

@inproceedings{pekar_fast_2001,
	title = {Fast detection of meaningful isosurfaces for volume data visualization},
	doi = {10.1109/VISUAL.2001.964515},
	abstract = {Automatic detection of meaningful isosurfaces is important for producing informative visualizations of volume data, especially when no information about the data origin and imaging protocol is available. We propose a computationally efficient method for the automated detection of intensity transitions in volume data. In this approach, the dominant transitions correspond to clear maxima in cumulative Laplacian-weighted gray value histograms. Only one pass through the data volume is required to compute the histogram. Several other features which may be useful for exploration of data of unknown origin can be efficiently computed in a similar manner. The detected intensity transitions can be used for setting of visualization parameters for surface rendering, as well as for direct volume rendering of 3D datasets. When using surface rendering, the detected dominant intensity transition values correspond to the optimal surface isovalues for extraction of boundaries of the objects of interest. In direct volume rendering, such transitions are important for generation of the transfer functions, which are used to assign visualization properties to data voxels and determine the appearance of the rendered image. The proposed method is illustrated by examples with synthetic data as well as real biomedical datasets.},
	booktitle = {Visualization, 2001. {VIS} '01. {Proceedings}},
	author = {Pekar, V. and Wiemker, R. and Hempel, D.},
	month = oct,
	year = {2001},
	keywords = {Biomedical Datasets, Biomedical imaging, Computational complexity, computationally efficient method, Computed tomography, Computer Graphics, cumulative Laplacian-weighted gray value histograms, data visualisation, data visualization, data voxels, direct volume rendering, dominant transitions, Histograms, intensity transitions, Isosurfaces, magnetic resonance imaging, Magnetic resonance imaging, meaningful isosurfaces, optimisation, Protocols, rendering (computer graphics), Rendering (computer graphics), surface rendering, transfer function generation, Transfer functions, volume data visualization},
	pages = {223--230},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\WHIIPAEW\\articleDetails.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\WV2S2DRV\\Pekar et al. - 2001 - Fast detection of meaningful isosurfaces for volum.pdf:application/pdf}
}

@inproceedings{bronstad_visibility_2012,
	title = {Visibility driven visualization of 3D cardiac ultrasound data on the {GPU}},
	doi = {10.1109/ULTSYM.2012.0664},
	abstract = {Direct volume rendering (DVR) has become a widely used technique for visualizing anatomical structures in medical 3D datasets The aim of this study was to locally adapt the opacity transfer function (OTF) in order to improve the results achieved when rendering 3D echocardiographic datasets using DVR. A novel approach for defining locally adaptive OTFs has been tested and adapted to echo data and implemented on the GPU. The local OTF is modeled as a truncated second order polynomial. The algorithm locates significant transitions along the ray profile (feature detection along the ray) in order to estimate an opacity threshold (below which all values are considered transparent) and the steepness of the polynomial for each ray. A reference global OTF and the locally adaptive algorithm have been implemented on a GPU using OpenCL and tested on a dataset of nine 3D echo recordings. The rendering resolution is 512Ã—512Ã—300, while average timing is 28ms, 104ms for the reference and the new method respectively. The locally adaptive OTFs were able to compensate for high variations in tissue (and such reducing wall drop-outs) and blood pool signal (reducing spurious structures inside the cavity). The method depends on a number of user defined parameters, determining these values robustly is subject of ongoing research.},
	booktitle = {Ultrasonics {Symposium} ({IUS}), 2012 {IEEE} {International}},
	author = {Bronstad, E.S. and Asen, J.P. and Torp, H.G. and Kiss, G.},
	month = oct,
	year = {2012},
	keywords = {3D cardiac ultrasound data, 3D echocardiographic datasets, 3D echo recordings, anatomical structures, biological tissues, Blood, blood pool signal, data visualisation, direct volume rendering, DVR, Echocardiography, echo data, feature detection, feature extraction, Feature extraction, GPU, graphics processing units, Graphics processing units, Histograms, locally adaptive algorithm, medical 3D datasets, medical image processing, opacity threshold, opacity transfer function, OpenCL, polynomial steepness, ray profile, reference global OTF, rendering (computer graphics), Rendering (computer graphics), tissue, Transfer functions, transfer functions, truncated second order polynomial, Ultrasonic imaging, visibility driven visualization},
	pages = {2651--2654},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\DXPSNS37\\articleDetails.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\ZJTH9RTE\\Bronstad et al. - 2012 - Visibility driven visualization of 3D cardiac ultr.pdf:application/pdf}
}

@article{jani_opacity_2005,
	title = {Opacity transfer function optimization for volume-rendered computed tomography images of the prostate},
	volume = {12},
	issn = {1076-6332},
	doi = {10.1016/j.acra.2005.03.054},
	abstract = {RATIONALE AND OBJECTIVES: The selection of an opacity transfer function is essential for volume visualization. Computed tomography (CT) scans of the pelvis were used to determine an optimal opacity transfer function for use in radiotherapy.
MATERIALS AND METHODS: On sample datasets (a mathematical phantom and a patient pelvis CT scan), standard viewing orientations were selected to render the prostate. Opacity functions were selected via (1) trapezoidal manual selection, (2) trapezoidal semiautomatic selection, and (3) histogram volume-based selection. Using an established metric, the errors using each of these methods were computed.
RESULTS: Trapezoidal manual opacity function optimization resulted in visually acceptable images, but the errors were considerable (6.3-9.1 voxel units). These errors could be reduced with the use of trapezoidal semiautomatic selection (4.9-6.2 voxel units) or with histogram volume-based selection (4.8-7.9 voxel units). As each visualization algorithm focused on enhancing the boundary of the prostate using a different approach, the scene information was considerably different using the three techniques.
CONCLUSION: Improved volume visualization of soft tissue interfaces was achieved using automated optimal opacity function determination, compared with manual selection.},
	language = {eng},
	number = {6},
	journal = {Academic Radiology},
	author = {Jani, Ashesh B. and Irick, John-Stockton and Pelizzari, Charles},
	month = jun,
	year = {2005},
	pmid = {15935974},
	keywords = {Algorithms, Humans, Image Processing, Computer-Assisted, Male, Phantoms, Imaging, Prostatic Neoplasms, Radiotherapy Planning, Computer-Assisted, Software, Tomography, X-Ray Computed},
	pages = {761--770}
}

@inproceedings{tzeng_novel_2003,
	address = {Washington, DC, USA},
	series = {{VIS} '03},
	title = {A {Novel} {Interface} for {Higher}-{Dimensional} {Classification} of {Volume} {Data}},
	isbn = {0-7695-2030-8},
	url = {http://dx.doi.org/10.1109/VISUAL.2003.1250413},
	doi = {10.1109/VISUAL.2003.1250413},
	abstract = {In the traditional volume visualization paradigm, the user specifies a transfer function that assigns each scalar value to a color and opacity by defining an opacity and a color map function.The transfer function has two limitations.First, the user must define curves based on histogram and value rather than seeing and working with the volume itself.Second, the transfer function is inflexiblein classifying regions of interest, where values at a voxel such as intensity and gradient are used to differentiate material, not taking into account additional properties such as texture and position. We describe an intuitive user interface for specifying the classification functions that consists of the users painting directly on sample slices of the volume.These painted regions are used to automatically define high-dimensional classification functions that can be implemented in hardware for interactive rendering.The classification of the volume is iteratively improved as the user paints samples, allowing intuitive and efficient viewing of materials of interest.},
	urldate = {2013-05-01},
	booktitle = {Proceedings of the 14th {IEEE} {Visualization} 2003 ({VIS}'03)},
	publisher = {IEEE Computer Society},
	author = {Tzeng, Fan-Yin and Lum, Eric B. and Ma, Kwan-Liu},
	year = {2003},
	keywords = {Artificial neural network, Classification, classification, classification function, color map function, data visualization, graphics hardware, image segmentation, interactive rendering, Interactive visualization, knowledge acquisition, multidimensional transfer function, neural nets, neural network, opacity map function, realistic images, rendering (computer graphics), Transfer functions, User Interface, user interface design, User interfaces, Volume data, volume visualization, voxel},
	pages = {66--},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\QJ56774N\\Tzeng et al. - 2003 - A novel interface for higher-dimensional classific.pdf:application/pdf}
}

@inproceedings{tzeng_cluster-space_2004,
	address = {Aire-la-Ville, Switzerland, Switzerland},
	series = {{VISSYM}'04},
	title = {A cluster-space visual interface for arbitrary dimensional classification of volume data},
	isbn = {3-905673-07-X},
	url = {http://dx.doi.org/10.2312/VisSym/VisSym04/017-024},
	doi = {10.2312/VisSym/VisSym04/017-024},
	abstract = {In volume visualization, users typically specify transfer functions to classify the data and assign visual attributes to each material class. Higher-dimensional classification makes it easier to differentiate material classes since more data properties are considered. One of the difficulties in using higher-dimensional classification is the absence of appropriate user interfaces. We introduce an intuitive user interface that allows the user to work in the cluster space, which shows the material classes with a set of material widgets, rather than work in the transfer function space. This interface not only provides the user the capability to specify arbitrary-dimensional transfer functions, but also allows the user to operate directly on the classification and visualization results.},
	urldate = {2013-04-30},
	booktitle = {Proceedings of the {Sixth} {Joint} {Eurographics} - {IEEE} {TCVG} conference on {Visualization}},
	publisher = {Eurographics Association},
	author = {Tzeng, Fan-Yin and Ma, Kwan-Liu},
	year = {2004},
	pages = {17--24},
	file = {vissym04b.pdf:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\N6CC2H57\\vissym04b.pdf:application/pdf}
}

@inproceedings{he_generation_1996,
	title = {Generation of transfer functions with stochastic search techniques},
	doi = {10.1109/VISUAL.1996.568113},
	abstract = {This paper presents a novel approach to assist the user in exploring appropriate transfer functions for the visualization of volumetric datasets. The search for a transfer function is treated as a parameter optimization problem and addressed with stochastic search techniques. Starting from an initial population of (random or pre-defined) transfer functions, the evolution of the stochastic algorithms is controlled by either direct user selection of intermediate images or automatic fitness evaluation using user-specified objective functions. This approach essentially shields the user from the complex and tedious "trial and error" approach, and demonstrates effective and convenient generation of transfer functions.},
	booktitle = {Visualization '96. {Proceedings}.},
	author = {He, Taosong and Hong, Lichan and Kaufman, A. and Pfister, H.},
	month = oct,
	year = {1996},
	keywords = {Automatic control, automatic fitness evaluation, Biomedical Engineering, Computer science, Data engineering, data visualization, Medical simulation, parameter optimization, rendering, rendering (computer graphics), stochastic algorithms, Stochastic processes, stochastic search techniques, transfer function generation, Transfer functions, trial and error approach, User Interface, user selection, user-specified objective functions, volumetric dataset visualization},
	pages = {227--234},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\7QH2B74V\\articleDetails.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\GWM6ETVK\\He et al. - 1996 - Generation of transfer functions with stochastic s.pdf:application/pdf}
}

@book{zudilova-seinstra_trends_2008,
	edition = {1},
	title = {Trends in {Interactive} {Visualization}: {State}-of-the-{Art} {Survey}},
	isbn = {1-84800-268-8 978-1-84800-268-5},
	shorttitle = {Trends in {Interactive} {Visualization}},
	abstract = {The purpose of Interactive Visualization is to develop new scientific methods to increase scientists abilities to explore data and to understand better the results of experiments based on extensive calculations. These techniques not only provide users with a possibility to view the data but also permit them to use interaction capabilities to interrogate and navigate through datasets and communicate these insights. This book is a unique multi-disciplinary collection of scientific articles, which provides readers with insight in Interactive Visualization from various perspectives, representing the state-of-the-art with the special emphasis on: Advanced visualization algorithms and methods, Interactive data exploration, Display systems and interaction techniques, Multi-modal and collaborative visualization, Design and evaluation of interactive visualization tools and systems, Various application topics.},
	publisher = {Springer Publishing Company, Incorporated},
	author = {Zudilova-Seinstra, Elena and Adriaansen, Tony and Liere, Robert van},
	year = {2008}
}

@book{preim_visual_2013,
	address = {San Francisco, CA, USA},
	edition = {2nd},
	title = {Visual {Computing} for {Medicine}, {Second} {Edition}: {Theory}, {Algorithms}, and {Applications}},
	isbn = {0-12-415873-0 978-0-12-415873-3},
	shorttitle = {Visual {Computing} for {Medicine}, {Second} {Edition}},
	abstract = {Visual Computing for Medicine, Second Edition, offers cutting-edge visualization techniques and their applications in medical diagnosis, education, and treatment. The book includesalgorithms, applications, and ideas on achieving reliability of results and clinical evaluation of the techniques covered. Preim and Botha illustrate visualization techniques fromresearch, but also cover the information required to solve practical clinical problems. They base the book on several years of combined teaching and research experience. This new edition includes six new chapters on treatment planning, guidance and training; an updated appendix on software support for visual computing for medicine; and a new global structure that better classifies and explains the major lines of work in the field. Complete guide to visual computing in medicine, fully revamped and updated with new developments in the fieldIllustrated in full colorIncludes a companion website offering additional content for professors, source code, algorithms, tutorials, videos, exercises, lessons, and more},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Preim, Bernhard and Botha, Charl P.},
	year = {2013}
}

@inproceedings{tzeng_intelligent_2005,
	address = {Washington, DC, USA},
	series = {{SC} '05},
	title = {Intelligent {Feature} {Extraction} and {Tracking} for {Visualizing} {Large}-{Scale} 4D {Flow} {Simulations}},
	isbn = {1-59593-061-2},
	url = {http://dx.doi.org/10.1109/SC.2005.37},
	doi = {10.1109/SC.2005.37},
	abstract = {Terascale simulations produce data that is vast in spatial, temporal, and variable domains, creating a formidable challenge for subsequent analysis. Feature extraction as a data reduction method offers a viable solution to this large data problem. This paper presents a new approach to the problem of extracting and visualizing 4D features within large volume data. Conventional methods requires either an analytical description of the feature of interest or tedious manual intervention throughout the feature extraction and tracking process. We show that it is possible for a visualization system to "learn" to extract and track features in complex 4D flow field according to their "visual" properties, location, shape, and size. The basic approach is to employ machine learning in the process of visualization. Such an intelligent system approach is powerful because it allows us to extract and track an feature of interest in a high-dimensional space without explicitly specifying the relations between those dimensions, resulting in a greatly simplified and intuitive visualization interface.},
	urldate = {2015-10-19},
	booktitle = {Proceedings of the 2005 {ACM}/{IEEE} {Conference} on {Supercomputing}},
	publisher = {IEEE Computer Society},
	author = {Tzeng, Fan-Yin and Ma, Kwan-Liu},
	year = {2005},
	pages = {6--},
	file = {ACM Full Text PDF:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\W5U96JWJ\\Tzeng and Ma - 2005 - Intelligent Feature Extraction and Tracking for Vi.pdf:application/pdf}
}

@article{ward_visual_2011,
	title = {Visual {Exploration} of {Time}-{Series} {Data} with {Shape} {Space} {Projections}},
	volume = {30},
	copyright = {Â© 2011 The Author(s) Journal compilation Â© 2011 The Eurographics Association and Blackwell Publishing Ltd.},
	issn = {1467-8659},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1467-8659.2011.01919.x/abstract},
	doi = {10.1111/j.1467-8659.2011.01919.x},
	abstract = {Time-series data is a common target for visual analytics, as they appear in a wide range of application domains. Typical tasks in analyzing time-series data include identifying cyclic behavior, outliers, trends, and periods of time that share distinctive shape characteristics. Many methods for visualizing time series data exist, generally mapping the data values to positions or colors. While each can be used to perform a subset of the above tasks, none to date is a complete solution. In this paper we present a novel approach to time-series data visualization, namely creating multivariate data records out of short subsequences of the data and then using multivariate visualization methods to display and explore the data in the resulting shape space. We borrow ideas from text analysis, where the use of N-grams is a common approach to decomposing and processing unstructured text. By mapping each temporal N-gram to a glyph, and then positioning the glyphs via PCA (basically a projection in shape space), many different kinds of patterns in the sequence can be readily identified. Interactive selection via brushing, in conjunction with linking to other visualizations, provides a wide range of tools for exploring the data. We validate the usefulness of this approach with examples from several application domains and tasks, comparing our methods with traditional time-series visualizations.},
	language = {en},
	number = {3},
	urldate = {2015-02-27},
	journal = {Computer Graphics Forum},
	author = {Ward, Matthew O. and Guo, Zhenyu},
	month = jun,
	year = {2011},
	keywords = {graphical user interfaces, Information Interfaces and Presentation [H.5.2]: User Interfaces},
	pages = {701--710},
	file = {Full Text PDF:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\U6HV2AU3\\Ward and Guo - 2011 - Visual Exploration of Time-Series Data with Shape .pdf:application/pdf;Snapshot:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\AQIV4JTR\\abstract.html:text/html}
}

@article{jang_time-varying_2012,
	title = {Time-{Varying} {Data} {Visualization} {Using} {Functional} {Representations}},
	volume = {18},
	issn = {1077-2626},
	doi = {10.1109/TVCG.2011.54},
	abstract = {In many scientific simulations, the temporal variation and analysis of features are important. Visualization and visual analysis of time series data is still a significant challenge because of the large volume of data. Irregular and scattered time series data sets are even more problematic to visualize interactively. Previous work proposed functional representation using basis functions as one solution for interactively visualizing scattered data by harnessing the power of modern PC graphics boards. In this paper, we use the functional representation approach for time-varying data sets and develop an efficient encoding technique utilizing temporal similarity between time steps. Our system utilizes a graduated approach of three methods with increasing time complexity based on the lack of similarity of the evolving data sets. Using this system, we are able to enhance the encoding performance for the time-varying data sets, reduce the data storage by saving only changed or additional basis functions over time, and interactively visualize the time-varying encoding results. Moreover, we present efficient rendering of the functional representations using binary space partitioning tree textures to increase the rendering performance.},
	number = {3},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Jang, Yun and Ebert, D.S. and Gaither, K.},
	month = mar,
	year = {2012},
	keywords = {Basis functions, data storage, data visualisation, data visualization, encoding, encoding technique, Equations, feature extraction, Feature extraction, functional representation, functional representations, octrees, PC graphics boards, rendering (computer graphics), Rendering (computer graphics), scientific simulations, temporal variation, time complexity, time series, Time-varying data, time varying data visualization, Time varying systems, visual analysis, volume rendering.},
	pages = {421 --433},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\P35RMR5K\\articleDetails.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\UR8KNUDS\\Jang et al. - 2012 - Time-Varying Data Visualization Using Functional R.pdf:application/pdf}
}

@article{woo_feature-driven_2012,
	title = {Feature-driven data exploration for volumetric rendering},
	volume = {18},
	issn = {1941-0506},
	doi = {10.1109/TVCG.2012.24},
	abstract = {We have developed an intuitive method to semiautomatically explore volumetric data in a focus-region-guided or value-driven way using a user-defined ray through the 3D volume and contour lines in the region of interest. After selecting a point of interest from a 2D perspective, which defines a ray through the 3D volume, our method provides analytical tools to assist in narrowing the region of interest to a desired set of features. Feature layers are identified in a 1D scalar value profile with the ray and are used to define default rendering parameters, such as color and opacity mappings, and locate the center of the region of interest. Contour lines are generated based on the feature layer level sets within interactively selected slices of the focus region. Finally, we utilize feature-preserving filters and demonstrate the applicability of our scheme to noisy data.},
	language = {eng},
	number = {10},
	journal = {IEEE transactions on visualization and computer graphics},
	author = {Woo, Insoo and Maciejewski, Ross and Gaither, Kelly P and Ebert, David S},
	month = oct,
	year = {2012},
	pmid = {22291153},
	keywords = {Algorithms, Computer Graphics, Computer Simulation, data visualization, Diagnostic Imaging, direct volume rendering, feature extraction, Feature extraction, focus+context visualization., Histograms, Humans, Image color analysis, Image Processing, Computer-Assisted, Noise measurement, rendering (computer graphics), Rendering (computer graphics), Tornadoes, transfer function, Transfer functions, transfer functions},
	pages = {1731--1743},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\JBITCCFU\\articleDetails.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\B37TB6F5\\Woo et al. - 2012 - Feature-Driven Data Exploration for Volumetric Ren.pdf:application/pdf}
}

@inproceedings{zhou_opacity_2014,
	title = {Opacity modulation based peeling for direct volume rendering},
	doi = {10.1109/ICIEA.2014.6931339},
	abstract = {Direct volume rendering has proven as a powerful tool for volume visualization. However, it is still challenging to achieve informative 2D images to convey the internal structures of 3D volume space. In this paper, we present a novel volume visualization technique with the opacity modulation based peeling. A parameter is calculated according to the intensity difference of local maximum and current samples whenever a new maximum is encountered during the ray casting, that is employed to weight the accumulated color and opacity values to highlight the current sampling structure. In order to further display more internal structures, we divide the intensity values along each viewing ray into different segmentations, while the accumulated modulation opacity is approaching to overflow. Inspired by traditional opacity peeling and maximum intensity difference accumulation, the proposed opacity modulation based peeling method cannot only provide the domain experts with different layered images for more internal information, but also display structures of interest occluded by conventional direct volume rendering in each layer. Furthermore, such an effective algorithm can be easily implemented based on volume ray casting with current graphics hardware and do have some application values.},
	booktitle = {2014 {IEEE} 9th {Conference} on {Industrial} {Electronics} and {Applications} ({ICIEA})},
	author = {Zhou, Zhiguang and Wang, Guofeng and Huang, Chaogeng and Lin, Hai},
	month = jun,
	year = {2014},
	keywords = {3D volume space, accumulated modulation opacity, Casting, color values, data visualisation, data visualization, direct volume rendering, graphics hardware, informative 2D images, intensity values, internal information, internal structures, layered images, local maximum, Mathematical model, maximum intensity difference accumulation, Modulation, opacity, opacity modulation-based peeling method, opacity values, Optical imaging, ray casting, rendering (computer graphics), sampling structure, Transfer functions, viewing ray, volume ray casting, volume visualization technique},
	pages = {1152--1157},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\5867NS5G\\articleDetails.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\79GKIHZM\\Zhou et al. - 2014 - Opacity modulation based peeling for direct volume.pdf:application/pdf}
}

@article{khlebnikov_noise-based_2013,
	title = {Noise-{Based} {Volume} {Rendering} for the {Visualization} of {Multivariate} {Volumetric} {Data}},
	volume = {19},
	issn = {1077-2626},
	doi = {10.1109/TVCG.2013.180},
	abstract = {Analysis of multivariate data is of great importance in many scientific disciplines. However, visualization of 3D spatially-fixed multivariate volumetric data is a very challenging task. In this paper we present a method that allows simultaneous real-time visualization of multivariate data. We redistribute the opacity within a voxel to improve the readability of the color defined by a regular transfer function, and to maintain the see-through capabilities of volume rendering. We use predictable procedural noise - random-phase Gabor noise - to generate a high-frequency redistribution pattern and construct an opacity mapping function, which allows to partition the available space among the displayed data attributes. This mapping function is appropriately filtered to avoid aliasing, while maintaining transparent regions. We show the usefulness of our approach on various data sets and with different example applications. Furthermore, we evaluate our method by comparing it to other visualization techniques in a controlled user study. Overall, the results of our study indicate that users are much more accurate in determining exact data values with our novel 3D volume visualization method. Significantly lower error rates for reading data values and high subjective ranking of our method imply that it has a high chance of being adopted for the purpose of visualization of multivariate 3D data.},
	number = {12},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Khlebnikov, R. and Kainz, B. and Steinberger, M. and Schmalstieg, D.},
	year = {2013},
	keywords = {3D volume visualization method, Algorithms, aliasing avoidance, Artifacts, Colored noise, color readability improvement, Computer Graphics, computer graphics, data attributes, data visualisation, data visualization, error rates, filtering theory, high-frequency redistribution pattern, Image color analysis, image enhancement, Image Interpretation, Computer-Assisted, Imaging, Three-Dimensional, mapping function filtering, Multivariate Analysis, multi-variate data visualization, multivariate volumetric data visualization, multi-volume rendering, noise-based volume rendering, opacity mapping function, opacity redistribution, predictable procedural noise, random noise, random-phase Gabor noise, real-time systems, regular transfer function, rendering (computer graphics), Rendering (computer graphics), Reproducibility of Results, scientific visualization, Sensitivity and Specificity, signal-to-noise ratio, simultaneous real-time visualization, three-dimensional displays, Transfer functions, transfer functions, transparent region maintenance, User-Computer Interface, Volume rendering, volume rendering},
	pages = {2926--2935},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\8TXBVXDX\\articleDetails.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\5454UDZG\\Khlebnikov et al. - 2013 - Noise-Based Volume Rendering for the Visualization.pdf:application/pdf}
}

@book{rosenblum_scientific_1994,
	address = {London},
	title = {Scientific visualization: advances and challenges},
	isbn = {0-12-227742-2},
	shorttitle = {Scientific visualization},
	publisher = {Academic Press},
	editor = {Rosenblum, L.},
	year = {1994},
	keywords = {Computer Graphics, Computers Graphics Design, Visualization}
}

@phdthesis{kim_saliency-guided_2008,
	type = {{PhD} {Thesis}},
	title = {Saliency-guided {Graphics} and {Visualization}},
	url = {http://drum.lib.umd.edu/handle/1903/8744},
	abstract = {In this dissertation, we show how we can use principles of saliency to enhance depiction, manage visual attention, and increase interactivity for 3D graphics and visualization. Current mesh saliency approaches are inspired by low-level human visual cues, but have not yet been validated. Our eye-tracking-based user study shows that the current computational model of mesh saliency can well approximate human eye movements. Artists, illustrators, photographers, and cinematographers have long used the principles of contrast and composition to guide visual attention. We present a visual-saliency-based operator to draw visual attention to selected regions of interest. We have observed that it is more successful at eliciting viewer attention than the traditional Gaussian enhancement operator for visualizing both volume datasets and 3D meshes.

Mesh saliency can be measured in various ways. The previous model of saliency computes saliency by identifying the uniqueness of curvature. Another way to identify uniqueness is to look for non-repeating structure in the middle of repeating structure. We have developed a system to detect repeating patterns in 3D point datasets. We introduce the idea of creating vertex and transformation streams that represent large point datasets via their interaction. This dramatically improves arithmetic intensity and addresses the input geometry bandwidth bottleneck for interactive 3D graphics applications.

Fast-previewing of time-varing datasets is important for the purpose of summarization and abstraction. We compute the salient frames in molecular dynamics simulations through the subspace analysis of the protein's residue orientations. We first compute an affinity matrix for each frame i of the simulation based on the similarity of the orientation of the protein's backbone residues. Eigenanalysis of the affinity matrix gives us the subspace that best represents the conformation of the current frame i. We use this subspace to represent the frames ahead and behind frame i. The more accurately we can use the subspace of frame i to represent its neighbors, the less salient it is.

Taken together, the tools and techniques developed in this dissertation are likely to provide the building blocks for the next generation visual analysis, reasoning, and discovery environments.},
	language = {en\_US},
	urldate = {2015-10-21},
	school = {University of Maryland, College Park},
	author = {Kim, Youngmin},
	month = aug,
	year = {2008},
	file = {Full Text PDF:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\VNTGM46Z\\Kim - 2008 - Saliency-guided Graphics and Visualization.pdf:application/pdf;Snapshot:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\MRDBZV6I\\8744.html:text/html}
}

@article{alper_selver_exploring_2015,
	title = {Exploring {Brushlet} {Based} 3D {Textures} in {Transfer} {Function} {Specification} for {Direct} {Volume} {Rendering} of {Abdominal} {Organs}},
	volume = {21},
	issn = {1077-2626},
	doi = {10.1109/TVCG.2014.2359462},
	abstract = {Intuitive and differentiating domains for transfer function (TF) specification for direct volume rendering is an important research area for producing informative and useful 3D images. One of the emerging branches of this research is the texture based transfer functions. Although several studies in two, three, and four dimensional image processing show the importance of using texture information, these studies generally focus on segmentation. However, TFs can also be built effectively using appropriate texture information. To accomplish this, methods should be developed to collect wide variety of shape, orientation, and texture of biological tissues and organs. In this study, volumetric data (i.e., domain of a TF) is enhanced using brushlet expansion, which represents both low and high frequency textured structures at different quadrants in transform domain. Three methods (i.e., expert based manual, atlas and machine learning based automatic) are proposed for selection of the quadrants. Non-linear manipulation of the complex brushlet coefficients is also used prior to the tiling of selected quadrants and reconstruction of the volume. Applications to abdominal data sets acquired with CT, MR, and PET show that the proposed volume enhancement effectively improves the quality of 3D rendering using well-known TF specification techniques.},
	number = {2},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Alper Selver, M.},
	month = feb,
	year = {2015},
	keywords = {3D rendering quality, Abdominal Imaging, abdominal organs, atlas, biological organs, biomedical MRI, brushlet based 3D textures, brushlet expansion, Brushlets, complex brushlet coefficients, computerised tomography, CT, direct volume rendering, expert based manual, frequency-domain analysis, high frequency textured structures, Histograms, Image reconstruction, image texture, learning (artificial intelligence), low frequency textured structures, machine learning based automatic, medical image processing, MR, Noise, PET, Positron emission tomography, rendering (computer graphics), Texture Analysis, TF specification techniques, Transfer functions, transfer function specification, transform domain, Transforms, Volume rendering},
	pages = {174--187},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\FDT7ZFRD\\articleDetails.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\D4F6PH8Q\\Alper Selver - 2015 - Exploring Brushlet Based 3D Textures in Transfer F.pdf:application/pdf}
}

@article{kroes_exposure_2012,
	title = {Exposure {Render}: {An} {Interactive} {Photo}-{Realistic} {Volume} {Rendering} {Framework}},
	volume = {7},
	shorttitle = {Exposure {Render}},
	url = {http://dx.doi.org/10.1371/journal.pone.0038586},
	doi = {10.1371/journal.pone.0038586},
	abstract = {The field of volume visualization has undergone rapid development during the past years, both due to advances in suitable computing hardware and due to the increasing availability of large volume datasets. Recent work has focused on increasing the visual realism in Direct Volume Rendering (DVR) by integrating a number of visually plausible but often effect-specific rendering techniques, for instance modeling of light occlusion and depth of field. Besides yielding more attractive renderings, especially the more realistic lighting has a positive effect on perceptual tasks. Although these new rendering techniques yield impressive results, they exhibit limitations in terms of their exibility and their performance. Monte Carlo ray tracing (MCRT), coupled with physically based light transport, is the de-facto standard for synthesizing highly realistic images in the graphics domain, although usually not from volumetric data. Due to the stochastic sampling of MCRT algorithms, numerous effects can be achieved in a relatively straight-forward fashion. For this reason, we have developed a practical framework that applies MCRT techniques also to direct volume rendering (DVR). With this work, we demonstrate that a host of realistic effects, including physically based lighting, can be simulated in a generic and flexible fashion, leading to interactive DVR with improved realism. In the hope that this improved approach to DVR will see more use in practice, we have made available our framework under a permissive open source license.},
	number = {7},
	urldate = {2014-06-16},
	journal = {PLoS ONE},
	author = {Kroes, Thomas and Post, Frits H. and Botha, Charl P.},
	month = jul,
	year = {2012},
	pages = {e38586},
	file = {PLoS Full Text PDF:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\NTKBMVPW\\Kroes et al. - 2012 - Exposure Render An Interactive Photo-Realistic Vo.pdf:application/pdf;PLoS Snapshot:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\3JR8JHIC\\infodoi10.1371journal.pone.html:text/html}
}

@inproceedings{jung_opacity-driven_2014,
	title = {Opacity-driven volume clipping for slice of interest ({SOI}) visualisation of multi-modality {PET}-{CT} volumes},
	doi = {10.1109/EMBC.2014.6945169},
	abstract = {Multi-modality positron emission tomography and computed tomography (PET-CT) imaging depicts biological and physiological functions (from PET) within a higher resolution anatomical reference frame (from CT). The need to efficiently assimilate the information from these co-aligned volumes simultaneously has resulted in 3D visualisation methods that depict e.g., slice of interest (SOI) from PET combined with direct volume rendering (DVR) of CT. However because DVR renders the whole volume, regions of interests (ROIs) such as tumours that are embedded within the volume may be occluded from view. Volume clipping is typically used to remove occluding structures by `cutting away' parts of the volume; this involves tedious trail-and-error tweaking of the clipping attempts until a satisfied visualisation is made, thus restricting its application. Hence, we propose a new automated opacity-driven volume clipping method for PET-CT using DVR-SOI visualisation. Our method dynamically calculates the volume clipping depth by considering the opacity information of the CT voxels in front of the PET SOI, thereby ensuring that only the relevant anatomical information from the CT is visualised while not impairing the visibility of the PET SOI. We outline the improvements of our method when compared to conventional 2D and traditional DVR-SOI visualisations.},
	booktitle = {2014 36th {Annual} {International} {Conference} of the {IEEE} {Engineering} in {Medicine} and {Biology} {Society} ({EMBC})},
	author = {Jung, Younhyun and Kim, Jinman and Fulham, M. and Feng, D.D.},
	month = aug,
	year = {2014},
	keywords = {automated opacity-driven volume clipping method, biological functions, Biomedical imaging, Computed tomography, computerised tomography, data visualization, direct volume rendering, DVR-slice-of-interest visualisation, higher resolution anatomical reference frame, Image resolution, medical image processing, multimodality PET-CT volumes, physiological functions, Positron emission tomography, region-of-interest visualisation, rendering (computer graphics), three-dimensional displays, tumours, Visualization},
	pages = {6714--6717},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\VBHKTTWF\\articleDetails.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\TW5SVAEP\\Jung et al. - 2014 - Opacity-driven volume clipping for slice of intere.pdf:application/pdf}
}

@inproceedings{jung_dual-modal_2012,
	title = {Dual-modal visibility metrics for interactive {PET}-{CT} visualization},
	doi = {10.1109/EMBC.2012.6346520},
	abstract = {Dual-modal positron emission tomography and computed tomography (PET-CT) imaging enables the visualization of functional structures (PET) within human bodies in the spatial context of their anatomical (CT) counterparts, and is providing unprecedented capabilities in understanding diseases. However, the need to access and assimilate the two volumes simultaneously has raised new visualization challenges. In typical dual-modal visualization, the transfer functions for the two volumes are designed in isolation with the resulting volumes being fused. Unfortunately, such transfer function design fails to exploit the correlation that exists between the two volumes. In this study, we propose a dual-modal visualization method where we employ `visibility' metrics to provide interactive visual feedback regarding the occlusion caused by the first volume on the second volume and vice versa. We further introduce a region of interest (ROI) function that allows visibility analyses to be restricted to subsection of the volume. We demonstrate the new visualization enabled by our proposed dual-modal visibility metrics using clinical whole-body PET-CT studies of various diseases.},
	booktitle = {2012 {Annual} {International} {Conference} of the {IEEE} {Engineering} in {Medicine} and {Biology} {Society} ({EMBC})},
	author = {Jung, Younhyun and Kim, Jinman and Feng, D.D.},
	month = aug,
	year = {2012},
	keywords = {Algorithms, clinical whole-body PET-CT property, Computed tomography, Computer Graphics, disease, diseases, dual-modal positron emission tomography, dual-modal visibility metrics, dual-modal visualization method, hidden feature removal, Histograms, Humans, interactive visual feedback, Lungs, medical image processing, Multimodal Imaging, occlusion, PET-CT visualization, Positron emission tomography, Positron-Emission Tomography, region of interest function, rendering (computer graphics), Tomography, X-Ray Computed, Transfer functions, visibility metrics, Visualization},
	pages = {2696--2699},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\9IGKHNN7\\articleDetails.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\HGVTQ5WB\\Jung et al. - 2012 - Dual-modal visibility metrics for interactive PET-.pdf:application/pdf}
}

@inproceedings{wang_saliency-aware_2013,
	title = {Saliency-{Aware} {Volume} {Data} {Resizing} by {Surface} {Carving}},
	doi = {10.1109/CADGraphics.2013.90},
	abstract = {We present a saliency-aware volume resizing operation called surface carving, which intelligently removes contextual voxels while preserving important features. By iteratively applying surface carving in all directions, we can create a volume of the desired size. For large volume data sets, a multilevel banded method is introduced to gracefully overcome the memory limit and speed up volume resizing. We compare our technique with traditionally cropping and scaling approaches and demonstrate the effectiveness and efficiency of our method with several volume data sets.},
	booktitle = {2013 {International} {Conference} on {Computer}-{Aided} {Design} and {Computer} {Graphics} ({CAD}/{Graphics})},
	author = {Wang, Qichao and Tao, Yubo and Lin, Hai},
	month = nov,
	year = {2013},
	keywords = {Context, contextual voxels, data reduction, data visualisation, Educational institutions, Energy measurement, Manifolds, Memory management, multilevel banded method, Saliency Aware, saliency-aware volume data resizing, saliency-aware volume resizing operation, surface carving, Transfer functions, Volume Resizing, Weight measurement},
	pages = {447--448},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\R8T8HECE\\articleDetails.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\WGG57C67\\Wang et al. - 2013 - Saliency-Aware Volume Data Resizing by Surface Car.pdf:application/pdf}
}

@article{shen_save:_2014,
	title = {{SAVE}: saliency-assisted volume exploration},
	issn = {1343-8875, 1875-8975},
	shorttitle = {{SAVE}},
	url = {http://link.springer.com/article/10.1007/s12650-014-0237-y},
	doi = {10.1007/s12650-014-0237-y},
	abstract = {Abstract Interactive visualization has become a valuable tool in visual exploration of scientific data. One prerequisite and fundamental issue is how to infer three-dimensional information through users’ two-dimensional input. Existing approaches commonly build on the hypothesis that user input is precise, which is sometimes invalid because of multiple causes like data noise, limited resolution of display devices and users’ casual input. In this paper, we reconsider some design choices of previous methods and propose an alternative effective algorithm for inferring interaction position in scientific data, especially volume data exploration. Our method automatically assists user interaction with the defined saliency. The presented saliency integrates data value, corresponding transfer function and user input. The result saliency implies remarkable regions of raw data as existing methods. Moreover, it reflects the areas of users’ concern. Thirdly, it eliminates the errors from data and device, helping users get the region they focus on. Various experiments have verified that our method can reasonably refine user interaction and effectively help users access interested features. Graphical Abstract},
	language = {en},
	urldate = {2014-10-08},
	journal = {Journal of Visualization},
	author = {Shen, Enya and Li, Sikun and Cai, Xun and Zeng, Liang and Wang, Wenke},
	month = oct,
	year = {2014},
	volume={18},
	number={2},
	keywords = {Classical Continuum Physics, Computer Imaging, Vision, Pattern Recognition and Graphics, Engineering Fluid Dynamics, Engineering Thermodynamics, Heat and Mass Transfer, Human perception, Interactive visualization, WYSIWYG},
	pages = {1--11}
}

@article{van_pelt_illustrative_2010,
	title = {Illustrative {Volume} {Visualization} {Using} {GPU}-{Based} {Particle} {Systems}},
	volume = {16},
	issn = {1077-2626},
	doi = {10.1109/TVCG.2010.32},
	abstract = {Illustrative techniques are generally applied to produce stylized renderings. Various illustrative styles have been applied to volumetric data sets, producing clearer images and effectively conveying visual information. We adopt particle systems to produce user-configurable stylized renderings from the volume data, imitating traditional pen-and-ink drawings. In the following, we present an interactive GPU-based illustrative volume rendering framework, called VolFliesGPU. In this framework, isosurfaces are sampled by evenly distributed particle sets, delineating surface shape by illustrative styles. The appearance of these styles is based on locally-measured surface properties. For instance, hatches convey surface shape by orientation and shape characteristics are enhanced by color, mapped using a curvature-based transfer function. Hidden-surfaces are generally removed to avoid visual clutter, after that a combination of styles is applied per isosurface. Multiple surfaces and styles can be explored interactively, exploiting parallelism in both graphics hardware and particle systems. We achieve real-time interaction and prompt parametrization of the illustrative styles, using an intuitive GPGPU paradigm that delivers the computational power to drive our particle system and visualization algorithms.},
	number = {4},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {van Pelt, R. and Vilanova, A. and van de Wetering, H.},
	month = aug,
	year = {2010},
	keywords = {Algorithms, Biomedical imaging, Computed tomography, computer graphic equipment, Computer Graphics, Computer Simulation, consumer graphics hardware, coprocessors, curvature-based transfer function, data visualisation, data visualization, distributed particle sets, GPU-based illustrative volume rendering framework, GPU-based particle systems, Graphics, graphics hardware, Hardware, illustrative rendering, Image Interpretation, Computer-Assisted, Imaging, Three-Dimensional, Isosurfaces, Models, Theoretical, parallel processing, parallel processing., Particle Size, Particle systems, real time interaction, Real time systems, rendering (computer graphics), Rendering (computer graphics), Shape, traditional pen-and-ink drawings, User-Computer Interface, user-configurable stylized renderings, Visual information, VolFliesGPU, volumetric data sets, Volume visualization, volume visualization},
	pages = {571 --582},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\WE5C52UA\\articleDetails.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\3HNRUZBZ\\van Pelt et al. - 2010 - Illustrative Volume Visualization Using GPU-Based .pdf:application/pdf}
}

@article{alper_selver_semiautomatic_2009,
	title = {Semiautomatic {Transfer} {Function} {Initialization} for {Abdominal} {Visualization} {Using} {Self}-{Generating} {Hierarchical} {Radial} {Basis} {Function} {Networks}},
	volume = {15},
	issn = {1077-2626},
	doi = {10.1109/TVCG.2008.198},
	abstract = {Being a tool that assigns optical parameters used in interactive visualization, transfer functions (TF) have important effects on the quality of volume rendered medical images. Unfortunately, finding accurate TFs is a tedious and time consuming task because of the trade off between using extensive search spaces and fulfilling the physician's expectations with interactive data exploration tools and interfaces. By addressing this problem, we introduce a semi-automatic method for initial generation of TFs. The proposed method uses a self generating hierarchical radial basis function network to determine the lobes of a volume histogram stack (VHS) which is introduced as a new domain by aligning the histograms of slices of a image series. The new self generating hierarchical design strategy allows the recognition of suppressed lobes corresponding to suppressed tissues and representation of the overlapping regions which are parts of the lobes but can not be represented by the Gaussian bases in VHS. Moreover, approximation with a minimum set of basis functions provides the possibility of selecting and adjusting suitable units to optimize the TF. Applications on different CT/MR data sets show enhanced rendering quality and reduced optimization time in abdominal studies.},
	number = {3},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Alper Selver, M. and Guzelis, C.},
	year = {2009},
	keywords = {Abdomen, Algorithms, Biomedical imaging, biomedical optical imaging, Computed tomography, Computer Graphics, Computer Simulation, data visualisation, data visualization, Gaussian base, Gaussian processes, Hierarchical radial basis function networks, Histograms, Humans, image recognition, Imaging, Three-Dimensional, interactive abdominal visualization, interactive systems, medical image, medical image processing, Models, Biological, multiscale analysis, Optical network units, Pattern Recognition, Automated, radial basis function networks, Radiographic Image Enhancement, Radiographic Image Interpretation, Computer-Assisted, Radiography, Abdominal, rendering (computer graphics), Rendering (computer graphics), Reproducibility of Results, self-generating hierarchical radial basis function network, semiautomatic transfer function initialization, Sensitivity and Specificity, statistical analysis, suppressed lobe recognition, suppressed tissue recognition, Transfer function design, Transfer functions, transfer functions, User-Computer Interface, volume histogram stack, volume rendered medical image, Volume visualization, volume visualization},
	pages = {395--409},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\7TEUW9AR\\articleDetails.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\6ERRPI4U\\Alper Selver and Guzelis - 2009 - Semiautomatic Transfer Function Initialization for.pdf:application/pdf}
}

@inproceedings{tang_depth-based_2011,
	title = {Depth-{Based} {Feature} {Enhancement} for {Volume} {Visualization}},
	doi = {10.1109/CAD/Graphics.2011.14},
	abstract = {Direct volume rendering (DVR) is established as a powerful tool for volume visualization, which accumulates the color and opacity contributions by means of a simple light transport model. However, the mapping from data attributes to the optical properties is defined by transfer functions, the design of which is always a challenging and time-consuming task, even for expert users. In order to build informative images from original volume datasets without specifying intricate transfer functions, we present in this paper a depth-based feature enhancement visualization technique that could provide all features along the viewing ray at once, even with a simple linear transfer function. Once the accumulated opacity overflows, our approach resorts to an adaptive depth-based weighting to reduce the accumulated opacity value for adjusting the contribution of each voxel in the final pixel. To improve the visual perception of interesting features, such a modulation would be further enhanced when local maximum structures are encountered along the viewing ray. In addition, we provide a focus and context interaction and achieve a depth-based clipping operation to help users distinguish the order of internal structures. We conduct experiments on several volumetric datasets, and more structural information is presented and features of interest are largely enhanced in our rendering results, which further demonstrates the effectiveness of our proposed method.},
	booktitle = {2011 12th {International} {Conference} on {Computer}-{Aided} {Design} and {Computer} {Graphics} ({CAD}/{Graphics})},
	author = {Tang, Bangjie and Zhou, Zhiguang and Lin, Hai},
	year = {2011},
	keywords = {adaptive depth-based weighting, Context, data visualisation, depth-based clipping operation, depth-based feature enhancement visualization, depth weighting, direct volume rendering, Equations, feature enhancement, focus and context, Mathematical model, Modulation, optical properties, rendering (computer graphics), Rendering (computer graphics), simple light transport model, Transfer functions, transfer functions, Visual Perception, volume visualization},
	pages = {381--388},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\CXBG3IZ3\\abs_all.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\QQ95B865\\Tang et al. - 2011 - Depth-Based Feature Enhancement for Volume Visuali.pdf:application/pdf}
}

@article{lu_volume_2010,
	title = {Volume {Composition} and {Evaluation} {Using} {Eye}-tracking {Data}},
	volume = {7},
	issn = {1544-3558},
	url = {http://doi.acm.org/10.1145/1658349.1658353},
	doi = {10.1145/1658349.1658353},
	abstract = {This article presents a method for automating rendering parameter selection to simplify tedious user interaction and improve the usability of visualization systems. Our approach acquires the important/interesting regions of a dataset through simple user interaction with an eye tracker. Based on this importance information, we automatically compute reasonable rendering parameters using a set of heuristic rules, which are adapted from visualization experience and psychophysical experiments. A user study has been conducted to evaluate these rendering parameters, and while the parameter selections for a specific visualization result are subjective, our approach provides good preliminary results for general users while allowing additional control adjustment. Furthermore, our system improves the interactivity of a visualization system by significantly reducing the required amount of parameter selections and providing good initial rendering parameters for newly acquired datasets of similar types.},
	number = {1},
	urldate = {2015-10-21},
	journal = {ACM Trans. Appl. Percept.},
	author = {Lu, Aidong and Maciejewski, Ross and Ebert, David S.},
	month = jan,
	year = {2010},
	keywords = {eye tracker, Illustrative visualization, Interaction, Usability and human factors in visualization, Volume rendering},
	pages = {4:1--4:20},
	file = {ACM Full Text PDF:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\B4DBHUAF\\Lu et al. - 2010 - Volume Composition and Evaluation Using Eye-tracki.pdf:application/pdf}
}

@article{hsieh_feature_2013,
	title = {Feature extraction using bionic particle swarm tracing for transfer function design in direct volume rendering},
	volume = {30},
	issn = {0178-2789, 1432-2315},
	url = {http://link.springer.com/article/10.1007/s00371-013-0777-5},
	doi = {10.1007/s00371-013-0777-5},
	abstract = {In direct volume rendering, features of interest are still typically classified by a transfer function based on the volume data’s intensity and the derived properties. Despite the efforts of previous research, classification remains a challenge. This paper presents a framework for designing new transfer functions that use bionic algorithms to map the frequency of particle occurrences to the color and opacity values. This allows us to extract features from the volume data. In particular, a novel approach is presented to allow a user to design a transfer function using the techniques of swarm intelligence. This approach consists of a population of simple agents interacting locally with one another and with the volume data. The agents scatter around the volume data and approach areas that contain features. Their movements are not only based on solution optimization, but are also governed by global optimization. After the agents have finished searching for features in the volume data, they can automatically modify the transfer function according to agents’ behavior. With these agents, we do not have to preprocess the volume data for visualizing and exploring the features.},
	language = {en},
	number = {1},
	urldate = {2015-10-21},
	journal = {The Visual Computer},
	author = {Hsieh, Tung-Ju and Yang, Yuan-Sen and Wang, Jenq-Haur and Shen, Wen-Jay},
	month = feb,
	year = {2013},
	keywords = {Artificial Intelligence (incl. Robotics), Computational Intelligence, Computer graphics, Computer Science, general, Image Processing and Computer Vision, particle swarm optimization, transfer function, Volume rendering},
	pages = {33--44}
}

@inproceedings{zhao_multi-dimensional_2010,
	address = {Aire-la-Ville, Switzerland, Switzerland},
	series = {{VG}'10},
	title = {Multi-dimensional reduction and transfer function design using parallel coordinates},
	isbn = {978-3-905674-23-1},
	url = {http://dx.doi.org/10.2312/VG/VG10/069-076},
	doi = {10.2312/VG/VG10/069-076},
	abstract = {Multi-dimensional transfer functions are widely used to provide appropriate data classification for direct volume rendering. Nevertheless, the design of a multi-dimensional transfer function is a complicated task. In this paper, we propose to use parallel coordinates, a powerful tool to visualize high-dimensional geometry and analyze multivariate data, for multi-dimensional transfer function design. This approach has two major advantages: (1) Combining the information of spatial space (voxel position) and parameter space; (2) Selecting appropriate highdimensional parameters to obtain sophisticated data classification. Although parallel coordinates offers simple interface for the user to design the high-dimensional transfer function, some extra work such as sorting the coordinates is inevitable. Therefore, we use a local linear embedding technique for dimension reduction to reduce the burdensome calculations in the high dimensional parameter space and to represent the transfer function concisely. With the aid of parallel coordinates, we propose some novel high-dimensional transfer function widgets for better visualization results. We demonstrate the capability of our parallel coordinates based transfer function (PCbTF) design method for direct volume rendering using CT and MRI datasets.},
	urldate = {2013-05-08},
	booktitle = {Proceedings of the 8th {IEEE}/{EG} international conference on {Volume} {Graphics}},
	publisher = {Eurographics Association},
	author = {Zhao, X. and Kaufman, A.},
	year = {2010},
	pages = {69--76},
	file = {10.1.1.165.6610.pdf:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\EN473GQA\\10.1.1.165.6610.pdf:application/pdf}
}

@inproceedings{wan_fast_2010,
	address = {Aire-la-Ville, Switzerland, Switzerland},
	series = {{VG}'10},
	title = {Fast {Volumetric} {Data} {Exploration} with {Importance}-based {Accumulated} {Transparency} {Modulation}},
	isbn = {978-3-905674-23-1},
	url = {http://dx.doi.org/10.2312/VG/VG10/061-068},
	doi = {10.2312/VG/VG10/061-068},
	abstract = {Direct volume rendering techniques have been successfully applied to visualizing volumetric datasets across many application domains. Due to the sensitivity of transfer functions and the complexity of fine-tuning transfer functions, direct volume rendering is still not widely used in practice. For fast volumetric data exploration, we propose Importance-Based Accumulated Transparency Modulation which does not rely on transfer function manipulation. This novel rendering algorithm is a generalization and extension of the Maximum Intensity Difference Accumulation technique. By only modifying the accumulated transparency, the resulted volume renderings are essentially high dynamic range. We show that by using several common importance measures, different features of the volumetric datasets can be highlighted. The results can be easily extended to a high-dimensional importance difference space, by mixing the results from an arbitrary number of importance measures with weighting factors, which all control the final output with a monotonic behavior. With Importance-Based Accumulated Transparency Modulation, the end-user can explore a wide variety of volumetric datasets quickly without the burden of manually setting and adjusting a transfer function.},
	urldate = {2015-10-21},
	booktitle = {Proceedings of the 8th {IEEE}/{EG} {International} {Conference} on {Volume} {Graphics}},
	publisher = {Eurographics Association},
	author = {Wan, Y. and Hansen, C.},
	year = {2010},
	pages = {61--68},
	file = {wan_vg2010_fast.pdf:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\N6I8ZNTM\\wan_vg2010_fast.pdf:application/pdf}
}

@article{mak_visibility-aware_2011,
	title = {Visibility-{Aware} {Direct} {Volume} {Rendering}},
	volume = {26},
	issn = {1000-9000, 1860-4749},
	url = {http://link.springer.com/article/10.1007/s11390-011-9428-3},
	doi = {10.1007/s11390-011-9428-3},
	abstract = {Direct volume rendering (DVR) is a powerful visualization technique which allows users to effectively explore and study volumetric datasets. Different transparency settings can be flexibly assigned to different structures such that some valuable information can be revealed in direct volume rendered images (DVRIs). However, end-users often feel that some risks are always associated with DVR because they do not know whether any important information is missing from the transparent regions of DVRIs. In this paper, we investigate how to semi-automatically generate a set of DVRIs and also an animation which can reveal information missed in the original DVRIs and meanwhile satisfy some image quality criteria such as coherence. A complete framework is developed to tackle various problems related to the generation and quality evaluation of visibility-aware DVRIs and animations. Our technique can reduce the risk of using direct volume rendering and thus boost the confidence of users in volume rendering systems.},
	language = {en},
	number = {2},
	urldate = {2013-05-01},
	journal = {Journal of Computer Science and Technology},
	author = {Mak, Wai-Ho and Wu, Yingcai and Chan, Ming-Yuen and Qu, Huamin},
	month = mar,
	year = {2011},
	keywords = {Artificial Intelligence (incl. Robotics), Computer Science, general, Data Structures, Cryptology and Information Theory, Information Systems Applications (incl.Internet), Software Engineering, Theory of Computation, Visualization systems and software, volume visualization},
	pages = {217--228},
	file = {Full Text PDF:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\XKF6NU8Z\\Mak et al. - 2011 - Visibility-Aware Direct Volume Rendering.pdf:application/pdf;Snapshot:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\WGUNWFVX\\10.html:text/html}
}

@article{zhou_transfer_2012,
	series = {2011 {Joint} {Symposium} on {Computational} {Aesthetics} ({CAe}), {Non}-{Photorealistic} {Animation} and {Rendering} ({NPAR}), and {Sketch}-{Based} {Interfaces} and {Modeling} ({SBIM})},
	title = {Transfer function combinations},
	volume = {36},
	issn = {0097-8493},
	url = {http://www.sciencedirect.com/science/article/pii/S0097849312000246},
	doi = {10.1016/j.cag.2012.02.007},
	abstract = {Direct volume rendering has been an active area of research for over two decades. Transfer function design remains a difficult task since current methods, such as traditional 1D and 2D transfer functions, are not always effective for all data sets. Various 1D or 2D transfer function spaces have been proposed to improve classification exploiting different aspects, such as using the gradient magnitude for boundary location and statistical, occlusion, or size metrics. In this paper, we present a novel transfer function method which can provide more specificity for data classification by combining different transfer function spaces. In this work, a 2D transfer function can be combined with 1D transfer functions which improve the classification. Specifically, we use the traditional 2D scalar/gradient magnitude, 2D statistical, and 2D occlusion spectrum transfer functions and combine these with occlusion and/or size-based transfer functions to provide better specificity. We demonstrate the usefulness of the new method by comparing to the following previous techniques: 2D gradient magnitude, 2D occlusion spectrum, 2D statistical transfer functions and 2D size based transfer functions.},
	number = {6},
	urldate = {2015-10-21},
	journal = {Computers \& Graphics},
	author = {Zhou, Liang and Schott, Mathias and Hansen, Charles},
	month = oct,
	year = {2012},
	keywords = {Classification, transfer function, User Interface, Volume rendering},
	pages = {596--606},
	file = {10-21-2015_Transfer f.pdf:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\B6HU8GZ7\\10-21-2015_Transfer f.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\TI7E3DBE\\S0097849312000246.html:text/html}
}

@inproceedings{guo_multi-dimensional_2011,
	title = {Multi-dimensional transfer function design based on flexible dimension projection embedded in parallel coordinates},
	doi = {10.1109/PACIFICVIS.2011.5742368},
	abstract = {In this paper, we present an effective transfer function (TF) design for multivariate volume, providing tightly coupled views of parallel coordinates plot (PCP), MDS-based dimension projection plots, and volume rendered image space. In our design, the PCP showing the data distribution of each variate dimension and the MDS showing reduced dimensional features are integrated seamlessly to provide flexible feature classification for the user without context switching between different data presentations. Our proposed interface enables users to identify interested clusters and assign optical properties with lassos, magic wand and other tools. Furthermore, sketching directly on the volume rendered images has been implemented to probe and edit features. To achieve interactivity, octree partitioning with Gaussian Mixture Model (GMM), and other data reduction techniques are applied. Our experiments show that the proposed method is effective for multidimensional TF design and data exploration.},
	booktitle = {Visualization {Symposium} ({PacificVis}), 2011 {IEEE} {Pacific}},
	author = {Guo, Hanqi and Xiao, He and Yuan, Xiaoru},
	month = mar,
	year = {2011},
	keywords = {Algorithm design and analysis, Context, context switching, data exploration, data presentation, data reduction technique, data visualization, dimension projection, flexible dimension projection, flexible feature classification, Gaussian Mixture Model, Gaussian processes, Image processing, multidimensional transfer function design, multivariate volume, Multivariate volume rendering, octree partitioning, octrees, Parallel coordinates, Parallel coordinates plot, rendering (computer graphics), Strips, transfer function, Transfer functions, user interface design, volume rendered image space},
	pages = {19--26},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\MEJSAD33\\abs_all.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\TPS664JV\\Guo et al. - 2011 - Multi-dimensional transfer function design based o.pdf:application/pdf}
}

@article{gu_transgraph:_2011,
	title = {{TransGraph}: {Hierarchical} {Exploration} of {Transition} {Relationships} in {Time}-{Varying} {Volumetric} {Data}},
	volume = {17},
	issn = {1077-2626},
	shorttitle = {{TransGraph}},
	doi = {10.1109/TVCG.2011.246},
	abstract = {A fundamental challenge for time-varying volume data analysis and visualization is the lack of capability to observe and track data change or evolution in an occlusion-free, controllable, and adaptive fashion. In this paper, we propose to organize a timevarying data set into a hierarchy of states. By deriving transition probabilities among states, we construct a global map that captures the essential transition relationships in the time-varying data. We introduce the TransGraph, a graph-based representation to visualize hierarchical state transition relationships. The TransGraph not only provides a visual mapping that abstracts data evolution over time in different levels of detail, but also serves as a navigation tool that guides data exploration and tracking. The user interacts with the TransGraph and makes connection to the volumetric data through brushing and linking. A set of intuitive queries is provided to enable knowledge extraction from time-varying data. We test our approach with time-varying data sets of different characteristics and the results show that the TransGraph can effectively augment our ability in understanding time-varying data.},
	number = {12},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Gu, Yi and Wang, Chaoli},
	month = dec,
	year = {2011},
	keywords = {data analysis, data mining, Data structures, data visualisation, data visualization, graph based representation, graph theory, hierarchical representation, hierarchical state transition relationships, Hierarchical systems, Histograms, knowledge extraction, navigation tool, occlusion free, probability, states, time-varying data visualization, time-varying volumetric data analysis, TransGraph, transition probability, transition relationship, user interface., User interfaces, visual mapping},
	pages = {2015--2024},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\3G2BAUEE\\abstractCitations.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\9QXD7II7\\Gu and Wang - 2011 - TransGraph Hierarchical Exploration of Transition.pdf:application/pdf}
}

@article{kersten-oertel_evaluation_2014,
	title = {An {Evaluation} of {Depth} {Enhancing} {Perceptual} {Cues} for {Vascular} {Volume} {Visualization} in {Neurosurgery}},
	volume = {20},
	issn = {1077-2626},
	doi = {10.1109/TVCG.2013.240},
	abstract = {Cerebral vascular images obtained through angiography are used by neurosurgeons for diagnosis, surgical planning, and intraoperative guidance. The intricate branching of the vessels and furcations, however, make the task of understanding the spatial three-dimensional layout of these images challenging. In this paper, we present empirical studies on the effect of different perceptual cues (fog, pseudo-chromadepth, kinetic depth, and depicting edges) both individually and in combination on the depth perception of cerebral vascular volumes and compare these to the cue of stereopsis. Two experiments with novices and one experiment with experts were performed. The results with novices showed that the pseudo-chromadepth and fog cues were stronger cues than that of stereopsis. Furthermore, the addition of the stereopsis cue to the other cues did not improve relative depth perception in cerebral vascular volumes. In contrast to novices, the experts also performed well with the edge cue. In terms of both novice and expert subjects, pseudo-chromadepth and fog allow for the best relative depth perception. By using such cues to improve depth perception of cerebral vasculature, we may improve diagnosis, surgical planning, and intraoperative guidance.},
	number = {3},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Kersten-Oertel, M. and Chen, S.J.-S. and Collins, D.L.},
	month = mar,
	year = {2014},
	keywords = {Angiography, biomedical MRI, cerebral vascular images, chromadepth, data visualisation, depth cues, depth enhancing perceptual cues, edge cue, fog, fog cues, Kinetic theory, neurophysiology, neurosurgery, pseudo-chromadepth cues, relative depth perception, rendering (computer graphics), spatial three-dimensional layout, stereo, Stereo image processing, stereopsis cue, surgery, three-dimensional displays, vascular data, vascular volume visualization, vessels, Volume rendering},
	pages = {391--403},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\C2I4I39A\\articleDetails.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\4W6CTURN\\Kersten-Oertel et al. - 2014 - An Evaluation of Depth Enhancing Perceptual Cues f.pdf:application/pdf}
}

@article{lindholm_spatial_2010,
	title = {Spatial {Conditioning} of {Transfer} {Functions} {Using} {Local} {Material} {Distributions}},
	volume = {16},
	issn = {1077-2626},
	doi = {10.1109/TVCG.2010.195},
	abstract = {In many applications of Direct Volume Rendering (DVR) the importance of a certain material or feature is highly dependent on its relative spatial location. For instance, in the medical diagnostic procedure, the patient's symptoms often lead to specification of features, tissues and organs of particular interest. One such example is pockets of gas which, if found inside the body at abnormal locations, are a crucial part of a diagnostic visualization. This paper presents an approach that enhances DVR transfer function design with spatial localization based on user specified material dependencies. Semantic expressions are used to define conditions based on relations between different materials, such as only render iodine uptake when close to liver. The underlying methods rely on estimations of material distributions which are acquired by weighing local neighborhoods of the data against approximations of material likelihood functions. This information is encoded and used to influence rendering according to the user's specifications. The result is improved focus on important features by allowing the user to suppress spatially less-important data. In line with requirements from actual clinical DVR practice, the methods do not require explicit material segmentation that would be impossible or prohibitively time-consuming to achieve in most real cases. The scheme scales well to higher dimensions which accounts for multi-dimensional transfer functions and multivariate data. Dual-Energy Computed Tomography, an important new modality in radiology, is used to demonstrate this scalability. In several examples we show significantly improved focus on clinically important aspects in the rendered images.},
	number = {6},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Lindholm, S. and Ljung, P. and Lundstrom, C. and Persson, A. and Ynnerman, A.},
	year = {2010},
	keywords = {Blood, Colonography, Computed Tomographic, Computer Graphics, Data Display, diagnostic visualization, direct volume rendering, dual-energy computed tomography, Gases, Histograms, Humans, Intracranial Aneurysm, Iodine, Liver, local material distribution, Magnetic resonance imaging, material likelihood function, Materials, medical diagnostic procedure, medical image processing, modality, multidimensional transfer function, multivariate data, Neighborhood Meta-Data., Probes, radiology, rendered image, rendering (computer graphics), Rendering (computer graphics), Semantics, solid modelling, Spatial Conditioning, spatial conditioning, spatial localization, Tissue Distribution, Tomography, Tomography, X-Ray Computed, transfer function, Transfer functions, transfer functions, user specification},
	pages = {1301--1310},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\CJS62HAF\\articleDetails.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\Z7JP8MMT\\Lindholm et al. - 2010 - Spatial Conditioning of Transfer Functions Using L.pdf:application/pdf}
}

@article{busking_particle-based_2007,
	title = {Particle-based non-photorealistic volume visualization},
	volume = {24},
	issn = {0178-2789, 1432-2315},
	url = {http://link.springer.com/article/10.1007/s00371-007-0192-x},
	doi = {10.1007/s00371-007-0192-x},
	abstract = {Non-photorealistic techniques are usually applied to produce stylistic renderings. In visualization, these techniques are often able to simplify data, producing clearer images than traditional visualization methods. We investigate the use of particle systems for visualizing volume datasets using non-photorealistic techniques. In our VolumeFlies framework, user-selectable rules affect particles to produce a variety of illustrative styles in a unified way. The techniques presented do not require the generation of explicit intermediary surfaces.},
	language = {en},
	number = {5},
	urldate = {2015-10-22},
	journal = {The Visual Computer},
	author = {Busking, Stef and Vilanova, Anna and Wijk, Jarke J. van},
	month = dec,
	year = {2007},
	keywords = {Artificial Intelligence (incl. Robotics), Computer Graphics, Image Processing and Computer Vision, non-photorealistic rendering, Particle systems, Visualization, Volume rendering},
	pages = {335--346},
	file = {Full Text PDF:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\3AXJHD7V\\Busking et al. - 2007 - Particle-based non-photorealistic volume visualiza.pdf:application/pdf;Snapshot:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\VBCUUREW\\10.html:text/html}
}

@inproceedings{hadwiger_high-quality_2003,
	title = {High-quality two-level volume rendering of segmented data sets on consumer graphics hardware},
	doi = {10.1109/VISUAL.2003.1250386},
	abstract = {One of the most important goals in volume rendering is to be able to visually separate and selectively enable specific objects of interest contained in a single volumetric data set, which can be approached by using explicit segmentation information. We show how segmented data sets can be rendered interactively on current consumer graphics hardware with high image quality and pixel-resolution filtering of object boundaries. In order to enhance object perception, we employ different levels of object distinction. First, each object can be assigned an individual transfer function, multiple of which can be applied in a single rendering pass. Second, different rendering modes such as direct volume rendering, iso-surfacing, and non-photorealistic techniques can be selected for each object. A minimal number of rendering passes is achieved by processing sets of objects that share the same rendering mode in a single pass. Third, local compositing modes such as alpha blending and MIP can be selected for each object in addition to a single global mode, thus enabling high-quality two-level volume rendering on GPUs.},
	booktitle = {{IEEE} {Visualization}, 2003. {VIS} 2003},
	author = {Hadwiger, M. and Berger, C. and Hauser, H.},
	month = oct,
	year = {2003},
	keywords = {alpha blending, blood vessels, Bones, Computer Graphics, consumer graphics hardware, data visualization, direct volume rendering, Filtering, Hardware, image quality, image segmentation, iso-surfacing, MIP, nonphotorealistic rendering, pixel-resolution filtering, rendering (computer graphics), rendering mode, segmentated data sets, single rendering pass, Skin, solid modelling, Transfer functions, volumetric data set},
	pages = {301--308},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\5TRRX6W6\\abs_all.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\55GDZX9D\\Hadwiger et al. - 2003 - High-quality two-level volume rendering of segment.pdf:application/pdf}
}

@article{kehrer_visualization_2013,
	title = {Visualization and {Visual} {Analysis} of {Multifaceted} {Scientific} {Data}: {A} {Survey}},
	volume = {19},
	issn = {1077-2626},
	shorttitle = {Visualization and {Visual} {Analysis} of {Multifaceted} {Scientific} {Data}},
	doi = {10.1109/TVCG.2012.110},
	abstract = {Visualization and visual analysis play important roles in exploring, analyzing, and presenting scientific data. In many disciplines, data and model scenarios are becoming multifaceted: data are often spatiotemporal and multivariate; they stem from different data sources (multimodal data), from multiple simulation runs (multirun/ensemble data), or from multiphysics simulations of interacting phenomena (multimodel data resulting from coupled simulation models). Also, data can be of different dimensionality or structured on various types of grids that need to be related or fused in the visualization. This heterogeneity of data characteristics presents new opportunities as well as technical challenges for visualization research. Visualization and interaction techniques are thus often combined with computational analysis. In this survey, we study existing methods for visualization and interactive visual analysis of multifaceted scientific data. Based on a thorough literature review, a categorization of approaches is proposed. We cover a wide range of fields and discuss to which degree the different challenges are matched with existing solutions for visualization and visual analysis. This leads to conclusions with respect to promising research directions, for instance, to pursue new solutions for multirun and multimodel data as well as techniques that support a multitude of facets.},
	number = {3},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Kehrer, J. and Hauser, H.},
	month = mar,
	year = {2013},
	keywords = {Algorithms, Atmospheric modeling, computational analysis, Computational modeling, Computer Graphics, Computer Simulation, database management systems, Databases, Factual, data characteristics heterogeneity, data mining, data models, data sources, data visualisation, data visualization, Image Interpretation, Computer-Assisted, Imaging, Three-Dimensional, interactive visual analysis, Models, Theoretical, multifaceted scientific data, multimodal, multimodel, multimodel data, multiphysics simulations, multirun, multirun-ensemble data, multivariate, multivariate data, Solid modeling, spatiotemporal data, User-Computer Interface, Visualization, visualization techniques},
	pages = {495--513},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\A9DNTB26\\articleDetails.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\I9FM2PPJ\\Kehrer and Hauser - 2013 - Visualization and Visual Analysis of Multifaceted .pdf:application/pdf}
}

@article{lindemann_about_2011,
	title = {About the {Influence} of {Illumination} {Models} on {Image} {Comprehension} in {Direct} {Volume} {Rendering}},
	volume = {17},
	issn = {1077-2626},
	doi = {10.1109/TVCG.2011.161},
	abstract = {In this paper, we present a user study in which we have investigated the influence of seven state-of-the-art volumetric illumination models on the spatial perception of volume rendered images. Within the study, we have compared gradient-based shading with half angle slicing, directional occlusion shading, multidirectional occlusion shading, shadow volume propagation, spherical harmonic lighting as well as dynamic ambient occlusion. To evaluate these models, users had to solve three tasks relying on correct depth as well as size perception. Our motivation for these three tasks was to find relations between the used illumination model, user accuracy and the elapsed time. In an additional task, users had to subjectively judge the output of the tested models. After first reviewing the models and their features, we will introduce the individual tasks and discuss their results. We discovered statistically significant differences in the testing performance of the techniques. Based on these findings, we have analyzed the models and extracted those features which are possibly relevant for the improved spatial comprehension in a relational task. We believe that a combination of these distinctive features could pave the way for a novel illumination model, which would be optimized based on our findings.},
	number = {12},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Lindemann, F. and Ropinski, T.},
	month = dec,
	year = {2011},
	keywords = {Computational modeling, Depth Perception, directional occlusion shading, direct volume rendering, dynamic ambient occlusion, gradient based shading, gradient methods, half angle slicing, Harmonic analysis, Image color analysis, image comprehension, lighting, Light sources, multidirectional occlusion shading, rendering (computer graphics), shadow volume propagation, size perception, Solid modeling, spatial comprehension, spatial comprehension., spherical harmonic lighting, Volume rendering, Volumetric illumination, volumetric illumination models},
	pages = {1922--1931},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\XG6FQ9DV\\abs_all.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\XKKKGN5X\\Lindemann and Ropinski - 2011 - About the Influence of Illumination Models on Imag.pdf:application/pdf}
}

@inproceedings{wang_magic_2005,
	title = {The magic volume lens: an interactive focus+context technique for volume rendering},
	shorttitle = {The magic volume lens},
	doi = {10.1109/VISUAL.2005.1532818},
	abstract = {The size and resolution of volume datasets in science and medicine are increasing at a rate much greater than the resolution of the screens used to view them. This limits the amount of data that can be viewed simultaneously, potentially leading to a loss of overall context of the data when the user views or zooms into a particular area of interest. We propose a focus+context framework that uses various standard and advanced magnification lens rendering techniques to magnify the features of interest, while compressing the remaining volume regions without clipping them away completely. Some of these lenses can be interactively configured by the user to specify the desired magnification patterns, while others are feature-adaptive. All our lenses are accelerated on the GPU. They allow the user to interactively manage the available screen area, dedicating more area to the more resolution-important features.},
	booktitle = {{IEEE} {Visualization}, 2005. {VIS} 05},
	author = {Wang, Lujin and Zhao, Ye and Mueller, K. and Kaufman, A.},
	month = oct,
	year = {2005},
	keywords = {Computer displays, Computer Graphics, Focusing, hardware-assisted volume rendering, Humans, Image generation, Image resolution, image segmentation, interactive focus-context technique, Large screen displays, lenses, magic volume lens, Navigation, rendering (computer graphics), resolution-important feature, Retina},
	pages = {367--374},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\3WEQTIFR\\articleDetails.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\88ZCRF5J\\Wang et al. - 2005 - The magic volume lens an interactive focus+contex.pdf:application/pdf}
}

@inproceedings{chen_intelligent_2008,
	title = {Intelligent {Focus}+{Context} {Volume} {Visualization}},
	volume = {1},
	doi = {10.1109/ISDA.2008.232},
	abstract = {Although graphics processing unit (GPU) acceleration makes possible interactive volume rendering, successful volume visualization relies on the ability to quickly and correctly classify the volume into different materials or features. Among various classification techniques, one very attractive and effective method is employing machine learning to classify the whole volume according to some minimum user input through an interactive brushing interface, where users paint directly on slices of the volume. For routine visualization tasks, we can thus reduce their cost if the visualization system can learn the tasks and apply the captured knowledge in future tasks. This paper presents an intelligent, interactive visualization system that supports Focus+Context viewing of volume data. Features of interest should be the focal point of the visualization, and by applying appropriate rendering methods we are able to enhance these features and create more illustrative visualizations in a Focus+Context style. We show with a set of case studies that it is possible to use machine learning to not only help classify volume but also better present the classified results. This new capability makes visualization a more usable tool.},
	booktitle = {Eighth {International} {Conference} on {Intelligent} {Systems} {Design} and {Applications}, 2008. {ISDA} '08},
	author = {Chen, Cheng-Kai and Thomason, R. and Ma, Kwan-Liu},
	month = nov,
	year = {2008},
	keywords = {Acceleration, classification technique, data visualisation, data visualization, focus+context, Focusing, GPU acceleration, Graphics, graphics processing unit, Hardware, intelligent focus-context volume visualization, intelligent system, Intelligent systems, Interaction, interactive brushing interface, interactive systems, Interactive Volume Rendering, learning (artificial intelligence), Machine learning, Paints, pattern classification, rendering (computer graphics), routine visualization task, User interfaces, Visualization, volume graphics, Volume rendering},
	pages = {368--374},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\8KKH6EMF\\articleDetails.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\NHX6RPJ4\\Chen et al. - 2008 - Intelligent Focus+Context Volume Visualization.pdf:application/pdf}
}

@inproceedings{sigg_intelligent_2012,
	title = {Intelligent cutaway illustrations},
	doi = {10.1109/PacificVis.2012.6183590},
	abstract = {Artistic illustrations of important structures in fluid flow have a long-standing tradition and are appreciated as clearly perceivable, instructive, but still conveying all relevant information to the viewer. One important illustrative technique for such visualizations are cutaways. Currently cutaways are placed manually or using view-vector based approaches. We propose to optimize the visibility of important target features based on a degree-of-interest (DOI) function. The DOI is specified during interactive visual analysis, e.g., by brushing scatterplots. We show that the problem of placing cutaway boxes optimally is NP-hard in the number of boxes. To overcome this obstacle, we present an intelligent method to compute cutaways. Geometric cutaway objects are positioned using a view-dependent objective function which optimizes the visibility of all features. In order to approximate the optimal solution, we use a Monte Carlo method and exploit temporal coherence in dynamic scenes. Performance-critical parts are implemented on the GPU. The proposed method can be integrated easily into existing rendering frameworks and is general enough to be able to optimize other parameters besides cutaways as well. We evaluate the performance of the algorithm and provide a case study of vorticity visualization in a turbulent flow.},
	booktitle = {Visualization {Symposium} ({PacificVis}), 2012 {IEEE} {Pacific}},
	author = {Sigg, S. and Fuchs, R. and Carnecky, R. and Peikert, R.},
	month = feb,
	year = {2012},
	keywords = {artistic illustration, Computational complexity, Cutaway, data analysis, data visualisation, degree-of-interest function, flow visualization, Geometry, GPU, graphics processing unit, graphics processing units, Image color analysis, intelligent cutaway illustration, interactive visual analysis, Monte Carlo, Monte Carlo method, Monte Carlo methods, NP-hard problem, Optimization, rendering (computer graphics), rendering framework, scatterplot brushing, scientific illustration, Shape, Temperature, temporal coherence, turbulent flow, view-dependent objective function, View-Dependent Optimization, view-vector based approach, Visualization Technique, vorticity visualization},
	pages = {185--192},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\5E3WZK8F\\articleDetails.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\RK4JVJ72\\Sigg et al. - 2012 - Intelligent cutaway illustrations.pdf:application/pdf}
}

@inproceedings{burns_feature_2007,
	address = {Aire-la-Ville, Switzerland, Switzerland},
	series = {{EUROVIS}'07},
	title = {Feature {Emphasis} and {Contextual} {Cutaways} for {Multimodal} {Medical} {Visualization}},
	isbn = {978-3-905673-45-6},
	url = {http://dx.doi.org/10.2312/VisSym/EuroVis07/275-282},
	doi = {10.2312/VisSym/EuroVis07/275-282},
	abstract = {Dense clinical data like 3D Computed Tomography (CT) scans can be visualized together with real-time imaging for a number of medical intervention applications. However, it is difficult to provide a fused visualization that allows sufficient spatial perception of the anatomy of interest, as derived from the rich pre-operative scan, while not occluding the real-time image displayed embedded within the volume. We propose an importance-driven approach that presents the embedded data such that it is clearly visible along with its spatial relation to the surrounding volumetric material. To support this, we present and integrate novel techniques for importance specification, feature emphasis, and contextual cutaway generation. We show results in a clinical context where a pre-operative CT scan is visualized alongside a tracked ultrasound image, such that the important vasculature is depicted between the viewpoint and the ultrasound image, while a more opaque representation of the anatomy is exposed in the surrounding area.},
	urldate = {2015-10-22},
	booktitle = {Proceedings of the 9th {Joint} {Eurographics} / {IEEE} {VGTC} {Conference} on {Visualization}},
	publisher = {Eurographics Association},
	author = {Burns, Michael and Haidacher, Martin and Wein, Wolfgang and Viola, Ivan and Gr{\"o}ller, M. Eduard},
	year = {2007},
	pages = {275--282}
}

@article{zhao_learning_2013,
	series = {Special issue on {Machine} {Learning} in {Intelligent} {Image} {Processing}},
	title = {Learning saliency-based visual attention: {A} review},
	volume = {93},
	issn = {0165-1684},
	shorttitle = {Learning saliency-based visual attention},
	url = {http://www.sciencedirect.com/science/article/pii/S0165168412002058},
	doi = {10.1016/j.sigpro.2012.06.014},
	abstract = {Humans and other primates shift their gaze to allocate processing resources to a subset of the visual input. Understanding and emulating the way that human observers free-view a natural scene has both scientific and economic impact. It has therefore attracted the attention from researchers in a wide range of science and engineering disciplines. With the ever increasing computational power, machine learning has become a popular tool to mine human data in the exploration of how people direct their gaze when inspecting a visual scene. This paper reviews recent advances in learning saliency-based visual attention and discusses several key issues in this topic.},
	number = {6},
	urldate = {2015-10-22},
	journal = {Signal Processing},
	author = {Zhao, Qi and Koch, Christof},
	month = jun,
	year = {2013},
	keywords = {Central fixation bias, Feature representation, Machine learning, Public eye tracking datasets, visual attention},
	pages = {1401--1407},
	file = {ScienceDirect Snapshot:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\ETJQAHBQ\\S0165168412002058.html:text/html}
}

@article{parkhurst_modeling_2002,
	title = {Modeling the role of salience in the allocation of overt visual attention},
	volume = {42},
	issn = {0042-6989},
	url = {http://www.sciencedirect.com/science/article/pii/S0042698901002504},
	doi = {10.1016/S0042-6989(01)00250-4},
	abstract = {A biologically motivated computational model of bottom-up visual selective attention was used to examine the degree to which stimulus salience guides the allocation of attention. Human eye movements were recorded while participants viewed a series of digitized images of complex natural and artificial scenes. Stimulus dependence of attention, as measured by the correlation between computed stimulus salience and fixation locations, was found to be significantly greater than that expected by chance alone and furthermore was greatest for eye movements that immediately follow stimulus onset. The ability to guide attention of three modeled stimulus features (color, intensity and orientation) was examined and found to vary with image type. Additionally, the effect of the drop in visual sensitivity as a function of eccentricity on stimulus salience was examined, modeled, and shown to be an important determiner of attentional allocation. Overall, the results indicate that stimulus-driven, bottom-up mechanisms contribute significantly to attentional guidance under natural viewing conditions.},
	number = {1},
	urldate = {2015-10-22},
	journal = {Vision Research},
	author = {Parkhurst, Derrick and Law, Klinton and Niebur, Ernst},
	month = jan,
	year = {2002},
	keywords = {computational model, Eye Movements, natural images, Salience, visual attention},
	pages = {107--123},
	file = {ScienceDirect Snapshot:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\7NI8AFFI\\S0042698901002504.html:text/html}
}

@article{chikkerur_what_2010,
	series = {Mathematical {Models} of {Visual} {Coding}},
	title = {What and where: {A} {Bayesian} inference theory of attention},
	volume = {50},
	issn = {0042-6989},
	shorttitle = {What and where},
	url = {http://www.sciencedirect.com/science/article/pii/S0042698910002348},
	doi = {10.1016/j.visres.2010.05.013},
	abstract = {In the theoretical framework of this paper, attention is part of the inference process that solves the visual recognition problem of what is where. The theory proposes a computational role for attention and leads to a model that predicts some of its main properties at the level of psychophysics and physiology. In our approach, the main goal of the visual system is to infer the identity and the position of objects in visual scenes: spatial attention emerges as a strategy to reduce the uncertainty in shape information while feature-based attention reduces the uncertainty in spatial information. Featural and spatial attention represent two distinct modes of a computational process solving the problem of recognizing and localizing objects, especially in difficult recognition tasks such as in cluttered natural scenes.

We describe a specific computational model and relate it to the known functional anatomy of attention. We show that several well-known attentional phenomena – including bottom-up pop-out effects, multiplicative modulation of neuronal tuning curves and shift in contrast responses – all emerge naturally as predictions of the model. We also show that the Bayesian model predicts well human eye fixations (considered as a proxy for shifts of attention) in natural scenes.},
	number = {22},
	urldate = {2015-10-22},
	journal = {Vision Research},
	author = {Chikkerur, Sharat and Serre, Thomas and Tan, Cheston and Poggio, Tomaso},
	month = oct,
	year = {2010},
	keywords = {Attention, Bayesian inference, computational model, object recognition},
	pages = {2233--2247}
}

@article{mahadevan_spatiotemporal_2010,
	title = {Spatiotemporal {Saliency} in {Dynamic} {Scenes}},
	volume = {32},
	issn = {0162-8828},
	doi = {10.1109/TPAMI.2009.112},
	abstract = {A spatiotemporal saliency algorithm based on a center-surround framework is proposed. The algorithm is inspired by biological mechanisms of motion-based perceptual grouping and extends a discriminant formulation of center-surround saliency previously proposed for static imagery. Under this formulation, the saliency of a location is equated to the power of a predefined set of features to discriminate between the visual stimuli in a center and a surround window, centered at that location. The features are spatiotemporal video patches and are modeled as dynamic textures, to achieve a principled joint characterization of the spatial and temporal components of saliency. The combination of discriminant center-surround saliency with the modeling power of dynamic textures yields a robust, versatile, and fully unsupervised spatiotemporal saliency algorithm, applicable to scenes with highly dynamic backgrounds and moving cameras. The related problem of background subtraction is treated as the complement of saliency detection, by classifying nonsalient (with respect to appearance and motion dynamics) points in the visual field as background. The algorithm is tested for background subtraction on challenging sequences, and shown to substantially outperform various state-of-the-art techniques. Quantitatively, its average error rate is almost half that of the closest competitor.},
	number = {1},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Mahadevan, V. and Vasconcelos, N.},
	month = jan,
	year = {2010},
	keywords = {Algorithms, Artificial Intelligence, background subtraction, biological mechanisms, center-surround saliency, discriminant center-surround architecture, dynamic backgrounds, dynamic scenes, dynamic texture, Image motion analysis, Image Processing, Computer-Assisted, Markov Chains, Models, Theoretical, motion-based perceptual grouping, motion saliency, Normal Distribution, object detection, Pattern Recognition, Automated, ROC Curve, saliency detection, Spatiotemporal saliency, spatiotemporal saliency algorithm, spatiotemporal video patches, video modeling., Video recording, video signal processing, Vision, Ocular, Visual Perception},
	pages = {171--177},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\JU57SHKW\\articleDetails.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\2PQX53JZ\\Mahadevan and Vasconcelos - 2010 - Spatiotemporal Saliency in Dynamic Scenes.pdf:application/pdf}
}

@article{song_mesh_2014,
	title = {Mesh {Saliency} via {Spectral} {Processing}},
	volume = {33},
	issn = {0730-0301},
	url = {http://doi.acm.org/10.1145/2530691},
	doi = {10.1145/2530691},
	abstract = {We propose a novel method for detecting mesh saliency, a perceptually-based measure of the importance of a local region on a 3D surface mesh. Our method incorporates global considerations by making use of spectral attributes of the mesh, unlike most existing methods which are typically based on local geometric cues. We first consider the properties of the log-Laplacian spectrum of the mesh. Those frequencies which show differences from expected behaviour capture saliency in the frequency domain. Information about these frequencies is considered in the spatial domain at multiple spatial scales to localise the salient features and give the final salient areas. The effectiveness and robustness of our approach are demonstrated by comparisons to previous approaches on a range of test models. The benefits of the proposed method are further evaluated in applications such as mesh simplification, mesh segmentation, and scan integration, where we show how incorporating mesh saliency can provide improved results.},
	number = {1},
	urldate = {2015-10-22},
	journal = {ACM Trans. Graph.},
	author = {Song, Ran and Liu, Yonghuai and Martin, Ralph R. and Rosin, Paul L.},
	month = feb,
	year = {2014},
	keywords = {Applications, mesh saliency, mesh segmentation, mesh simplification, spectral mesh processing},
	pages = {6:1--6:17},
	file = {ACM Full Text PDF:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\Z8IE25CB\\Song et al. - 2014 - Mesh Saliency via Spectral Processing.pdf:application/pdf}
}

@article{kim_mesh_2010,
	title = {Mesh {Saliency} and {Human} {Eye} {Fixations}},
	volume = {7},
	issn = {1544-3558},
	url = {http://doi.acm.org/10.1145/1670671.1670676},
	doi = {10.1145/1670671.1670676},
	abstract = {Mesh saliency has been proposed as a computational model of perceptual importance for meshes, and it has been used in graphics for abstraction, simplification, segmentation, illumination, rendering, and illustration. Even though this technique is inspired by models of low-level human vision, it has not yet been validated with respect to human performance. Here, we present a user study that compares the previous mesh saliency approaches with human eye movements. To quantify the correlation between mesh saliency and fixation locations for 3D rendered images, we introduce the normalized chance-adjusted saliency by improving the previous chance-adjusted saliency measure. Our results show that the current computational model of mesh saliency can model human eye movements significantly better than a purely random model or a curvature-based model.},
	number = {2},
	urldate = {2015-02-01},
	journal = {ACM Trans. Appl. Percept.},
	author = {Kim, Youngmin and Varshney, Amitabh and Jacobs, David W. and Guimbreti{\`e}re, Fran{\c c}ois},
	month = feb,
	year = {2010},
	keywords = {eye-tracker, mesh saliency, Visual Perception},
	pages = {12:1--12:13}
}

@article{feixas_unified_2009,
	title = {A {Unified} {Information}-theoretic {Framework} for {Viewpoint} {Selection} and {Mesh} {Saliency}},
	volume = {6},
	issn = {1544-3558},
	url = {http://doi.acm.org/10.1145/1462055.1462056},
	doi = {10.1145/1462055.1462056},
	abstract = {Viewpoint selection is an emerging area in computer graphics with applications in fields such as scene exploration, image-based modeling, and volume visualization. In particular, best view selection algorithms are used to obtain the minimum number of views (or images) in order to understand or model an object or scene better. In this article, we present a unified framework for viewpoint selection and mesh saliency based on the definition of an information channel between a set of viewpoints (input) and the set of polygons of an object (output). The mutual information of this channel is shown to be a powerful tool to deal with viewpoint selection, viewpoint stability, object exploration and viewpoint-based saliency. In addition, viewpoint mutual information is extended using saliency as an importance factor, showing how perceptual criteria can be incorporated to our method. Although we use a sphere of viewpoints around an object, our framework is also valid for any set of viewpoints in a closed scene. A number of experiments demonstrate the robustness of our approach and the good behavior of the proposed measures.},
	number = {1},
	urldate = {2015-10-22},
	journal = {ACM Trans. Appl. Percept.},
	author = {Feixas, Miquel and Sbert, Mateu and Gonz{\'a}lez, Francisco},
	month = feb,
	year = {2009},
	keywords = {information theory, mesh saliency, Viewpoint selection, Visual Perception},
	pages = {1:1--1:23}
}

@inproceedings{lee_view_2011,
	title = {View point evaluation and streamline filtering for flow visualization},
	doi = {10.1109/PACIFICVIS.2011.5742376},
	abstract = {Visualization of flow fields with geometric primitives is often challenging due to occlusion that is inevitably introduced by 3D streamlines. In this paper, we present a novel view-dependent algorithm that can minimize occlusion and reveal important flow features for three dimensional flow fields. To analyze regions of higher importance, we utilize Shannon's entropy as a measure of vector complexity. An entropy field in the form of a three dimensional volume is extracted from the input vector field. To utilize this view-independent complexity measure for view-dependent calculations, we introduce the notion of a maximal entropy projection (MEP) framebuffer, which stores maximal entropy values as well as the corresponding depth values for a given viewpoint. With this information, we develop a view-dependent streamline selection algorithm that can evaluate and choose streamlines that will cause minimum occlusion to regions of higher importance. Based on a similar concept, we also propose a viewpoint selection algorithm that works hand-in-hand with our streamline selection algorithm to maximize the visibility of high complexity regions in the flow field.},
	booktitle = {2011 {IEEE} {Pacific} {Visualization} {Symposium} ({PacificVis})},
	author = {Lee, Teng-Yok and Mishchenko, O. and Shen, Han-Wei and Crawfis, R.},
	month = mar,
	year = {2011},
	keywords = {3D streamline, Complexity theory, data visualisation, entropy, entropy field, flow field, flow visualisation, flow visualization, geometric primitive, I.3.3 [Computing Methodologies]: COMPUTER GRAPHICS—Picture/Image Generation, maximal entropy projection framebuffer, maximal entropy values, maximum entropy methods, occlusion, Pixel, Shannon entropy, Streaming media, streamline filtering, Three dimensional displays, three dimensional volume, Tiles, Tornadoes, vector complexity, view-dependent algorithm, view-dependent streamline selection algorithm, view point evaluation, viewpoint selection algorithm},
	pages = {83--90}
}

@article{bramon_multimodal_2012,
	title = {Multimodal {Data} {Fusion} {Based} on {Mutual} {Information}},
	volume = {18},
	issn = {1077-2626},
	doi = {10.1109/TVCG.2011.280},
	abstract = {Multimodal visualization aims at fusing different data sets so that the resulting combination provides more information and understanding to the user. To achieve this aim, we propose a new information-theoretic approach that automatically selects the most informative voxels from two volume data sets. Our fusion criteria are based on the information channel created between the two input data sets that permit us to quantify the information associated with each intensity value. This specific information is obtained from three different ways of decomposing the mutual information of the channel. In addition, an assessment criterion based on the information content of the fused data set can be used to analyze and modify the initial selection of the voxels by weighting the contribution of each data set to the final result. The proposed approach has been integrated in a general framework that allows for the exploration of volumetric data models and the interactive change of some parameters of the fused data set. The proposed approach has been evaluated on different medical data sets with very promising results.},
	number = {9},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Bramon, R. and Boada, I. and Bardera, A. and Rodriguez, J. and Feixas, M. and Puig, J. and Sbert, M.},
	year = {2012},
	keywords = {assessment criterion, Biomedical imaging, content management, data models, data visualisation, data visualization, Image color analysis, image fusion, information channel, information content, information-theoretic approach, information theory, Information theory, informative voxels, intensity value, medical computing, medical data sets, multimodal data visualization, multimodal visualization, mutual information, mutual information., mutual information-based multimodal data fusion, mutual information decomposition, rendering (computer graphics), Rendering (computer graphics), sensor fusion, Transfer functions, transfer functions, volumetric data models exploration},
	pages = {1574--1587},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\2RIV2BDA\\abs_all.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\SC2SAWD5\\Bramon et al. - 2012 - Multimodal Data Fusion Based on Mutual Information.pdf:application/pdf}
}

@ARTICLE{Wang2004SSIM, 
	author={Zhou Wang and A. C. Bovik and H. R. Sheikh and E. P. Simoncelli}, 
	journal={IEEE Transactions on Image Processing}, 
	title={Image quality assessment: from error visibility to structural similarity}, 
	year={2004}, 
	volume={13}, 
	number={4}, 
	pages={600-612}, 
	keywords={data compression;image coding;visual perception;JPEG;JPEG2000;distorted image;error sensitivity;error visibility;human visual perception;human visual system;image compression;image database;perceptual image quality assessment;reference image;structural information;structural similarity index;Data mining;Degradation;Humans;Image quality;Indexes;Layout;Quality assessment;Transform coding;Visual perception;Visual system;Algorithms;Data Interpretation, Statistical;Hypermedia;Image Enhancement;Image Interpretation, Computer-Assisted;Information Storage and Retrieval;Models, Statistical;Pattern Recognition, Automated;Quality Control;Reproducibility of Results;Sensitivity and Specificity;Signal Processing, Computer-Assisted;Subtraction Technique}, 
	doi={10.1109/TIP.2003.819861}, 
	ISSN={1057-7149}, 
	month={April},}

@article{wang_lod_2006,
	title = {{LOD} {Map} - {A} {Visual} {Interface} for {Navigating} {Multiresolution} {Volume} {Visualization}},
	volume = {12},
	issn = {1077-2626},
	doi = {10.1109/TVCG.2006.159},
	abstract = {In multiresolution volume visualization, a visual representation of level-of-detail (LOD) quality is important for us to examine, compare, and validate different LOD selection algorithms. While traditional methods rely on ultimate images for quality measurement, we introduce the LOD map - an alternative representation of LOD quality and a visual interface for navigating multiresolution data exploration. Our measure for LOD quality is based on the formulation of entropy from information theory. The measure takes into account the distortion and contribution of multiresolution data blocks. A LOD map is generated through the mapping of key LOD ingredients to a treemap representation. The ordered treemap layout is used for relative stable update of the LOD map when the view or LOD changes. This visual interface not only indicates the quality of LODs in an intuitive way, but also provides immediate suggestions for possible LOD improvement through visually-striking features. It also allows us to compare different views and perform rendering budget control. A set of interactive techniques is proposed to make the LOD adjustment a simple and easy task. We demonstrate the effectiveness and efficiency of our approach on large scientific and medical data sets},
	number = {5},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Wang, C. and Shen, H.-W.},
	month = oct,
	year = {2006},
	keywords = {Algorithms, Biomedical imaging, Chaos, Computer Graphics, computer graphics, data visualisation, data visualization, Diagnostic Imaging, Distortion measurement, entropy, Entropy, entropy measure, graphical user interfaces, image enhancement, Image Interpretation, Computer-Assisted, Image resolution, Imaging, Three-Dimensional, Information Storage and Retrieval, information theory, Information theory, knowledge representation, large volume visualization Author 1:, level-of-detail quality, LOD map, medical data set, Models, Anatomic, multiresolution rendering, multiresolution volume visualization, Navigation, perceptual reasoning, rendering (computer graphics), Rendering (computer graphics), Signal resolution, treemap representation, trees (mathematics), User-Computer Interface, Visible Human Projects, visual interface, Visual Representation},
	pages = {1029 --1036},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\HNKGTXJU\\abs_all.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\P887ZNR3\\Wang and Shen - 2006 - LOD Map - A Visual Interface for Navigating Multir.pdf:application/pdf}
}

@article{khlebnikov_procedural_2012,
	title = {Procedural {Texture} {Synthesis} for {Zoom}-{Independent} {Visualization} of {Multivariate} {Data}},
	volume = {31},
	copyright = {© 2012 The Author(s) Computer Graphics Forum © 2012 The Eurographics Association and Blackwell Publishing Ltd.},
	issn = {1467-8659},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1467-8659.2012.03127.x/abstract},
	doi = {10.1111/j.1467-8659.2012.03127.x},
	abstract = {Simultaneous visualization of multiple continuous data attributes in a single visualization is a task that is important for many application areas. Unsurprisingly, many methods have been proposed to solve this task. However, the behavior of such methods during the exploration stage, when the user tries to understand the data with panning and zooming, has not been given much attention. In this paper, we propose a method that uses procedural texture synthesis to create zoom-independent visualizations of three scalar data attributes. The method is based on random-phase Gabor noise, whose frequency is adapted for the visualization of the first data attribute. We ensure that the resulting texture frequency lies in the range that is perceived well by the human visual system at any zoom level. To enhance the perception of this attribute, we also apply a specially constructed transfer function that is based on statistical properties of the noise. Additionally, the transfer function is constructed in a way that it does not introduce any aliasing to the texture. We map the second attribute to the texture orientation. The third attribute is color coded and combined with the texture by modifying the value component of the HSV color model. The necessary contrast needed for texture and color perception was determined in a user study. In addition, we conducted a second user study that shows significant advantages of our method over current methods with similar goals. We believe that our method is an important step towards creating methods that not only succeed in visualizing multiple data attributes, but also adapt to the behavior of the user during the data exploration stage.},
	language = {en},
	number = {3pt4},
	urldate = {2015-10-23},
	journal = {Computer Graphics Forum},
	author = {Khlebnikov, R. and Kainz, B. and Steinberger, M. and Streit, M. and Schmalstieg, D.},
	month = jun,
	year = {2012},
	keywords = {I.3.3 [Computer Graphics]: Picture/Image Generation—Display algorithms},
	pages = {1355--1364}
}

@article{pinto_volume_2008,
	title = {Volume visualization and exploration through flexible transfer function design},
	volume = {32},
	issn = {0097-8493},
	url = {http://www.sciencedirect.com/science/article/pii/S0097849308000496},
	doi = {10.1016/j.cag.2008.04.004},
	abstract = {Direct volume rendering (DVR) is a well-known method for exploring volumetric data sets. Optical properties are assigned to the volume data and then a DVR algorithm produces visualizations by sampling volume elements and projecting them into the image plane. The mapping from voxel values to optical attribute values is known as transfer function (TF). Therefore, the quality of a visualization is highly dependent on the TF employed, but its specification is a non-trivial and unintuitive task. Without any help during the TF design process, the user goes through a frustrating and time-consuming trial-and-error cycle. This paper presents a useful combination of TF design techniques in an interactive workspace for volume visualization. Our strategy relies on semi-automatic TFs generation methods: boundary emphasis, stochastic evolutive search in TF space, and manual TF specification aided by dual domain interaction. A two-level user interface was also developed. In the first level, it provides multiple simultaneous interactive visualizations of the volume data using different TFs, while in the second one, a detailed visualization of a single TF and the respective rendered volume are displayed. Moreover, in the second level, the TF can be manually refined and the volume can be further inspected through geometric tools. The techniques combined in this work are complementary, allowing easy and fast TF design and data exploration.},
	number = {4},
	urldate = {2015-10-23},
	journal = {Computers \& Graphics},
	author = {Pinto, Francisco de M. and Freitas, Carla M. D. S.},
	month = aug,
	year = {2008},
	keywords = {direct volume rendering, transfer function, Volume data, Volume visualization},
	pages = {420--429},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\KDRI4599\\Pinto and Freitas - 2008 - Volume visualization and exploration through flexi.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\3QJHWSKF\\S0097849308000496.html:text/html}
}

@article{kim_dimensionality_2010,
	title = {Dimensionality {Reduction} on {Multi}-{Dimensional} {Transfer} {Functions} for {Multi}-{Channel} {Volume} {Data} {Sets}},
	volume = {9},
	issn = {1473-8716},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3153355/},
	doi = {10.1057/ivs.2010.6},
	abstract = {The design of transfer functions for volume rendering is a non-trivial task. This is particularly true for multi-channel data sets, where multiple data values exist for each voxel, which requires multi-dimensional transfer functions. In this paper, we propose a new method for multi-dimensional transfer function design. Our new method provides a framework to combine multiple computational approaches and pushes the boundary of gradient-based multi-dimensional transfer functions to multiple channels, while keeping the dimensionality of transfer functions at a manageable level, i.e., a maximum of three dimensions, which can be displayed visually in a straightforward way. Our approach utilizes channel intensity, gradient, curvature and texture properties of each voxel. Applying recently developed nonlinear dimensionality reduction algorithms reduces the high-dimensional data of the domain. In this paper, we use Isomap and Locally Linear Embedding as well as a traditional algorithm, Principle Component Analysis. Our results show that these dimensionality reduction algorithms significantly improve the transfer function design process without compromising visualization accuracy. We demonstrate the effectiveness of our new dimensionality reduction algorithms with two volumetric confocal microscopy data sets.},
	number = {3},
	urldate = {2015-05-08},
	journal = {Information visualization},
	author = {Kim, Han Suk and Schulze, J{\"u}rgen P. and Cone, Angela C. and Sosinsky, Gina E. and Martone, Maryann E.},
	month = sep,
	year = {2010},
	pmid = {21841914},
	pmcid = {PMC3153355},
	pages = {167--180}
}

@article{glaser_automatic_2010,
	title = {Automatic {Transfer} {Function} {Specification} for {Visual} {Emphasis} of {Coronary} {Artery} {Plaque}},
	volume = {29},
	copyright = {© 2010 The Authors Journal compilation © 2010 The Eurographics Association and Blackwell Publishing Ltd.},
	issn = {1467-8659},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1467-8659.2009.01590.x/abstract},
	doi = {10.1111/j.1467-8659.2009.01590.x},
	abstract = {Cardiovascular imaging with current multislice spiral computed tomography (MSCT) technology enables a non-invasive evaluation of the coronary arteries. Contrast-enhanced MSCT angiography with high spatial resolution allows for a segmentation of the coronary artery tree. We present an automatically adapted transfer function (TF) specification to highlight pathologic changes of the vessel wall based on the segmentation result of the coronary artery tree. The TFs are combined with common visualization techniques, such as multiplanar reformation and direct volume rendering for the evaluation of coronary arteries in MSCT image data. The presented TF-based mapping of CT values in Hounsfield Units (HU) to color and opacity leads to a different color coding for different plaque types. To account for varying HU values of the vessel lumen caused by the contrast medium, the TFs are adapted to each dataset by local histogram analysis. We describe an informal evaluation with three board-certified radiologists which indicates that the represented visualizations guide the user's attention to pathologic changes of the vessel wall as well as provide an overview about spatial variations.},
	language = {en},
	number = {1},
	urldate = {2015-10-23},
	journal = {Computer Graphics Forum},
	author = {Gla{\ss}er, S. and Oeltze, S. and Hennemuth, A. and Kubisch, C. and Mahnken, A. and Wilhelmsen, S. and Preim, B.},
	month = mar,
	year = {2010},
	keywords = {I.3.3 [Computer Graphics]: Display algorithms, transfer function, vessel visualization, Volume rendering},
	pages = {191--201}
}

@inproceedings{haidacher_volume_2010,
	title = {Volume {Visualization} based on {Statistical} {Transfer}-{Function} {Spaces}},
	url = {http://www.cg.tuwien.ac.at/research/publications/2010/haidacher_2010_statTF/},
	abstract = {It is a difficult task to design transfer functions for noisy data. In traditional transfer-function spaces, data values of different materials overlap. In this paper we introduce a novel statistical transfer-function space which in the presence of noise, separates different materials in volume data sets. Our method adaptively estimates statistical properties, i.e. the mean value and the standard deviation, of the data values in the neighborhood of each sample point. These properties are used to define a transfer-function space which enables the distinction of different materials. Additionally, we present a novel approach for interacting with our new transfer-function space which enables the design of transfer functions based on statistical properties. Furthermore, we demonstrate that statistical information can be applied to enhance visual appearance in the rendering process. We compare the new method with 1D, 2D, and LH transfer functions to demonstrate its usefulness.},
	booktitle = {Proceedings of the {IEEE} {Pacific} {Visualization} 2010},
	author = {Haidacher, Martin and Patel, Daniel and Bruckner, Stefan and Kanitsar, Armin and Gr{\"o}ller, Meister Eduard},
	month = mar,
	year = {2010},
	keywords = {Algorithm design and analysis, Classification, classification, classification, Computer graphics, data visualisation, data visualization, Medical services, Noise measurement, noisy data, noisy data, rendering (computer graphics), Rendering (computer graphics), rendering process, shading, shading, Space technology, statistical analysis, statistical transfer-function space, statistics, statistics, transfer function, transfer function, Transfer functions, transfer functions, volume visualization, White noise},
	pages = {17--24}
}

@article{kniss_model_2003,
	title = {A {Model} for {Volume} {Lighting} and {Modeling}},
	volume = {9},
	issn = {1077-2626},
	url = {http://dx.doi.org/10.1109/TVCG.2003.1196003},
	doi = {10.1109/TVCG.2003.1196003},
	number = {2},
	urldate = {2015-10-24},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Kniss, Joe and Premoze, Simon and Hansen, Charles and Shirley, Peter and McPherson, Allen},
	month = apr,
	year = {2003},
	keywords = {Clouds, fur, procedural modeling, shading model, volume modeling, volume perturbation., Volume rendering},
	pages = {150--162}
}

@article{polyak_conjugate_1969,
	title = {The conjugate gradient method in extremal problems},
	volume = {9},
	issn = {0041-5553},
	url = {http://www.sciencedirect.com/science/article/pii/0041555369900354},
	doi = {10.1016/0041-5553(69)90035-4},
	abstract = {THE conjugate gradient method was first described in [1, 2] for solving sets of linear algebraic equations. The method, being iterative in form, has all the merits of iterative methods, and enables a set of linear equations to be solved (or what amounts to the same thing, the minimum of a quadratic functional in finite-dimensional space to be found) after a finite number of steps. The method was later extended to the case of Hilbert space [3–5], and to the case of non-quadratic functionals [6, 7].

The present paper proves the convergence of the method as applied to non-quadratic functionals, describes its extension to constrained problems, considers means for further accelerating the convergence, and describes experience in the practical application of the method for solving a variety of extremal problems.},
	number = {4},
	urldate = {2015-10-24},
	journal = {USSR Computational Mathematics and Mathematical Physics},
	author = {Polyak, B. T.},
	year = {1969},
	pages = {94--112}
}

@article{beyer_state---art_2015,
	title = {State-of-the-{Art} in {GPU}-{Based} {Large}-{Scale} {Volume} {Visualization}},
	volume = {34},
	issn = {1467-8659},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/cgf.12605/abstract},
	doi = {10.1111/cgf.12605},
	abstract = {This survey gives an overview of the current state of the art in GPU techniques for interactive large-scale volume visualization. Modern techniques in this field have brought about a sea change in how interactive visualization and analysis of giga-, tera- and petabytes of volume data can be enabled on GPUs. In addition to combining the parallel processing power of GPUs with out-of-core methods and data streaming, a major enabler for interactivity is making both the computational and the visualization effort proportional to the amount and resolution of data that is actually visible on screen, i.e. ‘output-sensitive’ algorithms and system designs. This leads to recent output-sensitive approaches that are ‘ray-guided’, ‘visualization-driven’ or ‘display-aware’. In this survey, we focus on these characteristics and propose a new categorization of GPU-based large-scale volume visualization techniques based on the notions of actual output-resolution visibility and the current working set of volume bricks—the current subset of data that is minimally required to produce an output image of the desired display resolution. Furthermore, we discuss the differences and similarities of different rendering and data traversal strategies in volume rendering by putting them into a common context—the notion of address translation. For our purposes here, we view parallel (distributed) visualization using clusters as an orthogonal set of techniques that we do not discuss in detail but that can be used in conjunction with what we present in this survey.},
	language = {en},
	number = {8},
	urldate = {2016-08-26},
	journal = {Computer Graphics Forum},
	author = {Beyer, Johanna and Hadwiger, Markus and Pfister, Hanspeter},
	month = dec,
	year = {2015},
	keywords = {GPU, I.3.6 [Computer Graphics]: Methodology and Techniques—I.3.3 [Computer Graphics]: Picture/Image Generation—Display algorithms, Large Data, ray-guided, STAR, Volume rendering},
	pages = {13--37},
	file = {Full Text PDF:C\:\\Users\\lazaro\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\9kdnyu8g.default\\zotero\\storage\\W9UR5T34\\Beyer et al. - 2015 - State-of-the-Art in GPU-Based Large-Scale Volume V.pdf:application/pdf;Snapshot:C\:\\Users\\lazaro\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\9kdnyu8g.default\\zotero\\storage\\HRASX47H\\abstract.html:text/html}
}

@inproceedings{kniss_gaussian_2003,
	title = {Gaussian transfer functions for multi-field volume visualization},
	doi = {10.1109/VISUAL.2003.1250412},
	abstract = {Volume rendering is a flexible technique for visualizing dense 3D volumetric datasets. A central element of volume rendering is the conversion between data values and observable quantities such as color and opacity. This process is usually realized through the use of transfer functions that are precomputed and stored in lookup tables. For multidimensional transfer functions applied to multivariate data, these lookup tables become prohibitively large. We propose the direct evaluation of a particular type of transfer functions based on a sum of Gaussians. Because of their simple form (in terms of number of parameters), these functions and their analytic integrals along line segments can be evaluated efficiently on current graphics hardware, obviating the need for precomputed lookup tables. We have adopted these transfer functions because they are well suited for classification based on a unique combination of multiple data values that localize features in the transfer function domain. We apply this technique to the visualization of several multivariate datasets (CT, cryosection) that are difficult to classify and render accurately at interactive rates using traditional approaches.},
	booktitle = {{IEEE} {Visualization}, 2003. {VIS} 2003},
	author = {Kniss, J. and premoze, S. and Ikits, M. and Lefohn, A. and Hansen, C. and Praun, E.},
	month = oct,
	year = {2003},
	keywords = {3D volumetric datasets, Computed tomography, Computer Graphics, cryosection, CT, data visualisation, data visualization, Gaussian distribution, Gaussian transfer functions, Geometrical optics, graphics hardware, Hardware, Multidimensional systems, multidimensional transfer function, multifield visualization, multivariate data, realistic images, rendering (computer graphics), Scientific computing, table lookup, Transfer functions, Volume rendering, Volume visualization},
	pages = {497--504},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\IDAS293S\\articleDetails.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\QX653TK8\\Kniss et al. - 2003 - Gaussian transfer functions for multi-field volume.pdf:application/pdf}
}

@article{moreland_survey_2013,
	title = {A {Survey} of {Visualization} {Pipelines}},
	volume = {19},
	issn = {1077-2626},
	doi = {10.1109/TVCG.2012.133},
	abstract = {The most common abstraction used by visualization libraries and applications today is what is known as the visualization pipeline. The visualization pipeline provides a mechanism to encapsulate algorithms and then couple them together in a variety of ways. The visualization pipeline has been in existence for over 20 years, and over this time many variations and improvements have been proposed. This paper provides a literature review of the most prevalent features of visualization pipelines and some of the most recent research directions.},
	number = {3},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Moreland, K.},
	month = mar,
	year = {2013},
	keywords = {abstraction, Algorithms, central control, Centralized control, Computer Graphics, dataflow networks, data models, data parallelism, data visualisation, data visualization, demand driven, Distributed control, domain specific languages, event driven, functional field model, hybrid parallel, image enhancement, Image Interpretation, Computer-Assisted, Imaging, Three-Dimensional, Information Storage and Retrieval, in situ visualization, Libraries, MapReduce, out-of-core streaming, parallel visualization, Pipelilnes, pipeline contracts, pipeline executive, pipeline parallelism, prioritized streaming, provenance, pull model, push model, query-driven visualization, rendering, rendering (computer graphics), Reproducibility of Results, Scheduling, Sensitivity and Specificity, Software, task parallelism, temporal visualization, User-Computer Interface, visualization libraries, visualization pipelines},
	pages = {367--378},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\VZAK4RN9\\articleDetails.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\JWXXZ4TG\\Moreland - 2013 - A Survey of Visualization Pipelines.pdf:application/pdf}
}

@article{yu_hierarchical_2012,
	title = {Hierarchical {Streamline} {Bundles}},
	volume = {18},
	issn = {1077-2626},
	doi = {10.1109/TVCG.2011.155},
	abstract = {Effective 3D streamline placement and visualization play an essential role in many science and engineering disciplines. The main challenge for effective streamline visualization lies in seed placement, i.e., where to drop seeds and how many seeds should be placed. Seeding too many or too few streamlines may not reveal flow features and patterns either because it easily leads to visual clutter in rendering or it conveys little information about the flow field. Not only does the number of streamlines placed matter, their spatial relationships also play a key role in understanding the flow field. Therefore, effective flow visualization requires the streamlines to be placed in the right place and in the right amount. This paper introduces hierarchical streamline bundles, a novel approach to simplifying and visualizing 3D flow fields defined on regular grids. By placing seeds and generating streamlines according to flow saliency, we produce a set of streamlines that captures important flow features near critical points without enforcing the dense seeding condition. We group spatially neighboring and geometrically similar streamlines to construct a hierarchy from which we extract streamline bundles at different levels of detail. Streamline bundles highlight multiscale flow features and patterns through clustered yet not cluttered display. This selective visualization strategy effectively reduces visual clutter while accentuating visual foci, and therefore is able to convey the desired insight into the flow data.},
	number = {8},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Yu, Hongfeng and Wang, Chaoli and Shene, Ching-Kuang and Chen, J.H.},
	month = aug,
	year = {2012},
	keywords = {3D flow field visualization, 3D streamline placement, 3D streamline visualization, Clustering algorithms, critical points, data visualization, diffusion tensor imaging, feature extraction, flow data, flow saliency, flow visualisation, flow visualization., geometrically similar streamlines, hierarchical clustering, hierarchical streamline bundles, level-of-detail, multiscale flow features, multiscale flow patterns, pattern clustering, pattern formation, rendering, rendering (computer graphics), seed placement, spatially neighboring streamlines, spatial relationships, Streaming media, streamline bundle extraction, Streamline bundles, streamline seeding, Three dimensional displays, visual clutter reduction, visual foci accentuation, Visualization},
	pages = {1353--1367},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\IE6QEBFW\\articleDetails.html:text/html}
}

@article{kim_spatiotemporal_2011,
	title = {Spatiotemporal {Saliency} {Detection} and {Its} {Applications} in {Static} and {Dynamic} {Scenes}},
	volume = {21},
	issn = {1051-8215},
	doi = {10.1109/TCSVT.2011.2125450},
	abstract = {This paper presents a novel method for detecting salient regions in both images and videos based on a discriminant center-surround hypothesis that the salient region stands out from its surroundings. To this end, our spatiotemporal approach combines the spatial saliency by computing distances between ordinal signatures of edge and color orientations obtained from the center and the surrounding regions and the temporal saliency by simply computing the sum of absolute difference between temporal gradients of the center and the surrounding regions. Our proposed method is computationally efficient, reliable, and simple to implement and thus it can be easily extended to various applications such as image retargeting and moving object extraction. The proposed method has been extensively tested and the results show that the proposed scheme is effective in detecting saliency compared to various state-of-the-art methods.},
	number = {4},
	journal = {IEEE Transactions on Circuits and Systems for Video Technology},
	author = {Kim, Wonjun and Jung, Chanho and Kim, Changick},
	month = apr,
	year = {2011},
	keywords = {discriminant center-surround hypothesis, dynamic scenes, edge detection, Histograms, Image color analysis, image colour analysis, image detection, Image edge detection, Materials, moving object extraction, ordinal signature, Pixel, salient regions, spatiotemporal approach, Spatiotemporal phenomena, spatiotemporal saliency detection, static scenes, video detection, videos, visual attention},
	pages = {446--456},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\R2A3SKD7\\articleDetails.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\AWC8WI8E\\Kim et al. - 2011 - Spatiotemporal Saliency Detection and Its Applicat.pdf:application/pdf}
}

@inproceedings{kim_saliency_2010,
	title = {Saliency detection: {A} self-ordinal resemblance approach},
	shorttitle = {Saliency detection},
	doi = {10.1109/ICME.2010.5583287},
	abstract = {In saliency detection, regions attracting visual attention need to be highlighted while effectively suppressing non-salient regions for the semantic scene understanding. However, most previous methods tend to fail in suppressing highly textured backgrounds and also high contrast edges belonging to the non-salient regions. To address this problem, we propose a method for detecting salient regions based on a self-ordinal resemblance measure (SORM). Our saliency map is defined by using the center-surround computations based on the ordinal signatures obtained from local regions centered at each pixel. It can be regarded as an energy map and thus extended to image retargeting. Our approach is fully automatic and nonparametric. To justify robustness of our approach, the proposed method is compared with the state of the art methods on various images.},
	booktitle = {2010 {IEEE} {International} {Conference} on {Multimedia} and {Expo} ({ICME})},
	author = {Kim, Wonjun and Jung, Chanho and Kim, Changick},
	month = jul,
	year = {2010},
	keywords = {edge detection, energy map, high contrast edge, highly textured background, Image color analysis, Image edge detection, image retargeting, image texture, Noise, Noise measurement, Pixel, Robustness, saliency detection, self-ordinal resemblance, self-ordinal resemblance measure, semantic scene understanding, visual attention, Visualization},
	pages = {1260--1265},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\HF46UR93\\abs_all.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\BIEJ5QVA\\Kim et al. - 2010 - Saliency detection A self-ordinal resemblance app.pdf:application/pdf}
}

@article{benard_state---art_2011,
	title = {State-of-the-{Art} {Report} on {Temporal} {Coherence} for {Stylized} {Animations}},
	volume = {30},
	issn = {1467-8659},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1467-8659.2011.02075.x/abstract},
	doi = {10.1111/j.1467-8659.2011.02075.x},
	abstract = {Non-photorealistic rendering (NPR) algorithms allow the creation of images in a variety of styles, ranging from line drawing and pen-and-ink to oil painting and watercolour. These algorithms provide greater flexibility, control and automation over traditional drawing and painting. Despite significant progress over the past 15 years, the application of NPR to the generation of stylized animations remains an active area of research. The main challenge of computer-generated stylized animations is to reproduce the look of traditional drawings and paintings while minimizing distracting flickering and sliding artefacts present in hand-drawn animations. These goals are inherently conflicting and any attempt to address the temporal coherence of stylized animations is a trade-off. This state-of-the-art report is motivated by the growing number of methods proposed in recent years and the need for a comprehensive analysis of the trade-offs they propose. We formalize the problem of temporal coherence in terms of goals and compare existing methods accordingly. We propose an analysis for both line and region stylization methods and discuss initial steps towards their perceptual evaluation. The goal of our report is to help uninformed readers to choose the method that best suits their needs, as well as motivate further research to address the limitations of existing methods.},
	language = {en},
	number = {8},
	urldate = {2016-09-05},
	journal = {Computer Graphics Forum},
	author = {B{\'e}nard, Pierre and Bousseau, Adrien and Thollot, Jo{\"e}lle},
	month = dec,
	year = {2011},
	keywords = {I.3.3 [Computer Graphics], non-photorealistic rendering, Picture/Image Generation—I.3.7 [Computer Graphics], stylization, temporal coherence, Three-Dimensional Graphics and Realism},
	pages = {2367--2386},
	file = {Full Text PDF:C\:\\Users\\lazaro\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\9kdnyu8g.default\\zotero\\storage\\KFMI5CC3\\Bénard et al. - 2011 - State-of-the-Art Report on Temporal Coherence for .pdf:application/pdf;Snapshot:C\:\\Users\\lazaro\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\9kdnyu8g.default\\zotero\\storage\\XVV66WEG\\abstract.html:text/html}
}

@inproceedings{sereda_automating_2006,
	address = {Aire-la-Ville, Switzerland, Switzerland},
	series = {{EUROVIS}'06},
	title = {Automating {Transfer} {Function} {Design} for {Volume} {Rendering} {Using} {Hierarchical} {Clustering} of {Material} {Boundaries}},
	isbn = {3-905673-31-2},
	url = {http://dx.doi.org/10.2312/VisSym/EuroVis06/243-250},
	doi = {10.2312/VisSym/EuroVis06/243-250},
	abstract = {Transfer function design plays a crucial role in direct volume rendering. Furthermore, it has a major influence on the efficiency of the visualization process. We have developed a framework that facilitates the semi-automatic design of transfer functions. Similarly to other approaches we generate clusters in the transfer function domain. We created a real-time interaction with a hierarchy of clusters. This interaction effectively substitutes cumbersome settings of clustering thresholds. Our framework is also able to easily combine different clustering criteria. We have developed two similarity measures for clustering of material boundaries. One is based on the similarity of the boundaries in the transfer function domain and the other on their spatial relation. We use the LH space as the transfer function domain. This space facilitates the clustering of material boundaries. We demonstrate our approach on several examples.},
	urldate = {2015-10-24},
	booktitle = {Proceedings of the {Eighth} {Joint} {Eurographics} / {IEEE} {VGTC} {Conference} on {Visualization}},
	publisher = {Eurographics Association},
	author = {{\v S}ereda, Petr and Vilanova, Anna and Gerritsen, Frans A.},
	year = {2006},
	pages = {243--250}
}

@article{sereda_visualization_2006,
	title = {Visualization of boundaries in volumetric data sets using {LH} histograms},
	volume = {12},
	issn = {1077-2626},
	doi = {10.1109/TVCG.2006.39},
	abstract = {A crucial step in volume rendering is the design of transfer functions that highlights those aspects of the volume data that are of interest to the user. For many applications, boundaries carry most of the relevant information. Reliable detection of boundaries is often hampered by limitations of the imaging process, such as blurring and noise. We present a method to identify the materials that form the boundaries. These materials are then used in a new domain that facilitates interactive and semiautomatic design of appropriate transfer functions. We also show how the obtained boundary information can be used in region-growing-based segmentation.},
	number = {2},
	journal = {{IEEE} Transactions on Visualization and Computer Graphics},
	author = {{\v S}ereda, P. and Bartrol{\'i}, {A.V.} and Serlie, {I.W.O.} and Gerritsen, {F.A.}},
	month = apr,
	year = {2006},
	keywords = {Algorithms, boundary detection, Computed tomography, Computer Graphics, Computer Simulation, Data Interpretation, Statistical, data visualisation, data visualization, Direct volume rendering, Histograms, image enhancement, Image Interpretation, Computer-Assisted, image reconstruction, image segmentation, Imaging, Three-Dimensional, Information Storage and Retrieval, interactive design, {LH} histogram, Medical diagnostic imaging, Models, Statistical, Multidimensional systems, multidimensional transfer functions, Numerical Analysis, Computer-Assisted, Optical imaging, region growing., rendering (computer graphics), semiautomatic design, Shape, Signal Processing, Computer-Assisted, transfer function, transfer functions, User-Computer Interface, volume rendering, Volume Visualization},
	pages = {208--218},
	file = {01580455.pdf:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\BQ3DKI6G\01580455.pdf:application/pdf;IEEE Xplore Abstract Record:C:\Users\Joe\AppData\Roaming\Zotero\Zotero\Profiles\3r6072id.default\zotero\storage\PKPHVGRS\abs_all.html:text/html}
}

@article{silva_survey_2005,
	title = {A survey of {GPU}-{Based} volume rendering of unstructured grids},
	issn = {0103-4308},
	url = {http://www.lume.ufrgs.br/handle/10183/20016},
	abstract = {A visualização volumétrica de grandes malhas não estruturadas é uma das principais metas da comunidade de visualização científica. Enquanto que em grades regulares o uso de técnicas baseadas em textura são adequadas para as Unidades de Processamento Gráfico (GPUs) atuais, os passos necessários para exibir malhas não estruturas não são diretamente mapeadas para o hardware atual. Este artigo revisa algoritmos e técnicas de visualização volumétrica que exploram GPUs de alta performance. São discutidos tanto os algoritmos como seus detalhes de implementação, incluindo as principais dificuldades das abordagens atuais.},
	language = {eng},
	urldate = {2016-09-05},
	author = {Silva, Cl{\'a}udio Teixeira and Comba, Joao Luiz Dihl and Callahan, Steven Paul and Bernardon, Fabio Fedrizzi},
	year = {2005},
	file = {Full Text PDF:C\:\\Users\\lazaro\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\9kdnyu8g.default\\zotero\\storage\\7BEFRF28\\Silva et al. - 2005 - A survey of GPU-Based volume rendering of unstruct.pdf:application/pdf}
}

@phdthesis{corcoran_enhancing_2013,
	address = {Dublin, Ireland},
	type = {{PhD} {Thesis}},
	title = {Enhancing real-time focus and context direct volume rendering},
	school = {Trinity College Dublin},
	author = {Corcoran, Andrew},
	year = {2013},
	keywords = {Computer Science \& Statistics, Ph.D, Dissertations, Ph.D. thesis Trinity College Dublin, Trinity College (Dublin, Ireland)},
	file = {Corcoran-final-phd.pdf:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\ICXFUC4M\\Corcoran-final-phd.pdf:application/pdf}
}

@inproceedings{lorensen_marching_1987,
	address = {New York, NY, USA},
	series = {{SIGGRAPH} '87},
	title = {Marching {Cubes}: {A} {High} {Resolution} 3D {Surface} {Construction} {Algorithm}},
	isbn = {0-89791-227-6},
	shorttitle = {Marching {Cubes}},
	url = {http://doi.acm.org/10.1145/37401.37422},
	doi = {10.1145/37401.37422},
	abstract = {We present a new algorithm, called marching cubes, that creates triangle models of constant density surfaces from 3D medical data. Using a divide-and-conquer approach to generate inter-slice connectivity, we create a case table that defines triangle topology. The algorithm processes the 3D medical data in scan-line order and calculates triangle vertices using linear interpolation. We find the gradient of the original data, normalize it, and use it as a basis for shading the models. The detail in images produced from the generated surface models is the result of maintaining the inter-slice connectivity, surface data, and gradient information present in the original 3D data. Results from computed tomography (CT), magnetic resonance (MR), and single-photon emission computed tomography (SPECT) illustrate the quality and functionality of marching cubes. We also discuss improvements that decrease processing time and add solid modeling capabilities.},
	urldate = {2015-10-27},
	booktitle = {Proceedings of the 14th {Annual} {Conference} on {Computer} {Graphics} and {Interactive} {Techniques}},
	publisher = {ACM},
	author = {Lorensen, William E. and Cline, Harvey E.},
	year = {1987},
	pages = {163--169},
	file = {ACM Full Text PDF:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\VD24FARB\\Lorensen and Cline - 1987 - Marching Cubes A High Resolution 3D Surface Const.pdf:application/pdf}
}

@inproceedings{stegmaier_simple_2005,
	title = {A simple and flexible volume rendering framework for graphics-hardware-based raycasting},
	doi = {10.1109/VG.2005.194114},
	abstract = {In this work we present a flexible framework for GPU-based volume rendering. The framework is based on a single pass volume raycasting approach and is easily extensible in terms of new shader functionality. We demonstrate the flexibility of our system by means of a number of high-quality standard and nonstandard volume rendering techniques. Our implementation shows a promising performance in a number of benchmarks while producing images of higher accuracy than obtained by standard pre-integrated slice-based volume rendering.},
	booktitle = {Fourth {International} {Workshop} on {Volume} {Graphics}, 2005},
	author = {Stegmaier, S. and Strengert, M. and Klein, T. and Ertl, T.},
	month = jun,
	year = {2005},
	keywords = {Biomedical imaging, Carbon capture and storage, Computer Graphics, Data engineering, data visualisation, Geometry, GPU-based volume rendering, graphics-hardware-based raycasting, Hardware, interactive systems, Isosurfaces, ray tracing, rendering (computer graphics), shader functionality, single pass volume raycasting, Visualization, volume rendering framework},
	pages = {187--241},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\FA97J97Z\\abs_all.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\937PN39B\\Stegmaier et al. - 2005 - A simple and flexible volume rendering framework f.pdf:application/pdf}
}

@inproceedings{hadwiger_advanced_2009,
	address = {New York, NY, USA},
	series = {{SIGGRAPH} '09},
	title = {Advanced illumination techniques for {GPU}-based volume raycasting},
	url = {http://doi.acm.org/10.1145/1667239.1667241},
	doi = {10.1145/1667239.1667241},
	abstract = {Volume raycasting techniques are important for both visual arts and visualization. They allow efficient generation of visual effects and visualization of scientific data obtained by tomography or numerical simulation. Volume-rendering techniques are also effective for direct rendering of implicit surfaces used for soft-body animation and constructive solid geometry. The focus of this course is on volumetric illumination techniques that approximate physically based light transport in participating media. Such techniques include interactive implementation of soft and hard shadows, ambient occlusion, and simple Monte Carlo-based approaches to global illumination, including translucency and scattering.},
	urldate = {2013-05-07},
	booktitle = {{ACM} {SIGGRAPH} 2009 {Courses}},
	publisher = {ACM},
	author = {Hadwiger, Markus and Ljung, Patric and Salama, Christof Rezk and Ropinski, Timo},
	year = {2009},
	pages = {2:1--2:166},
	file = {ACM Full Text PDF:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\AEMMTUT2\\Hadwiger et al. - 2009 - Advanced illumination techniques for GPU-based vol.pdf:application/pdf;ACM Full Text PDF:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\Q4IFH2W8\\Hadwiger et al. - 2009 - Advanced illumination techniques for GPU-based vol.pdf:application/pdf}
}

@inproceedings{westermann_accelerated_2001,
	title = {Accelerated volume ray-casting using texture mapping},
	doi = {10.1109/VISUAL.2001.964521},
	abstract = {Acceleration techniques for volume ray-casting are primarily based on pre-computed data structures that allow one to efficiently traverse empty or homogeneous regions. In order to display volume data that successively undergoes color lookups, however, the data structures have to be re-built continuously. In this paper we propose a technique that circumvents this drawback using hardware accelerated texture mapping. In a first rendering pass we employ graphics hardware to interactively determine for each ray where the material is hit. In a second pass ray-casting is performed, but ray traversal starts right in front of the previously determined regions. The algorithm enables interactive classification and it considerably accelerates the view dependent display of selected materials and surfaces from volume data. In contrast to other techniques that are solely based on texture mapping our approach requires less memory and accurately performs the composition of material contributions along the ray.},
	booktitle = {Visualization, 2001. {VIS} '01. {Proceedings}},
	author = {Westermann, R. and Sevenich, B.},
	month = oct,
	year = {2001},
	keywords = {accelerated volume ray-casting, Acceleration, Classification algorithms, color lookups, Composite materials, Computational complexity, Computer displays, Computer Graphics, Data structures, data visualisation, data visualization, graphics hardware, Hardware, Interactive Classification, interactive systems, pre-computed data structures, rendering (computer graphics), Surface fitting, Texture Mapping},
	pages = {271--278},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\4W5ECQR8\\articleDetails.html:text/html}
}

@inproceedings{lacroute_fast_1994,
	address = {New York, NY, USA},
	series = {{SIGGRAPH} '94},
	title = {Fast {Volume} {Rendering} {Using} a {Shear}-warp {Factorization} of the {Viewing} {Transformation}},
	isbn = {0-89791-667-0},
	url = {http://doi.acm.org/10.1145/192161.192283},
	doi = {10.1145/192161.192283},
	abstract = {Several existing volume rendering algorithms operate by factoring the viewing transformation into a 3D shear parallel to the data slices, a projection to form an intermediate but distorted image, and a 2D warp to form an undistorted final image. We extend this class of algorithms in three ways. First, we describe a new object-order rendering algorithm based on the factorization that is significantly faster than published algorithms with minimal loss of image quality. Shear-warp factorizations have the property that rows of voxels in the volume are aligned with rows of pixels in the intermediate image. We use this fact to construct a scanline-based algorithm that traverses the volume and the intermediate image in synchrony, taking advantage of the spatial coherence present in both. We  use spatial data structures based on run-length encoding for both the volume and the intermediate image. Our implementation running on an SGI Indigo workstation renders a 2563 voxel medical data set in one second. Our second extension is a shear-warp factorization for perspective viewing transformations, and we show how our rendering algorithm can support this extension. Third, we introduce a data structure for encoding spatial coherence in unclassified volumes (i.e. scalar fields with no precomputed opacity). When combined with our shear-warp rendering algorithm this data structure allows us to classify and render a 2563 voxel volume in three seconds. The method extends to support mixed volumes and geometry and is parallelizable.},
	urldate = {2015-10-27},
	booktitle = {Proceedings of the 21st {Annual} {Conference} on {Computer} {Graphics} and {Interactive} {Techniques}},
	publisher = {ACM},
	author = {Lacroute, Philippe and Levoy, Marc},
	year = {1994},
	keywords = {Coherence, Medical Imaging, scientific visualization, Volume rendering},
	pages = {451--458},
	file = {ACM Full Text PDF:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\ZDKNFSKB\\Lacroute and Levoy - 1994 - Fast Volume Rendering Using a Shear-warp Factoriza.pdf:application/pdf}
}

@phdthesis{westover_splatting:_1991,
	address = {Chapel Hill, NC, USA},
	title = {Splatting: {A} {Parallel}, {Feed}-forward {Volume} {Rendering} {Algorithm}},
	shorttitle = {Splatting},
	school = {University of North Carolina at Chapel Hill},
	author = {Westover, Lee Alan},
	year = {1991},
	note = {UMI Order No. GAX92-08005}
}

@inproceedings{mueller_splatting_1999,
	address = {Los Alamitos, CA, USA},
	series = {{VIS} '99},
	title = {Splatting {Without} the {Blur}},
	isbn = {0-7803-5897-X},
	url = {http://dl.acm.org/citation.cfm?id=319351.319433},
	abstract = {Splatting is a volume rendering algorithm that combines efficient volume projection with a sparse data representation: Only voxels that have values inside the iso-range need to be considered, and these voxels can be projected via efficient rasterization schemes. In splatting, each projected voxel is represented as a radially symmetric interpolation kernel, equivalent to a fuzzy ball. Projecting such a basis function leaves a fuzzy impression, called a footprint or splat, on the screen. Splatting traditionally classifies and shades the voxels prior to projection, and thus each voxel footprint is weighted by the assigned voxel color and opacity. Projecting these fuzzy color balls provides a uniform screen image for homogeneous object regions, but leads to a blurry appearance of object edges. The latter is clearly undesirable, especially when the view is zoomed on the object. In this work, we manipulate the rendering pipeline of splatting by performing the classification and shading process after the voxels have been projected onto the screen. In this way, volume contributions outside the iso-range never affect the image. Since shading requires gradients, we not only splat the density volume, using regular splats, but we also project the gradient volume, using gradient splats. However, alternative to gradient splats, we can also compute the gradients on the projection plane, using central differencing. This latter scheme cuts the number of footprint rasterization by a factor of four, since only the voxel densities have to be projected. Our new method renders objects with crisp edges and well-preserved surface detail. Added overhead is the calculation of the screen gradients and the per-pixel shading. Both of these operations, however, may be performed using fast techniques employing lookup tables.},
	urldate = {2015-10-27},
	booktitle = {Proceedings of the {Conference} on {Visualization} '99: {Celebrating} {Ten} {Years}},
	publisher = {IEEE Computer Society Press},
	author = {Mueller, Klaus and M{\"o}ller, Torsten and Crawfis, Roger},
	year = {1999},
	pages = {363--370}
}

@book{yang_optimization_2001,
	title = {Optimization {Methods} and {Applications}},
	isbn = {978-0-7923-6866-3},
	abstract = {This edited book is dedicated to Professor N. U. Ahmed, a leading scholar and a renowned researcher in optimal control and optimization on the occasion of his retirement from the Department of Electrical Engineering at University of Ottawa in 1999. The contributions of this volume are in the areas of optimal control, non linear optimization and optimization applications. They are mainly the im proved and expanded versions of the papers selected from those presented in two special sessions of two international conferences. The first special session is Optimization Methods, which was organized by K. L. Teo and X. Q. Yang for the International Conference on Optimization and Variational Inequality, the City University of Hong Kong, Hong Kong, 1998. The other one is Optimal Control, which was organized byK. {\textasciitilde}Teo and L. Caccetta for the Dynamic Control Congress, Ottawa, 1999. This volume is divided into three parts: Optimal Control; Optimization Methods; and Applications. The Optimal Control part is concerned with com putational methods, modeling and nonlinear systems. Three computational methods for solving optimal control problems are presented: (i) a regularization method for computing ill-conditioned optimal control problems, (ii) penalty function methods that appropriately handle final state equality constraints, and (iii) a multilevel optimization approach for the numerical solution of opti mal control problems. In the fourth paper, the worst-case optimal regulation involving linear time varying systems is formulated as a minimax optimal con trol problem.},
	language = {en},
	publisher = {Springer Science \& Business Media},
	author = {Yang, Xiao-qi},
	month = apr,
	year = {2001},
	keywords = {Computers / Information Technology, Computers / Software Development \& Engineering / General, Mathematics / Applied, Mathematics / Calculus, Mathematics / Functional Analysis, Mathematics / Linear \& Nonlinear Programming, Mathematics / Optimization, Technology \& Engineering / Electrical}
}

@book{spedicato_algorithms_2012,
	title = {Algorithms for {Continuous} {Optimization}: {The} {State} of the {Art}},
	isbn = {978-94-009-0369-2},
	shorttitle = {Algorithms for {Continuous} {Optimization}},
	abstract = {The NATO Advanced Study Institute on "Algorithms for continuous optimiza tion: the state of the art" was held September 5-18, 1993, at II Ciocco, Barga, Italy. It was attended by 75 students (among them many well known specialists in optimiza tion) from the following countries: Belgium, Brasil, Canada, China, Czech Republic, France, Germany, Greece, Hungary, Italy, Poland, Portugal, Rumania, Spain, Turkey, UK, USA, Venezuela. The lectures were given by 17 well known specialists in the field, from Brasil, China, Germany, Italy, Portugal, Russia, Sweden, UK, USA. Solving continuous optimization problems is a fundamental task in computational mathematics for applications in areas of engineering, economics, chemistry, biology and so on. Most real problems are nonlinear and can be of quite large size. Devel oping efficient algorithms for continuous optimization has been an important field of research in the last 30 years, with much additional impetus provided in the last decade by the availability of very fast and parallel computers. Techniques, like the simplex method, that were already considered fully developed thirty years ago have been thoroughly revised and enormously improved. The aim of this ASI was to present the state of the art in this field. While not all important aspects could be covered in the fifty hours of lectures (for instance multiob jective optimization had to be skipped), we believe that most important topics were presented, many of them by scientists who greatly contributed to their development.},
	language = {en},
	publisher = {Springer Science \& Business Media},
	author = {Spedicato, Emilio Goiuseppe},
	month = dec,
	year = {2012},
	keywords = {Computers / Computer Science, Computers / Programming / Algorithms, Mathematics / Applied, Mathematics / Counting \& Numeration, Mathematics / Discrete Mathematics, Mathematics / General, Mathematics / Numerical Analysis, Mathematics / Optimization}
}

@incollection{peachey_parallel_2009,
	series = {Springer {Optimization} and {Its} {Applications}},
	title = {Parallel line search},
	copyright = {Â©2009 Springer-Verlag New York},
	isbn = {978-0-387-98095-9 978-0-387-98096-6},
	url = {http://link.springer.com/chapter/10.1007/978-0-387-98096-6_20},
	abstract = {We consider the well-known line search algorithm that iteratively refines the search interval by subdivision and bracketing the optimum. In our applications, evaluations of the objective function typically require minutes or hours, so it becomes attractive to use more than the standard three steps in the subdivision, performing the evaluations in parallel. A statistical model for this scenario is presented giving the total execution time T in terms of the number of steps k and the probability distribution for the individual evaluation times. Both the model and extensive simulations show that the expected value of T does not fall monotonically with k, in fact more steps may significantly increase the execution time. We propose heuristics for speeding convergence by continuing to the next iteration before all evaluations are complete. Simulations are used to estimate the speedup achieved.},
	language = {en},
	number = {32},
	urldate = {2015-10-28},
	booktitle = {Optimization},
	publisher = {Springer New York},
	author = {Peachey, T. C. and Abramson, D. and Lewis, A.},
	editor = {Pearce, Charles and Hunt, Emma},
	year = {2009},
	note = {DOI: 10.1007/978-0-387-98096-6\_20},
	keywords = {line search, Mathematical Modeling and Industrial Mathematics, Optimization, parallel computation},
	pages = {369--381},
	file = {00b4953222a6dbf568000000.pdf:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\M2CARQW6\\00b4953222a6dbf568000000.pdf:application/pdf;Snapshot:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\BBZIMJJA\\10.html:text/html}
}

@article{phua_parallel_1998,
	title = {Parallel {Algorithms} for {Large}-scale {Nonlinear} {Optimization}},
	volume = {5},
	issn = {1475-3995},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1475-3995.1998.tb00103.x/abstract},
	doi = {10.1111/j.1475-3995.1998.tb00103.x},
	abstract = {Multi-step, multi-directional parallel variable metric (PVM) methods for unconstrained optimization problems are presented in this paper. These algorithms generate several VM directions at each iteration, dierent line search and scaling strategies are then applied in parallel along each search direction. In comparison to some serial VM methods, computational results show that a reduction of 200\% or more in terms of number of iterations and function/gradient evaluations respectively could be achieved by the new parallel algorithm over a wide range of 63 test problems. In particular, when the complexity, or the size of the problem increases, greater savings could be achieved by the proposed parallel algorithm. In fact, the speedup factors gained by our PVM algorithms could be as high as 28 times for some test problems.},
	language = {en},
	number = {1},
	urldate = {2015-10-28},
	journal = {International Transactions in Operational Research},
	author = {Phua, P.k-H. and Fan, W. and Zeng, Y.},
	month = jan,
	year = {1998},
	pages = {67--77},
	file = {Full Text PDF:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\QAF3FXAA\\Phua et al. - 1998 - Parallel Algorithms for Large-scale Nonlinear Opti.pdf:application/pdf;Snapshot:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\QJ89NNBP\\abstract.html:text/html}
}

@article{koko_parallel_1998,
	title = {Parallel {Implementation} of a {Generalized} {Conjugate} {Gradient} {Algorithm}.},
	volume = {9},
	url = {http://dblp.uni-trier.de/db/journals/informaticaLT/informaticaLT9.html#KokoM98},
	number = {4},
	urldate = {2015-10-28},
	journal = {Informatica, Lith. Acad. Sci.},
	author = {Koko, Jonas and Moukrim, Aziz},
	year = {1998},
	keywords = {dblp},
	pages = {437--448},
	file = {INFO134.pdf:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\V9KAH6PE\\INFO134.pdf:application/pdf}
}

@book{cunningham_experimental_2011,
	opt_address = {Natick, MA, USA},
	edition = {1st},
	title = {Experimental {Design}: {From} {User} {Studies} to {Psychophysics}},
	opt_isbn = {1-56881-468-2 978-1-56881-468-1},
	shorttitle = {Experimental {Design}},
	abstract = {As computers proliferate and as the field of computer graphics matures, it has become increasingly important for computer scientists to understand how users perceive and interpret computer graphics. Experimental Design: From User Studies to Psychophysics is an accessible introduction to psychological experiments and experimental design, covering the major components in the design, execution, and analysis of perceptual studies. The book begins with an introduction to the concepts central to designing and understanding experiments, including developing a research question, setting conditions and controls, and balancing specificity with generality. The book then explores in detail a number of types of experimental tasks: free description, rating scales, forced-choice, specialized multiple choice, and real-world tasks as well as physiological studies. It discusses the advantages and disadvantages of each type and provides examples of that type of experiment from the authors own work. The book also covers stimulus-related issues, including popular stimulus resources. It concludes with a thorough examination of statistical techniques for analyzing results, including methods specific to individual tasks.},
	publisher = {A. K. Peters, Ltd.},
	author = {Cunningham, Douglas and Wallraven, Christian},
	year = {2011}
}

@inproceedings{hibbard_interactivity_1989,
	address = {New York, NY, USA},
	series = {{VVS} '89},
	title = {Interactivity is the {Key}},
	url = {http://doi.acm.org/10.1145/329129.329356},
	doi = {10.1145/329129.329356},
	urldate = {2015-11-09},
	booktitle = {Proceedings of the 1989 {Chapel} {Hill} {Workshop} on {Volume} {Visualization}},
	publisher = {ACM},
	author = {Hibbard, William and Santek, David},
	year = {1989},
	keywords = {earth science, interactive, Texture Mapping, volume image},
	pages = {39--43},
	file = {ACM Full Text PDF:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\2ZGNNTDA\\Hibbard and Santek - 1989 - Interactivity is the Key.pdf:application/pdf}
}

@incollection{wilson_interactive_2002,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Interactive {Multi}-volume {Visualization}},
	copyright = {©2002 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-540-43593-8 978-3-540-46080-0},
	url = {http://link.springer.com/chapter/10.1007/3-540-46080-2_11},
	abstract = {This paper is concerned with simultaneous visualization of two or more volumes, which may be from different imaging modalities or numerical simulations for the same subject of study. The main visualization challenge is to establish visual correspondences while maintaining distinctions among multiple volumes. One solution is to use different rendering styles for different volumes. Interactive rendering is required so the user can choose with ease an appropriate rendering style and its associated parameters for each volume. Rendering efficiency is maximized by utilizing commodity graphics cards. We demonstrate our preliminary results with two case studies.},
	language = {en},
	number = {2330},
	urldate = {2015-11-09},
	booktitle = {Computational {Science} — {ICCS} 2002},
	publisher = {Springer Berlin Heidelberg},
	author = {Wilson, Brett and Lum, Eric B. and Ma, Kwan-Liu},
	editor = {Sloot, Peter M. A. and Hoekstra, Alfons G. and Tan, C. J. Kenneth and Dongarra, Jack J.},
	month = apr,
	year = {2002},
	note = {DOI: 10.1007/3-540-46080-2\_11},
	keywords = {Computational Mathematics and Numerical Analysis, Computer Communication Networks, Mathematical and Computational Physics, Mathematics of Computing, Software Engineering/Programming and Operating Systems, Theory of Computation},
	pages = {102--110}
}

@inproceedings{muraki_basic_2000,
	title = {Basic research for coloring multichannel {MRI} data},
	doi = {10.1109/VISUAL.2000.885693},
	abstract = {This is basic research for assigning color values to voxels of multichannel MRI volume data. The MRI volume data sets obtained under different scanning conditions are transformed into their components by independent component analysis (ICA), which enhances the physical characteristics of the tissue. The transfer functions for generating color values from independent components are obtained using a radial basis function network, a kind of neural net, by training the network with sample data chosen from the Visible Female data set. The resultant color volume data sets correspond well with the full-color cross-sections of the Visible Human data sets.},
	booktitle = {Visualization 2000. {Proceedings}},
	author = {Muraki, S. and Nakai, T. and Kita, Y.},
	month = oct,
	year = {2000},
	keywords = {biological tissues, Biomedical imaging, biomedical MRI, color volume data sets, colour graphics, Computed tomography, data visualisation, full-color cross-sections, Humans, independent component analysis, magnetic resonance imaging, medical diagnostic imaging, medical image processing, multichannel MRI volume data, neural net training, Protons, radial basis function network, radial basis function networks, sample data, scanning conditions, surgery, tissue physical characteristics enhancement, Transfer functions, Visible Female data set, Visible Human data sets, voxel color value assignment},
	pages = {187--194},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\V567BUSG\\articleDetails.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\ZRX5V3JU\\Muraki et al. - 2000 - Basic research for coloring multichannel MRI data.pdf:application/pdf}
}

@inproceedings{shepard_two-dimensional_1968,
	opt_address = {New York, NY, USA},
	opt_series = {{ACM} '68},
	title = {A {Two}-dimensional {Interpolation} {Function} for {Irregularly}-spaced {Data}},
	url = {http://doi.acm.org/10.1145/800186.810616},
	doi = {10.1145/800186.810616},
	abstract = {In many fields using empirical areal data there arises a need for interpolating from irregularly-spaced data to produce a continuous surface. These irregularly-spaced locations, hence referred to as “data points,” may have diverse meanings: in meterology, weather observation stations; in geography, surveyed locations; in city and regional planning, centers of data-collection zones; in biology, observation locations. It is assumed that a unique number (such as rainfall in meteorology, or altitude in geography) is associated with each data point. In order to display these data in some type of contour map or perspective view, to compare them with data for the same region based on other data points, or to analyze them for extremes, gradients, or other purposes, it is extremely useful, if not essential, to define a continuous function fitting the given values exactly. Interpolated values over a fine grid may then be evaluated. In using such a function it is assumed that the original data are without error, or that compensation for error will be made after interpolation.},
	urldate = {2015-11-02},
	booktitle = {Proceedings of the 1968 23rd {ACM} {National} {Conference}},
	opt_publisher = {ACM},
	author = {Shepard, Donald},
	year = {1968},
	pages = {517--524}
}

@phdthesis{ruiz_altisent_advanced_2012,
	type = {{PhD} {Thesis}},
	title = {Advanced illumination and view-selection techniques for volume rendering and its application to medical imaging},
	school = {University of Girona},
	author = {Ruiz Altisent, Marc and Boada, Imma and Sbert, Mateu and Feixas Feixas, Miquel},
	collaborator = {{Universitat de Girona}},
	year = {2012},
	keywords = {Imatges, InformÃ tica, Medicina, Processament TÃ¨cniques digitals, Tesis, Universitat de Girona, VisiÃ³ per ordinador en medicina, VisualitzaciÃ³ tridimensional (InformÃ tica)},
	file = {MarcRuizPhD.pdf:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\SFD4U2FT\\MarcRuizPhD.pdf:application/pdf}
}

@article{sbert_information_2009,
	title = {Information {Theory} {Tools} for {Computer} {Graphics}},
	volume = {4},
	issn = {1933-8996},
	url = {http://www.morganclaypool.com/doi/abs/10.2200/S00208ED1V01Y200909CGR012},
	doi = {10.2200/S00208ED1V01Y200909CGR012},
	number = {1},
	urldate = {2015-05-19},
	journal = {Synthesis Lectures on Computer Graphics and Animation},
	author = {Sbert, Mateu and Feixas, Miquel and Rigau, Jaume and Chover, Miguel and Viola, Ivan},
	month = jan,
	year = {2009},
	pages = {1--153},
	file = {18c4-3.pdf:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\SFCN3V5V\\18c4-3.pdf:application/pdf;Sbert M., et al. Information theory tools for computer graphics (Morgan, 2009)(ISBN 1598299298)(C)(O)(166s)_CsCg_.pdf:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\6B32SIU7\\Sbert M., et al. Information theory tools for computer graphics (Morgan, 2009)(ISBN 1598299298)(C)(O)(166s)_CsCg_.pdf:application/pdf;Snapshot:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\DZTR47GT\\S00208ED1V01Y200909CGR012.html:text/html}
}

@inproceedings{rezk-salama_interactive_1999,
	address = {Los Alamitos, CA, USA},
	series = {{VIS} '99},
	title = {Interactive {Exploration} of {Volume} {Line} {Integral} {Convolution} {Based} on 3D-texture {Mapping}},
	isbn = {0-7803-5897-X},
	url = {http://dl.acm.org/citation.cfm?id=319351.319379},
	abstract = {Line integral convolution (LIC) is an effective technique for visualizing vector fields. The application of LIC to 3D flow fields has yet been limited by difficulties to efficiently display and animate the resulting 3D-images. Texture-based volume rendering allows interactive visualization and manipulation of 3D-LIC textures. In order to ensure the comprehensive and convenient exploration of flow fields, we suggest interactive functionality including transfer functions and different clipping mechanisms. Thereby, we efficiently substitute the calculation of LIC based on sparse noise textures and show the convenient visual access of interior structures. Further on, we introduce two approaches for animating static 3D-flow fields without the computational expense and the immense memory requirements for pre-computed 3D-textures and without loss of interactivity. This is achieved by using a single 3D-LIC texture and a set of time surfaces as clipping geometries. In our first approach we use the clipping geometry to pre-compute a special 3D-LIC texture that can be animated by time-dependent color tables. Our second approach uses time volumes to actually clip the 3D-LIC volume interactively during rasterization. Additionally, several examples demonstrate the value of our strategy in practice.},
	urldate = {2015-11-12},
	booktitle = {Proceedings of the {Conference} on {Visualization} '99: {Celebrating} {Ten} {Years}},
	publisher = {IEEE Computer Society Press},
	author = {Rezk-Salama, C. and Hastreiter, P. and Teitzel, C. and Ertl, T.},
	year = {1999},
	keywords = {3D-textures mapping, animated LIC, direct volume rendering, flow visualization, interactive volume exploration},
	pages = {233--240},
	file = {ACM Full Text PDF:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\3S4WP4QZ\\Rezk-Salama et al. - 1999 - Interactive Exploration of Volume Line Integral Co.pdf:application/pdf}
}

@article{silverstein_automatic_2008,
	title = {Automatic {Perceptual} {Color} {Map} {Generation} for {Realistic} {Volume} {Visualization}},
	volume = {41},
	issn = {1532-0464},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2651027/},
	doi = {10.1016/j.jbi.2008.02.008},
	abstract = {Advances in computed tomography imaging technology and inexpensive high performance computer graphics hardware are making high-resolution, full color (24-bit) volume visualizations commonplace. However, many of the color maps used in volume rendering provide questionable value in knowledge representation and are non-perceptual thus biasing data analysis or even obscuring information. These drawbacks, coupled with our need for realistic anatomical volume rendering for teaching and surgical planning, has motivated us to explore the auto-generation of color maps that combine natural colorization with the perceptual discriminating capacity of grayscale. As evidenced by the examples shown that have been created by the algorithm described, the merging of perceptually accurate and realistically colorized virtual anatomy appears to insightfully interpret and impartially enhance volume rendered patient data.},
	number = {6},
	urldate = {2015-11-13},
	journal = {Journal of biomedical informatics},
	author = {Silverstein, Jonathan C. and Parsad, Nigel M. and Tsirline, Victor},
	month = dec,
	year = {2008},
	pmid = {18430609},
	pmcid = {PMC2651027},
	pages = {927--935},
	file = {PubMed Central Full Text PDF:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\WNNPFVPJ\\Silverstein et al. - 2008 - Automatic Perceptual Color Map Generation for Real.pdf:application/pdf}
}

@article{cai_rule-enhanced_2015,
	title = {Rule-{Enhanced} {Transfer} {Function} {Generation} for {Medical} {Volume} {Visualization}},
	volume = {34},
	copyright = {© 2015 The Author(s) Computer Graphics Forum © 2015 The Eurographics Association and John Wiley \& Sons Ltd. Published by John Wiley \& Sons Ltd.},
	issn = {1467-8659},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/cgf.12624/abstract},
	doi = {10.1111/cgf.12624},
	abstract = {In volume visualization, transfer functions are used to classify the volumetric data and assign optical properties to the voxels. In general, transfer functions are generated in a transfer function space, which is the feature space constructed by data values and properties derived from the data. If volumetric objects have the same or overlapping data values, it would be difficult to separate them in the transfer function space. In this paper, we present a rule-enhanced transfer function design method that allows important structures of the volume to be more effectively separated and highlighted. We define a set of rules based on the local frequency distribution of volume attributes. A rule-selection method based on a genetic algorithm is proposed to learn the set of rules that can distinguish the user-specified target tissue from other tissues. In the rendering stage, voxels satisfying these rules are rendered with higher opacities in order to highlight the target tissue. The proposed method was tested on various volumetric datasets to enhance the visualization of important structures that are difficult to be visualized by traditional transfer function design methods. The results demonstrate the effectiveness of the proposed method.},
	language = {en},
	number = {3},
	urldate = {2015-11-15},
	journal = {Computer Graphics Forum},
	author = {Cai, Li-Le and Nguyen, Binh P. and Chui, Chee-Kong and Ong, Sim-Heng},
	month = jun,
	year = {2015},
	keywords = {Categories and Subject Descriptors (according to ACM CCS), Computer Graphics [I.3.3]: Picture/Image Generation—Line and curve generation},
	pages = {121--130}
}

@article{johnson_distribution-driven_2009,
	title = {Distribution-{Driven} {Visualization} of {Volume} {Data}},
	volume = {15},
	issn = {1077-2626},
	doi = {10.1109/TVCG.2009.25},
	abstract = {Feature detection and display are the essential goals of the visualization process. Most visualization software achieves these goals by mapping properties of sampled intensity values and their derivatives to color and opacity. In this work, we propose to explicitly study the local frequency distribution of intensity values in broader neighborhoods centered around each voxel. We have found frequency distributions to contain meaningful and quantitative information that is relevant for many kinds of feature queries. Our approach allows users to enter predicate-based hypotheses about relational patterns in local distributions and render visualizations that show how neighborhoods match the predicates. Distributions are a familiar concept to nonexpert users, and we have built a simple graphical user interface for forming and testing queries interactively. The query framework readily applies to arbitrary spatial data sets and supports queries on time variant and multifield data. Users can directly query for classes of features previously inaccessible in general feature detection tools. Using several well-known data sets, we show new quantitative features that enhance our understanding of familiar visualization results.},
	number = {5},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Johnson, C.R. and Huang, Jian},
	month = oct,
	year = {2009},
	keywords = {data visualisation, distribution-driven visualization, feature detection, features in volume data., frequency distribution, graphical user interface, graphical user interfaces, multivariate data, predicate-based hypotheses, relational pattern, rendering (computer graphics), Rendering (computer graphics), volume data visualization, Volume rendering, volume visualization},
	pages = {734 --746},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\joe\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\z4ailp3m.default\\zotero\\storage\\FFGHXVAU\\articleDetails.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\joe\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\z4ailp3m.default\\zotero\\storage\\KHS2D9VQ\\Johnson and Huang - 2009 - Distribution-Driven Visualization of Volume Data.pdf:application/pdf}
}

@inproceedings{rezk-salama_interactive_2000,
	address = {New York, NY, USA},
	series = {{HWWS} '00},
	title = {Interactive {Volume} on {Standard} {PC} {Graphics} {Hardware} {Using} {Multi}-textures and {Multi}-stage {Rasterization}},
	isbn = {1-58113-257-3},
	url = {http://doi.acm.org/10.1145/346876.348238},
	doi = {10.1145/346876.348238},
	abstract = {Interactive direct volume rendering has yet been restricted to high-end graphics workstations and special-purpose hardware, due to the large amount of trilinear interpolations, that are necessary to obtain high image quality. Implementations that use the 2D-texture capabilities of standard PC hardware, usually render object-aligned slices in order to substitute trilinear by bilinear interpolation. However the resulting images often contain visual artifacts caused by the lack of spatial interpolation. In this paper we propose new rendering techniques that significantly improve both performance and image quality of the 2D-texture based approach. We will show how in ulti-texturing capabilitiesof modern consumer PC graphboards are exploited to enable in teractive high quality volume visualization on low-cost hardware. Furthermore we demonstrate how multi-stage rasterization hardware can be used to efficiently render shaded isosurfaces and to compute diffuse illumination for semi-transparent volume rendering at interactive frame rates.},
	urldate = {2015-11-28},
	booktitle = {Proceedings of the {ACM} {SIGGRAPH}/{EUROGRAPHICS} {Workshop} on {Graphics} {Hardware}},
	publisher = {ACM},
	author = {Rezk-Salama, C. and Engel, K. and Bauer, M. and Greiner, G. and Ertl, T.},
	year = {2000},
	keywords = {multi-textures, PC hardware, rasterization, Volume rendering},
	pages = {109--118}
}
@phdthesis{diaz_iriberri_enhanced_2013,
	address = {Barcelona, Catalonia, Spain},
	type = {{PhD} {Thesis}},
	title = {Enhanced perception in volume visualization},
	url = {http://upcommons.upc.edu/handle/10803/117524},
	abstract = {Due to the nature of scientic data sets, the generation of convenient visualizations may be a difficult task, but crucial to correctly convey the relevant information of the data. When working with complex volume models, such as the anatomical ones, it is important to provide accurate representations, since a misinterpretation can lead to serious mistakes while diagnosing a disease or planning surgery. In these cases, enhancing the perception of the features of interest usually helps to properly understand the data.
Throughout years, researchers have focused on different methods to improve the visualization of volume data sets. For instance, the definition of good transfer functions is a key issue in Volume Visualization, since transfer functions determine how materials are classified. Other approaches are based on simulating realistic illumination models to enhance the spatial perception, or using illustrative effects to provide the level of abstraction needed to correctly interpret the data.
This thesis contributes with new approaches to enhance the visual and spatial perception in Volume Visualization. Thanks to the new computing capabilities of modern graphics hardware, the proposed algorithms are capable of modifying the illumination model and simulating illustrative motifs in real time.
In order to enhance local details, which are useful to better perceive the shape and the surfaces of the volume, our first contribution is an algorithm that employs a common sharpening operator to modify the lighting applied. As a result, the overall contrast of the visualization is enhanced by brightening the salient features and darkening the deeper regions of the volume model.
The enhancement of depth perception in Direct Volume Rendering is also covered in the thesis. To do this, we propose two algorithms to simulate ambient occlusion: a screen-space technique based on using depth information to estimate the amount of light occluded, and a view-independent method that uses the density values of the data set to estimate the occlusion. Additionally, depth perception is also enhanced by adding halos around the structures of interest.
Maximum Intensity Projection images provide a good understanding of the high intensity features of the data, but lack any contextual information. In order to enhance the depth perception in such a case, we present a novel technique based on changing how intensity is accumulated. Furthermore, the perception of the spatial arrangement of the displayed structures is also enhanced by adding certain colour cues.
The last contribution is a new manipulation tool designed for adding contextual information when cutting the volume. Based on traditional illustrative effects, this method allows the user to directly extrude structures from the cross-section of the cut. As a result, the clipped structures are displayed at different heights, preserving the information needed to correctly perceive them.},
	language = {eng},
	urldate = {2015-10-20},
	school = {Polytechnic University of Catalonia},
	author = {D{\'i}az Iriberri, Jos{\'e}},
	month = apr,
	year = {2013},
	file = {Snapshot:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\DZKS3FNT\\117524.html:text/html;TJDI1de1.pdf:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\N4HRGNDH\\TJDI1de1.pdf:application/pdf}
}

@article{diaz_adaptive_2012,
	title = {Adaptive {Cross}-sections of {Anatomical} {Models}},
	volume = {31},
	copyright = {© 2012 The Author(s) Computer Graphics Forum © 2012 The Eurographics Association and Blackwell Publishing Ltd.},
	issn = {1467-8659},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1467-8659.2012.03208.x/abstract},
	doi = {10.1111/j.1467-8659.2012.03208.x},
	abstract = {Medical illustrations have been used for a long time for teaching and communicating information for diagnosis or surgery planning. Illustrative visualization systems create methods and tools that adapt traditional illustration techniques to enhance the result of renderings. Clipping the volume is a popular operation in volume rendering for inspecting the inner parts, though it may remove some information of the context that is worth preserving. In this paper we present a new editing technique based on the use of clipping planes, direct structure extrusion, and illustrative methods, which preserves the context by adapting the extruded region to the structures of interest of the volumetric model. We will show that users may interactively modify the clipping plane and edit the structures to highlight, in order to easily create the desired result. Our approach works with segmented volume models and non-segmented ones. In the last case, a local segmentation is performed on-the-fly. We will demonstrate the efficiency and utility of our method.},
	language = {en},
	number = {7},
	urldate = {2015-11-28},
	journal = {Computer Graphics Forum},
	author = {D{\'i}az, J. and Moncl{\'u}s, E. and Navazo, I. and V{\'a}zquez, P.},
	month = sep,
	year = {2012},
	keywords = {I.3.3 [Computer Graphics]: Picture/Image Generation—Display algorithms.},
	pages = {2155--2164}
}

@inproceedings{diaz_perceptual_2015,
	title = {Perceptual effects of volumetric shading models in stereoscopic desktop-based environments},
	booktitle = {Computer {Graphics} {International}},
	author = {D{\'i}az, J. and Ropinski, T. and Navazo, I. and Gobbetti, E. and Vazquez, P.},
	year = {2015},
	pages = {1--10},
	file = {cgi2015-perceptual.pdf:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\QIAG4888\\cgi2015-perceptual.pdf:application/pdf}
}

@article{bruckner_exploded_2006,
	title = {Exploded {Views} for {Volume} {Data}},
	volume = {12},
	issn = {1077-2626},
	doi = {10.1109/TVCG.2006.140},
	abstract = {Exploded views are an illustration technique where an object is partitioned into several segments. These segments are displaced to reveal otherwise hidden detail. In this paper we apply the concept of exploded views to volumetric data in order to solve the general problem of occlusion. In many cases an object of interest is occluded by other structures. While transparency or cutaways can be used to reveal a focus object, these techniques remove parts of the context information. Exploded views, on the other hand, do not suffer from this drawback. Our approach employs a force-based model: the volume is divided into a part configuration controlled by a number of forces and constraints. The focus object exerts an explosion force causing the parts to arrange according to the given constraints. We show that this novel and flexible approach allows for a wide variety of explosion-based visualizations including view-dependent explosions. Furthermore, we present a high-quality GPU-based volume ray casting algorithm for exploded views which allows rendering and interaction at several frames per second},
	number = {5},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Bruckner, S. and Groller, M.E.},
	month = sep,
	year = {2006},
	keywords = {Algorithms, Anatomy, Cross-Sectional, Casting, Computer-Assisted Instruction, computer graphics, Computer Simulation, context information, data visualisation, data visualization, exploded views, Explosions, force-based model, Force control, Geometry, GPU-based volume ray casting algorithm, hidden feature removal, Humans, Illustrative visualization, image enhancement, Image Interpretation, Computer-Assisted, Imaging, Three-Dimensional, Information Storage and Retrieval, Models, Anatomic, occlusion, rendering (computer graphics), Tomography, User-Computer Interface, view-dependent explosion, Volume data, Volume rendering},
	pages = {1077--1084},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\SRSBPRPF\\abs_all.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\6EBZFVGE\\Bruckner and Groller - 2006 - Exploded Views for Volume Data.pdf:application/pdf}
}

@article{emami_selection_2013,
	title = {Selection of a best metric and evaluation of bottom-up visual saliency models},
	volume = {31},
	issn = {0262-8856},
	url = {http://www.sciencedirect.com/science/article/pii/S0262885613001303},
	doi = {10.1016/j.imavis.2013.08.004},
	abstract = {There are many “machine vision” models of the visual saliency mechanism, which controls the process of selecting and allocating attention to the most “prominent” locations in the scene and helps humans interact with the visual environment efficiently (Itti and C. Koch, 2001; Gao et al., 2000). It is important to know which models perform the best in mimicking the saliency mechanism of the human visual system. There are several metrics to compare saliency models; however, results from different metrics vary widely in evaluating models. In this paper, a procedure is proposed for evaluating metrics for comparing saliency maps using a database of human fixations on approximately 1000 images. This procedure is then employed to identify the best metric. This best metric is then used to evaluate ten published bottom-up saliency models. An optimized level of the blurriness and center-bias is found for each visual saliency model. Performance of the models is also analyzed on a dataset of 54 synthetic images.},
	number = {10},
	urldate = {2016-08-25},
	journal = {Image and Vision Computing},
	author = {Emami, Mohsen and Hoberock, Lawrence L.},
	month = oct,
	year = {2013},
	keywords = {Best comparison metric, Bottom-up saliency mechanism, Computer vision, human visual system},
	pages = {796--808},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\lazaro\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\9kdnyu8g.default\\zotero\\storage\\M33B5PID\\Emami and Hoberock - 2013 - Selection of a best metric and evaluation of botto.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\lazaro\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\9kdnyu8g.default\\zotero\\storage\\5H4AGUSS\\S0262885613001303.html:text/html}
}

@inproceedings{engel_cera-tvr:_2011,
	title = {{CERA}-{TVR}: {A} framework for interactive high-quality teravoxel volume visualization on standard {PCs}},
	shorttitle = {{CERA}-{TVR}},
	doi = {10.1109/LDAV.2011.6092330},
	abstract = {We present a volume visualization framework that allows high-quality interactive rendering of teravoxel scale volume data sets on standard PCs. It is based on a progressive multi-resolution out-of-core volume rendering approach. Data loading is controlled by rays that are cast through the volume data set. The tree structure representing the multi-resolution hierarchy is traversed and updated directly by the GPU during ray casting. Consequently, occluded and empty data is never loaded or rendered. The framework is able to render dense, anisotropic, scalar volume data on regular grids with all common rendering modes.},
	booktitle = {2011 {IEEE} {Symposium} on {Large} {Data} {Analysis} and {Visualization} ({LDAV})},
	author = {Engel, K.},
	month = oct,
	year = {2011},
	keywords = {CERA-TVR, computational geometry, data loading, data visualisation, data visualization, graphics processing unit, Image reconstruction, Image resolution, interactive high-quality teravoxel volume visualization, Loading, multi-resolution, Out-of-core Methods, Progressive, progressive multi resolution out-of-core volume rendering approach, ray casting, rendering (computer graphics), Teravoxel Data, Transfer functions, tree structure, Volume visualization},
	pages = {123--124},
	file = {06092330.pdf:C\:\\Users\\lazaro\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\9kdnyu8g.default\\zotero\\storage\\NCHFBAFV\\06092330.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\lazaro\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\9kdnyu8g.default\\zotero\\storage\\7BBJ7Q73\\6092330.html:text/html}
}

@inproceedings{fogal_analysis_2013,
	title = {An analysis of scalable {GPU}-based ray-guided volume rendering},
	doi = {10.1109/LDAV.2013.6675157},
	abstract = {Volume rendering continues to be a critical method for analyzing large-scale scalar fields, in disciplines as diverse as biomedical engineering and computational fluid dynamics. Commodity desktop hardware has struggled to keep pace with data size increases, challenging modern visualization software to deliver responsive interactions for O(N3) algorithms such as volume rendering. We target the data type common in these domains: regularly-structured data. In this work, we demonstrate that the major limitation of most volume rendering approaches is their inability to switch the data sampling rate (and thus data size) quickly. Using a volume renderer inspired by recent work, we demonstrate that the actual amount of visualizable data for a scene is typically bound considerably lower than the memory available on a commodity GPU. Our instrumented renderer is used to investigate design decisions typically swept under the rug in volume rendering literature. The renderer is freely available, with binaries for all major platforms as well as full source code, to encourage reproduction and comparison with future research.},
	booktitle = {2013 {IEEE} {Symposium} on {Large}-{Scale} {Data} {Analysis} and {Visualization} ({LDAV})},
	author = {Fogal, T. and Schiewe, A. and Kr{\"u}ger, J.},
	month = oct,
	year = {2013},
	keywords = {Biomedical Engineering, commodity desktop hardware, computational fluid dynamics, data sampling rate, data size, data visualisation, design decisions, graphics processing unit, graphics processing units, O(N3) algorithms, regularly-structured data domain, rendering (computer graphics), scalable GPU-based ray-guided volume rendering, scalar fields analysis, visualization software},
	pages = {43--51},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\lazaro\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\9kdnyu8g.default\\zotero\\storage\\5C8Q4J6S\\6675157.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\lazaro\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\9kdnyu8g.default\\zotero\\storage\\GPDGHC6M\\Fogal et al. - 2013 - An analysis of scalable GPU-based ray-guided volum.pdf:application/pdf}
}

@inproceedings{kruger_new_2010,
	address = {Aire-la-Ville, Switzerland, Switzerland},
	series = {{VG}'10},
	title = {A {New} {Sampling} {Scheme} for {Slice} {Based} {Volume} {Rendering}},
	isbn = {978-3-905674-23-1},
	url = {http://dx.doi.org/10.2312/VG/VG10/001-004},
	doi = {10.2312/VG/VG10/001-004},
	abstract = {In this paper we present a novel approach to generate proxy geometry for slice based volume rendering. The basic idea is derived from the behavior of a ray-caster and is a simple extension of the well known 2D object-aligned texture stack based technique. From this our novel scheme inherits the advantage that it enables hardware-based volume rendering for devices that do not support 3D textures. On these devices previous object-aligned 2D texture based approaches suffered from disturbing view angle dependent stack-switching artifacts which are avoided by our novel method. Our approach also shows benefits compared to the widely used view aligned slicing algorithm as it avoids jagged boundary artifacts and increases performance.},
	urldate = {2016-08-26},
	booktitle = {Proceedings of the 8th {IEEE}/{EG} {International} {Conference} on {Volume} {Graphics}},
	publisher = {Eurographics Association},
	author = {Kr{\"u}ger, J.},
	year = {2010},
	pages = {1--4},
	file = {vg.pdf:C\:\\Users\\lazaro\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\9kdnyu8g.default\\zotero\\storage\\JBN8KPHV\\vg.pdf:application/pdf}
}

@article{dai_nonlinear_1999,
	title = {A {Nonlinear} {Conjugate} {Gradient} {Method} with a {Strong} {Global} {Convergence} {Property}},
	volume = {10},
	issn = {1052-6234},
	url = {http://epubs.siam.org/doi/abs/10.1137/S1052623497318992},
	doi = {10.1137/S1052623497318992},
	abstract = {Conjugate gradient methods are widely used for unconstrained optimization, especially large scale problems. The strong Wolfe conditions are usually used in the analyses and implementations of conjugate gradient methods. This paper presents a new version of the conjugate gradient method, which converges globally, provided the line search satisfies the standard Wolfe conditions. The conditions on the objective function are also weak, being similar to those required by the Zoutendijk condition.},
	number = {1},
	urldate = {2016-08-04},
	journal = {SIAM Journal on Optimization},
	author = {Dai, Y. and Yuan, Y.},
	month = jan,
	year = {1999},
	pages = {177--182},
	file = {Snapshot:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\lnhf8ovg.default\\zotero\\storage\\NSXX83F3\\S1052623497318992.html:text/html}
}

@article{qin_voxel_2015,
	title = {The voxel visibility model: {An} efficient framework for transfer function design},
	volume = {40},
	issn = {0895-6111},
	shorttitle = {The voxel visibility model},
	url = {http://www.sciencedirect.com/science/article/pii/S089561111400189X},
	doi = {10.1016/j.compmedimag.2014.11.014},
	abstract = {Volume visualization is a very important work in medical imaging and surgery plan. However, determining an ideal transfer function is still a challenging task because of the lack of measurable metrics for quality of volume visualization. In the paper, we presented the voxel vibility model as a quality metric to design the desired visibility for voxels instead of designing transfer functions directly. Transfer functions are obtained by minimizing the distance between the desired visibility distribution and the actual visibility distribution. The voxel model is a mapping function from the feature attributes of voxels to the visibility of voxels. To consider between-class information and with-class information simultaneously, the voxel visibility model is described as a Gaussian mixture model. To highlight the important features, the matched result can be obtained by changing the parameters in the voxel visibility model through a simple and effective interface. Simultaneously, we also proposed an algorithm for transfer functions optimization. The effectiveness of this method is demonstrated through experimental results on several volumetric data sets.},
	urldate = {2016-09-09},
	journal = {Computerized Medical Imaging and Graphics},
	author = {Qin, Hongxing and Ye, Bin and He, Rui},
	month = mar,
	year = {2015},
	keywords = {Gaussian Mixture Model, GPU, Transfer function design, Visibility, Volume visualization},
	pages = {138--146},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\lazaro\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\9kdnyu8g.default\\zotero\\storage\\VG6NTPRN\\Qin et al. - 2015 - The voxel visibility model An efficient framework.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\lazaro\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\9kdnyu8g.default\\zotero\\storage\\7R5PR94S\\S089561111400189X.html:text/html}
}

@inproceedings{luo_information-guided_2014,
	title = {Information-Guided Transfer Function Refinement},
	doi = {10.2312/egsh.20141015},
	booktitle = {Eurographics 2014 - Short Papers},
	author = {Luo, Shengzhou and Dingliana, John},
	month = apr,
	year = {2014},
	pages = {61--64},
	URL = {http://diglib.eg.org/EG/DL/conf/EG2014/short/061-064.pdf},
	DOI = {10.2312/egsh.20141015},
	editor = {Eric Galin and Michael Wand},
	issn = {1017-4656},
	address = {Strasbourg, France},
	publisher = {Eurographics Association}
}

@inproceedings{shengzhou_luo_transfer_2014,
	address = {Ankara, Turkey},
	title = {Transfer {Function} {Refinement} for {Exploring} {Volume} {Data}},
	url = {http://www.eurasiagraphics.hacettepe.edu.tr/papers/06-Luo.pdf},
	booktitle = {Proceedings of {Eurasia} {Graphics} 2014: {International} {Conference} on {Computer} {Graphics}, {Animation} and {Gaming} {Technologsies}},
	author = {{Shengzhou Luo} and {John Dingliana}},
	year = {2014},
	file = {06-Luo.pdf:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\RJ62AP82\\06-Luo.pdf:application/pdf}
}

@inproceedings{shengzhou_luo_selective_2015,
	address = {Cagliari, Italy},
	title = {Selective {Saturation} and {Brightness} for {Visualizing} {Time}-{Varying} {Volume} {Data}},
	booktitle = {{EuroVis} 2015 - {Posters}},
	publisher = {The Eurographics Association},
	author = {Luo, Shengzhou and Dingliana, John},
	year = {2015},
	file = {ev2015poster.pdf:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\727v3yux.default\\zotero\\storage\\3UBSE93R\\ev2015poster.pdf:application/pdf}
}

@inproceedings{luo_transfer_2016,
	title = {Transfer {Function} {Optimization} {Based} on a {Combined} {Model} of {Visibility} and {Saliency}},
	isbn = {978-3-03868-015-4},
	doi = {10.2312/eurp.20161135},
	booktitle = {{EuroVis} 2016 - {Posters}},
	publisher = {The Eurographics Association},
	author = {Luo, Shengzhou and Dingliana, John},
	editor = {Isenberg, Tobias and Sadlo, Filip},
	year = {2016}
}

@inproceedings{luo_visibility-weighted_2015,
	address = {London, UK},
	title = {Visibility-{Weighted} {Saliency} for {Volume} {Visualization}},
	opt_isbn = {978-3-905674-94-1},
	doi = {10.2312/cgvc.20151243},
	booktitle = {Computer {Graphics} and {Visual} {Computing} ({CGVC})},
	opt_publisher = {The Eurographics Association},
	author = {Luo, Shengzhou and Dingliana, John},
	opt_editor = {Borgo, Rita and Turkay, Cagatay},
	year = {2015}
}

@article{baldevbhai2012color,
	title={Color image segmentation for medical images using L* a* b* color space},
	author={Baldevbhai, Patel Janakkumar and Anand, RS},
	journal={IOSR Journal of Electronics and Communication Engineering},
	volume={1},
	number={2},
	pages={24--45},
	year={2012}
}

@inproceedings{jr_supernovae_2016,
	title = {Supernovae {Observational} {Cosmology} using {GPU} {High} {Performance} {Computing}},
	volume = {3},
	url = {http://www.proceedings.blucher.com.br/article-details/supernovae-observational-cosmology-using-gpu-high-performance-computing-25251},
	abstract = {We performed computing calculations of SNe Ia (type Ia supernova) in observational cosmology using CPU (Central Processing Unit) and GPU/CUDA (Graphics Processi...},
	urldate = {2017-04-15},
	booktitle = {Blucher {Physics} {Proceedings}},
	author = {Jr, R. Colistete and Giostri, R.},
	year = {2016},
	pages = {62--64},
	file = {Full Text PDF:C\:\\Users\\joe\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\z4ailp3m.default\\zotero\\storage\\FVHBMVRX\\Jr and Giostri - 2016 - Supernovae Observational Cosmology using GPU High .pdf:application/pdf;Snapshot:C\:\\Users\\joe\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\z4ailp3m.default\\zotero\\storage\\5Q7WKIMQ\\supernovae-observational-cosmology-using-gpu-high-performance-computing-25251.html:text/html}
}

@article{wiebel_wysiwyp:_2012,
	title = {{WYSIWYP}: {What} {You} {See} {Is} {What} {You} {Pick}},
	volume = {18},
	issn = {1077-2626},
	shorttitle = {{WYSIWYP}},
	doi = {10.1109/TVCG.2012.292},
	abstract = {Scientists, engineers and physicians are used to analyze 3D data with slice-based visualizations. Radiologists for example are trained to read slices of medical imaging data. Despite the numerous examples of sophisticated 3D rendering techniques, domain experts, who still prefer slice-based visualization do not consider these to be very useful. Since 3D renderings have the advantage of providing an overview at a glance, while 2D depictions better serve detailed analyses, it is of general interest to better combine these methods. Recently there have been attempts to bridge this gap between 2D and 3D renderings. These attempts include specialized techniques for volume picking in medical imaging data that result in repositioning slices. In this paper, we present a new volume picking technique called WYSIWYP (“what you see is what you pick”) that, in contrast to previous work, does not require pre-segmented data or metadata and thus is more generally applicable. The positions picked by our method are solely based on the data itself, the transfer function, and the way the volumetric rendering is perceived by the user. To demonstrate the utility of the proposed method, we apply it to automated positioning of slices in volumetric scalar fields from various application areas. Finally, we present results of a user study in which 3D locations selected by users are compared to those resulting from WYSIWYP. The user study confirms our claim that the resulting positions correlate well with those perceived by the user.},
	number = {12},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Wiebel, A. and Vos, F.M. and Foerster, D. and Hege, H.-C.},
	year = {2012},
	keywords = {3D data analysis, 3D rendering, Biomedical imaging, data analysis, data visualisation, Data visualization, Equations, Geometry, Image color analysis, medical image processing, medical imaging data, Picking, radiologists, radiology, rendering (computer graphics), Rendering (computer graphics), slice-based visualizations, Transfer functions, transfer functions, volume picking, volume rendering, what you see is what you pick, WYSIWYG, WYSIWYP},
	pages = {2236--2244},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\lvbdd5tj.default\\zotero\\storage\\UJDBRMCG\\Wiebel et al. - 2012 - WYSIWYP What You See Is What You Pick.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\lvbdd5tj.default\\zotero\\storage\\DZ8JAZPG\\Wiebel et al. - 2012 - WYSIWYP What You See Is What You Pick.pdf:application/pdf}
}

@article{guo_wysiwyg_2011,
	title = {{WYSIWYG} ({What} {You} {See} is {What} {You} {Get}) {Volume} {Visualization}},
	volume = {17},
	issn = {1077-2626},
	doi = {10.1109/TVCG.2011.261},
	abstract = {In this paper, we propose a volume visualization system that accepts direct manipulation through a sketch-based What You See Is What You Get (WYSIWYG) approach. Similar to the operations in painting applications for 2D images, in our system, a full set of tools have been developed to enable direct volume rendering manipulation of color, transparency, contrast, brightness, and other optical properties by brushing a few strokes on top of the rendered volume image. To be able to smartly identify the targeted features of the volume, our system matches the sparse sketching input with the clustered features both in image space and volume space. To achieve interactivity, both special algorithms to accelerate the input identification and feature matching have been developed and implemented in our system. Without resorting to tuning transfer function parameters, our proposed system accepts sparse stroke inputs and provides users with intuitive, flexible and effective interaction during volume data exploration and visualization.},
	number = {12},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Guo, Hanqi and Mao, Ningyu and Yuan, Xiaoru},
	year = {2011},
	keywords = {2D images, Algorithms, Animals, Brightness, colour, Computer graphics, data visualisation, Data visualization, direct volume rendering manipulation, feature clustering, feature extraction, Feature extraction, feature identification, feature matching, Feature space., Human-computer interaction, Humans, Image color analysis, Imaging, Three-Dimensional, optical properties, painting applications, Real time systems, rendering (computer graphics), Rendering (computer graphics), Semantics, sketch-based What You See Is What You Get approach, Sketching input, Software, sparse sketching, transfer function parameter tuning, Transfer functions, transfer functions, User-Computer Interface, volume data exploration, volume data visualization, volume rendering, WYSIWYG volume visualization system},
	pages = {2106--2114},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\lvbdd5tj.default\\zotero\\storage\\95DKZPFE\\abs_all.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\lvbdd5tj.default\\zotero\\storage\\ZE9Q8V7C\\Guo et al. - 2011 - WYSIWYG (What You See is What You Get) Volume Visu.pdf:application/pdf}
}


@inproceedings{guo_local_2013,
	title = {Local {WYSIWYG} volume visualization},
	doi = {10.1109/PacificVis.2013.6596129},
	abstract = {In this paper, we propose a novel volume visualization system enabling local transfer function specification through direct painting or sketching on the rendered image, in a WYSIWYG style. Localized transfer functions are defined on scalar topology regions specified by the user. Intelligent and fast feature inference algorithms have been developed to convert user's input to the region specification and to achieve desirable feature styles with the local transfer functions. In our system, users can not only manipulate the color appearance of the object volume, but also apply style transfer and generate various illustration styles with a unified input gesture. Without manual transfer function editing and without parameter specification, our system is capable of generating informative illustrations that intuitively highlight user specified local features.},
	booktitle = {Visualization {Symposium} ({PacificVis}), 2013 {IEEE} {Pacific}},
	author = {Guo, Hanqi and Yuan, Xiaoru},
	month = feb,
	year = {2013},
	keywords = {color appearance, data visualisation, direct rendered image painting, fast feature inference algorithms, inference mechanisms, intelligent inference algorithms, localized transfer functions, Local transfer functions, local transfer function specification, local WYSIWYG volume visualization system, object volume, Painting, region specification, rendered image sketching, rendering (computer graphics), scalar field topology, scalar topology regions, Semantics, Sketch-based user interface, three-dimensional displays, Topology, Transfer functions, unified input gesture, User interfaces, Visualization, Volume visualization, WYSIWYG},
	pages = {65--72},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\lvbdd5tj.default\\zotero\\storage\\88XJQD5R\\Guo and Yuan - 2013 - Local WYSIWYG volume visualization.pdf:application/pdf}
}

@article{li_visualization_2016,
	title = {Visualization of boundaries in volumetric data sets through a what material you pick is what boundary you see approach},
	volume = {126},
	issn = {0169-2607},
	url = {http://www.sciencedirect.com/science/article/pii/S0169260715300663},
	doi = {10.1016/j.cmpb.2015.11.014},
	abstract = {Transfer function design is a key issue in direct volume rendering. Many sophisticated transfer functions have been proposed to visualize boundaries in volumetric data sets such as computed tomography and magnetic resonance imaging. However, it is still conventionally challenging to reliably detect boundaries. Meanwhile, the interactive strategy is complicated for new users or even experts. In this paper, we first propose the human-centric boundary extraction criteria and our boundary model. Based on the model we present a boundary visualization method through a what material you pick is what boundary you see approach. Users can pick out the material of interest to directly convey semantics. In addition, the 3-D canny edge detection is utilized to ensure the good localization of boundaries. Furthermore, we establish a point-to-material distance measure to guarantee the accuracy and integrity of boundaries. The proposed boundary visualization is intuitive and flexible for the exploration of volumetric data.},
	urldate = {2016-12-08},
	journal = {Computer Methods and Programs in Biomedicine},
	author = {Li, Lu and Peng, Hu and Chen, Xun and Cheng, Juan and Gao, Dayong},
	month = apr,
	year = {2016},
	keywords = {3-D edge detection, Boundary visualization, direct volume rendering, transfer function},
	pages = {76--88},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\lvbdd5tj.default\\zotero\\storage\\VJGCIKN6\\Li et al. - 2016 - Visualization of boundaries in volumetric data set.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\dell\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\lvbdd5tj.default\\zotero\\storage\\S2826NEN\\S0169260715300663.html:text/html}
}

@inproceedings{ropinski_stroke-based_2008,
	title = {Stroke-{Based} {Transfer} {Function} {Design}},
	url = {http://viscg.uni-muenster.de/publications/2008/RPSH08},
	booktitle = {{IEEE}/{EG} {International} {Symposium} on {Volume} and {Point}-{Based} {Graphics}},
	publisher = {IEEE},
	author = {Ropinski, Timo and Pra{\ss}ni, J{\"o}rg-Stefan and Steinicke, Frank and Hinrichs, Klaus H.},
	year = {2008},
	keywords = {vr, voreen},
	pages = {41--48},
	file = {RPSH08.pdf:C\:\\Users\\dell\\Zotero\\storage\\TEBUGDG2\\RPSH08.pdf:application/pdf}
}

@techreport{castro_transfer_1998,
	address = {Universit{\"a}t Wien},
	type = {Technical {Report}},
	title = {Transfer {Function} {Specification} for the {Visualization} of {Medical} {Data}},
	url = {http://www.cg.tuwien.ac.at/courses/Forschungsseminar/WS06/307-c.pdf},
	abstract = {The application of transfer functions to map data values to visual properties as, e.g., color and opacity, is a crucial step in direct volume rendering. Due to the complex relationship between the transfer function and the resulting image it is usually extremely difﬁcult to model an appropriate mapping. In this paper we present an advanced transfer function speciﬁcation scheme for the visualization of medical data. The concept of metadata is used to make the modeling of transfer functions more intuitive. A small number of parameters is sufﬁcient to completely describe a transfer function, thus this speciﬁcation scheme is suitable for (semi-)automated search techniques.},
	urldate = {2013-08-10},
	author = {Castro, Silvia and K{\"o}nig, Andreas and L{\"o}ffelmann, Helwig and {Eduard Gr{\"o}ller}},
	year = {1998},
	file = {Citeseer - Full Text PDF:C\:\\Users\\dell\\Zotero\\storage\\B5VH2CRI\\Castro et al. - 1998 - Transfer Function Specification for the Visualizat.pdf:application/pdf;Citeseer - Snapshot:C\:\\Users\\dell\\Zotero\\storage\\3TXG5BX4\\summary.html:text/html}
}


